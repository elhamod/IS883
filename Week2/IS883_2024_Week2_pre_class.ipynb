{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elhamod/IS883/blob/main/Week2/IS883_2024_Week2_pre_class.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IS883 Week 2: Basic Language Modeling\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "f4sUdGT4laxM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-a5FSOqeOaa"
      },
      "source": [
        "1. Use Google Colab for this assignment.\n",
        "\n",
        "2. **You are allowed to use ChatGPT for this assignment. However, as per the syllabus, you are required to cite your usage and submit the prompts and responses used as a PDF file. You are also responsible for understanding the solution and defending it when asked in class.**\n",
        "\n",
        "3. For each question, fill in the answer in the cell(s) right below it. The answer could be code or text. You can add as many cells as you need for clarity.\n",
        "\n",
        "4. Enter your BUID (only numerical part) below.\n",
        "\n",
        "5. **Your submission on Blackboard should be the downloaded notebook (i.e., ipynb file). It should be prepopulated with your solution (i.e., the TA and/or instructor need not rerun the notebook to inspect the output). The code, when executed by the TA and/or instructor, should run with no runtime errors.**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part 1: Pre-class Work"
      ],
      "metadata": {
        "id": "Q-jIbvt_YEaY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Setup"
      ],
      "metadata": {
        "id": "_ic8VY0SNXX5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdoGs8-UeOab"
      },
      "outputs": [],
      "source": [
        "BUID = 123456 #e.g., 123456 ONLY NUMERICAL PART"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ssLsZbOeOac"
      },
      "source": [
        " Machine learning is generally stochastic, meaning you get different results for different runs. To avoid that, you can \"seed\" your code. This code uses your BU id (only the numeric part) as a seed for all random number generators."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3djGT1QeOac"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Set a seed for the built-in Python random module\n",
        "random.seed(BUID)\n",
        "# Set a seed for NumPy\n",
        "np.random.seed(BUID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvC7SmWmeOai"
      },
      "source": [
        "##1.2 Language Modeling with N-grams\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's focus on language modeling now! In this section, you will create some n-grams and experiment with how they work."
      ],
      "metadata": {
        "id": "pr3sgwa5OtcO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esPIIC-weOai"
      },
      "source": [
        "### 1.2.1 Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use `nltk` to a 2-gram (i.e., bigram). Extract the bigrams in the sentence\n",
        "\n",
        "> This is a sample sentence."
      ],
      "metadata": {
        "id": "SiGfY9njP5ju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "# Ensure you have the tokenizers\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRU3RAxKQ2Yj",
        "outputId": "d60e2052-be03-47c1-d575-4e4d65763d6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAuBPTDGeOai"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "\n",
        "sentence = \"This is a sample sentence.\"\n",
        "\n",
        "n = 2\n",
        "\n",
        "### Get the tokens of the sentence.\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "\n",
        "### Create the bigram model\n",
        "bigrams = list(ngrams(tokens, n))\n",
        "\n",
        "\n",
        "# Print the bigrams\n",
        "print(bigrams)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.2 Creating an N-gram Based on a Text Corpus."
      ],
      "metadata": {
        "id": "-uckwPbWRGqW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c60TrKT_eOai"
      },
      "source": [
        "Using [this functionality](https://www.nltk.org/api/nltk.lm.api.html) in `nltk`, Create a bigram based on the following dataset of sentences:\n",
        "\n",
        "        - to be or not to be. that is the question!\n",
        "        - ask not what your country can do for you. Ask what you can do for your country.\n",
        "        - is this the real life? is this just fantasy?\n",
        "\n",
        "1. Show the bigram you have constructed (i.e., the dictionary).\n",
        "2. Generate 10 new sentences. What do you notice about these sentences? Explain what's interesting about your observation(s)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\n",
        "    \"To be or not to be. that is the question!\",\n",
        "    \"Ask not what your country can do for you. Ask what you can do for your country.\",\n",
        "    \"Is this the real life? is this just fantasy?\"\n",
        "]"
      ],
      "metadata": {
        "id": "dLNOiY_ekZNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.lm import MLE\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "\n",
        "### tokenize the sentences and put them in a list.\n",
        "tokenized_text =\n",
        "\n",
        "# prepare the data using padded_everygram_pipeline\n",
        "train_data, padded_sents = padded_everygram_pipeline(n, tokenized_text)\n",
        "\n",
        "### train the model\n",
        "\n"
      ],
      "metadata": {
        "id": "lqCgtBuqfRG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's print the frequencies of the different n-grams."
      ],
      "metadata": {
        "id": "Wfduh4HZsASl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "ngrams_freq = Counter()\n",
        "\n",
        "train_data, padded_sents = padded_everygram_pipeline(n, tokenized_text)\n",
        "\n",
        "for ngram in train_data:\n",
        "    ngrams_freq.update(ngram)\n",
        "\n",
        "# Display n-grams and their frequencies\n",
        "print(\"N-grams and their frequencies:\")\n",
        "for ngram, freq in ngrams_freq.items():\n",
        "    print(f\"{ngram}: {freq}\")"
      ],
      "metadata": {
        "id": "q1VzBnodt-UD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's generate some sentences with these n-grams!"
      ],
      "metadata": {
        "id": "y9sgN5iGuT8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sentence(model, num_words=15, start=\"to\"):\n",
        "    content = []\n",
        "    current_word = start\n",
        "\n",
        "    for _ in range(num_words):\n",
        "        ### update the prefix\n",
        "\n",
        "\n",
        "\n",
        "        ### Generate the next word based on the last n-1 words\n",
        "\n",
        "\n",
        "\n",
        "        # if \"end of sentence\" is reached, exit\n",
        "        if current_word == '</s>':\n",
        "            break\n",
        "\n",
        "    # Convert the list of words into a string.\n",
        "    return ' '.join(content)\n"
      ],
      "metadata": {
        "id": "ORRr8RSkl4dy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_sentences = 10\n",
        "start_word = \"Ask\"\n",
        "\n",
        "for i in range(num_sentences):\n",
        "    print(f\"Sentence {i + 1}:\")\n",
        "    print(generate_sentence(model, 15, start=start_word))\n",
        "    print(\"------\")\n",
        "\n"
      ],
      "metadata": {
        "id": "c2qCTXY9l7n0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egGRYKozeOaj"
      },
      "source": [
        "**Answers**\n",
        "\n",
        "*Add answer here.*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: In-class Work"
      ],
      "metadata": {
        "id": "xjbumvp9ioP1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13YsrzUMeOaj"
      },
      "source": [
        "Now, you will read the file `https://raw.githubusercontent.com/elhamod/IS883/main/Assignments/Week1/IS883_Week1_bustlingcity.txt`. You will create multiple __n-grams__, where n ={2, 3, 4, 5, 10}. You will then, using each n-gram, generate a text of similar length to the original file.\n",
        "\n",
        "1. Compare the different generated texts. What observations do you make? Explain your observations with examples. __(0.5 points)__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOUoGt7PeOaj",
        "outputId": "fdf01dba-42ac-4928-bcf8-b44f4092500a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original text:\n",
            "In the heart of the bustling city, there is a park. The park is not just any park; it is a park of dreams. Dreams that come alive every morning as people gather here, each with their unique aspirations and stories. \n",
            "\n",
            "Under the shade of an ancient oak tree, children play. Their laughter and innocent chatter fill the air. They chase butterflies, imagining they are on an epic adventure. To them, the park is a playground of infinite possibilities.\n",
            "\n",
            "Near the serene pond, a young writer sits on a weathered bench. The park is her refuge, a place where she finds inspiration among the dancing ripples of the water. She observes the ducks and swans, penning down verses that capture the essence of the natural world.\n",
            "\n",
            "A group of elderly folks assembles by the chess tables, eager for their daily match. Here, the park transforms into a battlefield of strategic thinking and camaraderie. Every move is a calculated decision, and every game tells a different story.\n",
            "\n",
            "As the sun sets, the park undergoes yet another transformation. Couples stroll hand in hand, sharing whispered secrets and stolen kisses. To them, the park is a sanctuary of love and romance.\n",
            "\n",
            "Late at night, a lone musician takes center stage on the open-air amphitheater. His guitar strums and soulful voice echo through the empty park. The park becomes an intimate concert hall, resonating with the melodies of his songs.\n",
            "\n",
            "Through the day and night, the park reveals its multifaceted nature. It is a canvas upon which countless stories are painted, a tapestry woven with the threads of joy, creativity, competition, and love.\n",
            "\n",
            "The park is a testament to the enduring power of a single word – \"park,\" which, in its different contexts, conjures a kaleidoscope of emotions and experiences.\n",
            "\n",
            "--------\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "## Read the text\n",
        "url = \"https://raw.githubusercontent.com/elhamod/IS883/main/Assignments/Week1/IS883_Week1_bustlingcity.txt\"\n",
        "response = requests.get(url)\n",
        "text_content = response.text\n",
        "\n",
        "# print the original text\n",
        "print(\"original text:\")\n",
        "print(text_content)\n",
        "print(\"--------\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for n in [2, 3, 4, 5, 10]:\n",
        "  print(\"n =\", n)\n",
        "\n",
        "  ### tokenize the sentences\n",
        "\n",
        "  # prepare the data using padded_everygram_pipeline\n",
        "  train_data, padded_sents = padded_everygram_pipeline(n, [tokenized_text])\n",
        "\n",
        "  ### train the model\n",
        "\n",
        "  start_word = \"<s>\" # start a new sentence\n",
        "  generated_text = generate_sentence(model, len(tokenized_text), start=start_word)\n",
        "  print(generated_text)\n",
        "  print(\"----------\")"
      ],
      "metadata": {
        "id": "KktJ4FqZt-gA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0z4sI4zGeOaj"
      },
      "source": [
        "**Answer**\n",
        "\n",
        "*Leave answer here.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3: Homework"
      ],
      "metadata": {
        "id": "JDE_Vv7x1S2f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.1 Right-to-Left Language Modeling"
      ],
      "metadata": {
        "id": "WGJEUwiGnZ_b"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypz0Q4_beOak"
      },
      "source": [
        "1. Now, that you have experimented with ngrams, construct a __\"reveresed n-gram\"__. Meaning, you will construct n-grams that take right-to-left context (i.e., start with the last word and predict backwards).   __(10 points)__\n",
        "\n",
        "2. How does the quality of the reverse-generated text compare to that generated using vanilla n-grams in 2.1? Comment and explain with examples.  __(10 points)__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIWzcFi6eOak"
      },
      "outputs": [],
      "source": [
        "print(\"original text:\")\n",
        "print(text_content)\n",
        "print(\"--------\")\n",
        "\n",
        "for n in [2,3,4,5,10]:\n",
        "  print(\"n =\", n)\n",
        "\n",
        "  ### tokenize the \"reversed\" sentences\n",
        "\n",
        "\n",
        "  # prepare the data using padded_everygram_pipeline\n",
        "  train_data, padded_sents = padded_everygram_pipeline(n, [reversed_tokenized_text])\n",
        "\n",
        "  ### train the model\n",
        "\n",
        "\n",
        "  # print\n",
        "  start_word = \"in\"\n",
        "  generated_text = generate_sentence(model, len(tokenized_text), start=start_word)\n",
        "  print(\" \".join(generated_text.split(\" \")[::-1]))\n",
        "  print(\"----------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5CSc2lQeOal"
      },
      "source": [
        "**Answer**\n",
        "\n",
        "- The text is still legible, meaning that left-to-right context is particular to humans and that for a machine, both directions are fine (i.e., useful context can be on both sides of the word).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnoX25SneOal"
      },
      "source": [
        "3. Finally, [calculate the perplexity](https://www.nltk.org/api/nltk.lm.api.html) of the following sentences for the original `n in [2,3,4,5]` models in 2.1 (i.e., not including the reverse models). __(10 points)__\n",
        "\n",
        "4. Comment on the results and elaborate on your findings.  __(10 points)__\n",
        "\n",
        " > In the heart of the bustling city,\n",
        "\n",
        " > There is a park. The park is beautiful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0ys8xtkeOal"
      },
      "outputs": [],
      "source": [
        "def calculate_perplexity(text, model, n):\n",
        "  return ###"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for n in [2,3,4,5]:\n",
        "\n",
        "  ### tokenize the \"reversed\" sentences\n",
        "\n",
        "  # prepare the data using padded_everygram_pipeline\n",
        "  train_data, padded_sents = padded_everygram_pipeline(n, [tokenized_text])\n",
        "\n",
        "  ### Create and train the model\n",
        "\n",
        "\n",
        "  ### print\n",
        "\n",
        "\n",
        "  print(\"*****\")"
      ],
      "metadata": {
        "id": "sCNT3nPm4WHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NP1v4JOUeOal"
      },
      "source": [
        "**Answer**\n",
        "\n",
        " - Perplexity gets lower and lower as n gets larger, meaning the model is no longer surprised because it is \"copy-pasting\" the training data. So, while perplexity is a good metric to see if a model is underfitting, it is particularly useful for detecting text from the training data.\n",
        "\n",
        " - The second sentence has infinite perplexity because \"beautiful\" never comes up in the original training text."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.2 AI Legal Assistant\n",
        "\n"
      ],
      "metadata": {
        "id": "XJ4-Qld11qM9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to measure how well machine learning could be used for legal assistance, the bar association has hired you to curate a dataset of a large corpora of legal documents for training and testing different machine learning models. Once the dataset is curated [(e.g. this)](https://www.kaggle.com/datasets/anudit/india-legal-cases-dataset), many researchers and practitioners will bid and use the publicized dataset to demonstrate the superiority of their model.\n",
        "\n",
        "1. Can you think of a potential issue with such a practice in terms of model quality? __(5 points)__\n",
        "2. Can you suggest remedies that are easy to implement for such issue(s)? __(5 points)__"
      ],
      "metadata": {
        "id": "yao08u-m2Log"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answers**\n",
        "\n",
        "*Leave answer here.*"
      ],
      "metadata": {
        "id": "eUgs_Uxa1--d"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}