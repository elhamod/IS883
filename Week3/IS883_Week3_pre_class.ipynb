{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elhamod/IS883/blob/main/Week3/IS883_Week3_pre_class.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4FDekTFJuaW"
      },
      "source": [
        "# IS883 Week 3: Advanced Language Modeling\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7KF7bFqKEIG"
      },
      "source": [
        "1. Use Google Colab for this assignment.\n",
        "\n",
        "2. **You are NOT allowed to use ChatGPT for this assignment. However, you may use Google and other online resources. As per the syllabus, you are required to cite your usage. You are also responsible for understanding the solution and defending it when asked in class.**\n",
        "\n",
        "3. For each question, fill in the answer in the cell(s) right below it. The answer could be code or text. You can add as many cells as you need for clarity.\n",
        "\n",
        "4. Enter your BUID (only numerical part) below.\n",
        "\n",
        "5. **Your submission on Blackboard should be the downloaded notebook (i.e., ipynb file). It should be prepopulated with your solution (i.e., the TA and/or instructor need not rerun the notebook to inspect the output). The code, when executed by the TA and/or instructor, should run with no runtime errors.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F95dcB4zKOnw"
      },
      "source": [
        "#Part 1: Pre-class Work"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ic8VY0SNXX5"
      },
      "source": [
        "## 1.1 Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QxJrXiOXgP5"
      },
      "source": [
        "Install some important HuggingFace packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YL8sYzUuXpR8"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdoGs8-UeOab"
      },
      "outputs": [],
      "source": [
        "BUID = 123456 #e.g., 123456 ONLY NUMERICAL PART"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ssLsZbOeOac"
      },
      "source": [
        " Machine learning is generally stochastic, meaning you get different results for different runs. To avoid that, you can \"seed\" your code. This code uses your BU id (only the numeric part) as a seed for all random number generators."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3djGT1QeOac"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import transformers\n",
        "from transformers import set_seed\n",
        "\n",
        "# Set a seed for the built-in Python random module\n",
        "random.seed(BUID)\n",
        "# Set a seed for NumPy\n",
        "np.random.seed(BUID)\n",
        "# Set a seed for HuggingFace\n",
        "set_seed(BUID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAbNAfRIKXrW"
      },
      "source": [
        "## 1.2 Using a Pre-trained GPT Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXzhi8BwId5o"
      },
      "source": [
        "###1.2.1 Complete the Sentence..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEar0ym7MHji"
      },
      "source": [
        "Let's get our feet wet by loading a GPT2 model and using it to generate some text based on a prompt. You may want to refer to [this webpage](https://huggingface.co/openai-community/gpt2) for help. **(10 Points)**\n",
        "\n",
        "- You will generate completions for two prefixes:\n",
        "  - \"Damascus is a\"\n",
        "  - \"Barcelona is a\"\n",
        "- For each prefix, generate 10 completions.\n",
        "- Limit the maximum of length of each completion to 20 *tokens*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7kCqDW26MXCJ"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "### Create a GPT2 generator pipeline\n",
        "\n",
        "\n",
        "### Generate the answer to the question \"Damascus is a\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tNiwlZ9xPYKZ"
      },
      "outputs": [],
      "source": [
        "### Generate the answer to the question \"Barcelona is a\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fj3_U-QZSauf"
      },
      "source": [
        "### 1.2.2 Reflective Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8slB7-AcSjvL"
      },
      "source": [
        "1. Based on our former class discussions and material, what is the explanation for GPT2's ability of generating a diverse set of completions from the same prefix? **(5 Points)**\n",
        "\n",
        "2. What do you notice about the generated texts for the two prompts? Any interesting commonalities or stark differences? What is the underlying explanation for your observations? **(10 Points)**\n",
        "\n",
        "3. Based on our former class discussions and material, what are the underlying set of steps the model is taking to generate the completions, given the prefix? **(5 Points)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbyxZbcVUON_"
      },
      "source": [
        "**Answers**\n",
        "\n",
        "Write your answers below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWtanyk2ODGq"
      },
      "source": [
        "#Part 2: In-class Work"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPpZIA28OFUh"
      },
      "source": [
        "## 2.1 Training a model from scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLvpksYZOMuB"
      },
      "source": [
        "In 1.2, we saw how we could load a pre-trained model and use it to complete sentences. Now, let's see how we could train our own model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjuP6HopOYkf"
      },
      "source": [
        "### 2.1.1 Loading an untrained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-1qiorqOcL2"
      },
      "source": [
        "First, let's load an untrained GPT2 model. Take a look at [this documentation](https://huggingface.co/docs/transformers/en/model_doc/gpt2#transformers.GPT2Config). Generate a maximum of 20 tokens per completion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqcFJpjFOf1h"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2Config, GPT2LMHeadModel, AutoTokenizer\n",
        "\n",
        "### Initializing a GPT2 configuration.\n",
        "\n",
        "\n",
        "### Initializing an untrained model using the configuration.\n",
        "\n",
        "\n",
        "### Load a GPT2 tokenizer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F48z2AOrQQIj"
      },
      "outputs": [],
      "source": [
        "prompt = \"Damascus is a\"\n",
        "\n",
        "### Tokenize the prompt\n",
        "\n",
        "\n",
        "### Generate the completion as tokens\n",
        "\n",
        "### Print the result after converting it back to text.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2GpLP26Tu32"
      },
      "source": [
        "###2.1.2 Reflective questions\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zB1QtqycWplT"
      },
      "source": [
        "- How could you verify that the model is untrained?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SumejL27FzA7"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "Write your answers here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGZO3Ul4ObmG"
      },
      "source": [
        "### 2.1.3 Training the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ghVYxEaT8mM"
      },
      "source": [
        "Now, let's train the model on some corpus about Damascus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hD6aGQeLU7w0"
      },
      "outputs": [],
      "source": [
        "corpus = \"\"\"Damascus, the capital of Syria, is one of the oldest continuously inhabited cities in the world, offering a blend of history, culture, and charm. Its ancient streets are lined with historical landmarks, from the grand Umayyad Mosque to the Citadel, where layers of history from different civilizations can be traced. The Old City, with its narrow alleyways and bustling souks, provides a window into the city's rich past, where traders, artisans, and visitors alike mingle in a timeless setting. The scent of jasmine and citrus trees, which dot the courtyards of traditional Damascene houses, adds to the city's allure, making every corner feel like a step back in time.\n",
        "\n",
        "Beyond its historical significance, Damascus is known for its hospitality and warmth. Locals welcome visitors with open arms, eager to share their stories and offer traditional Syrian delights like shawarma, kibbeh, and baklava. The city's cafés, where people gather over tea and coffee, offer a relaxed atmosphere, making it easy to soak in the daily rhythm of life. From the bustling Hamidiyeh Bazaar to the quieter, tucked-away cafés in the Old City, Damascus offers a unique mix of old and new, where centuries of history coexist with modern life.\n",
        "\n",
        "Damascus is not just a city of the past but one with a thriving, vibrant culture. Its art galleries, music festivals, and poetry readings showcase a lively creative scene that continues to grow, despite the challenges the city has faced. The natural beauty of nearby Mount Qasioun provides a stunning backdrop, especially at sunset when the city is bathed in a golden glow. Whether strolling through its ancient streets or enjoying the breathtaking views, visitors are sure to be captivated by the beauty, resilience, and charm of Damascus.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0A36_q6XLFX"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "\n",
        "### Initializing a untrained model using the configuration.\n",
        "\n",
        "\n",
        "\n",
        "### We need a collator to calculate the loss\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "# This function create a dataset as subsequences of the paragraph\n",
        "def create_subsequences(text, max_length=512):\n",
        "    input_ids = tokenizer(text, return_tensors='pt', add_special_tokens=False).input_ids[0]\n",
        "\n",
        "    sequences = []\n",
        "    for start_idx in range(len(input_ids)):\n",
        "        # Slice the input_ids to create a sequence starting at each token\n",
        "        # sequence = input_ids[start_idx- max_length+1:start_idx+1 ]\n",
        "        sequence = input_ids[start_idx:start_idx+max_length ]\n",
        "        sequences.append({'input_ids': sequence})\n",
        "    return sequences\n",
        "\n",
        "# Create the dataset and tokenize it\n",
        "data = create_subsequences(corpus)\n",
        "tokenized_dataset = Dataset.from_list(data)\n",
        "\n",
        "\n",
        "\n",
        "# Define the training arguments\n",
        "\n",
        "\n",
        "# Set up the Trainer\n",
        "\n",
        "\n",
        "# Train the model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7TPEV2dXDyd"
      },
      "source": [
        "Now, let's generate some completions using the trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FlhhFETfeW4b"
      },
      "outputs": [],
      "source": [
        "prompt = \"Damascus\"\n",
        "\n",
        "### Tokenize the prompt\n",
        "\n",
        "\n",
        "### Set model in evaluation  mode.\n",
        "\n",
        "\n",
        "### Print the result\n",
        "\n",
        "\n",
        "### Convert from tokens back to text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JP8vXlFcdmry"
      },
      "source": [
        "## 2.2 Other LLM Applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZl8YrHPhDqN"
      },
      "source": [
        "Now, that we tried text completion with LLMs, let's try other possible tasks. Generally, if you intend to use a model localy, you would find a model on [HuggingFace](https://huggingface.co/models)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRRbwnO2hZic"
      },
      "source": [
        "###2.2.1 Translation fom English to French."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDsFA0VBfRWR"
      },
      "outputs": [],
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "sentence_to_translate =\n",
        "\n",
        "### Create the pipeline\n",
        "\n",
        "### Use the pipeline\n",
        "\n",
        "\n",
        "### Print the translation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrHx2uFrm2y0"
      },
      "source": [
        "Let's evaluate the model's ability to translate. We will try two different scoring schemes:\n",
        "1. [Bleu score](https://huggingface.co/spaces/evaluate-metric/bleu)\n",
        "2. [Bert score](https://huggingface.co/spaces/evaluate-metric/bertscore)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLorch1dqLFa"
      },
      "outputs": [],
      "source": [
        "!pip install evaluate\n",
        "!pip install rouge_score\n",
        "!pip install bert_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's calculate the blue score."
      ],
      "metadata": {
        "id": "jLBUMub7mDOO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gL2h0iOnm6Pc"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "Google_translation =\n",
        "\n",
        "### the predictions and references\n",
        "predictions =\n",
        "references =\n",
        "\n",
        "### Create a blue evaluator\n",
        "\n",
        "\n",
        "### Calculate the score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's calculate the bert score."
      ],
      "metadata": {
        "id": "20fCydJRmD-6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cvCfyNCoQEj"
      },
      "outputs": [],
      "source": [
        "\n",
        "### the predictions and references\n",
        "predictions =\n",
        "references =\n",
        "\n",
        "### Create a bert evaluator\n",
        "\n",
        "\n",
        "### Calculate the score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reflective Questions:**\n",
        "\n",
        "- Which would you say was a better measure of the translation task. Why?"
      ],
      "metadata": {
        "id": "Qdw9Kw-fmX8w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "\n",
        "The bert score is a better function because it uses semantics rather than simple string matching, leading to a more meaningful translation assessment."
      ],
      "metadata": {
        "id": "QpAQ9B6mmzdl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2VJtVl9hgH5"
      },
      "source": [
        "###2.2.2 Language Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, let's see if we can find a `HuggingFace` model that detects a text's language."
      ],
      "metadata": {
        "id": "HB6dDv-InHii"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2Gcp581hgNy"
      },
      "outputs": [],
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "\n",
        "### Load the model and tokenizer\n",
        "\n",
        "\n",
        "\n",
        "### Tokenize the input\n",
        "\n",
        "\n",
        "### Get the output\n",
        "\n",
        "\n",
        "### Apply softmax to get probabilities\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecrC1JLuOK6d"
      },
      "source": [
        "# Part 3: Homework"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jD1wfWsxZe-j"
      },
      "source": [
        "## 3.1 Evaluate The GPT2 Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQznrnV7ZlDO"
      },
      "source": [
        "After having trained the GPT2 model on your custom paragraph in 2.1.3, can you calculate the perplexity for both the trained and untrained models in 2.1.1 and 2.1.3, respectively, *using the same paragraph*. **(10 Points)**\n",
        "\n",
        "Use this [webpage](https://medium.com/@priyankads/perplexity-of-language-models-41160427ed72) for guidance.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "### Set both trained and untrained models in evaluation mode.\n",
        "\n",
        "\n",
        "### Load a GPT2 tokenizer\n",
        "\n",
        "\n",
        "\n",
        "### Calculate and print the perplexity of the untrained model.\n",
        "\n",
        "\n",
        "\n",
        "### Calculate and print the perplexity of the trained mode,\n",
        "\n"
      ],
      "metadata": {
        "id": "7DUN8pnc-8Dc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b5qxZ5zd-8u"
      },
      "source": [
        "1. How do the perplexities compare? What is the underlying justification for this observation? **(5 Points)**\n",
        "2. *Based on the perplexities you calculated*, can you say one of the models is better than the other *as a general purpose text generator*? Explain your reasoning. **(5 Points)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-Ca8hi-lNzi"
      },
      "source": [
        "**Answer**\n",
        "\n",
        "Write your answers here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Z2Hl4mHEywF"
      },
      "source": [
        "##3.2 Using HuggingFace LLMs for Rating Yelp Reviews"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Brfa1v4Gdmy"
      },
      "source": [
        "The CEO of your company wants to use AI to rate the company's product's Yelp reviews.\n",
        "You are provided with the following [Yelp dataset](https://huggingface.co/datasets/codyburker/yelp_review_sampled).\n",
        "\n",
        "1. You are to decide whether to use a pre-trained model or train your own. What is the basis of your rationale? Show your full work on how you made the decision. **(10 Points)**\n",
        "2. Whether you choose to train your own or use a pre-trained model, what's the accuracy you achieve? **(5 Points)**\n",
        "  - For computational consderations, shuffle the *test* subset of the provided Yelp dataset and then only select 1000 reviews. Also, make sure you [use the GPU device](https://huggingface.co/transformers/v3.0.2/main_classes/pipelines.html) for this task.\n",
        "\n",
        "You may consult the following guides:\n",
        "\n",
        "- [How to evaluate a mode.](https://huggingface.co/docs/evaluate/en/base_evaluator) Note that evaluation may take a few minutes.\n",
        "- [How to take a subset of a dataset](https://huggingface.co/docs/datasets/en/process#sort-shuffle-select-split-and-shard).\n",
        "\n",
        "**Important note**: This question is only concerned with the process of decision making. It is not concerned with actually obtaining a model with high accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqhAPuETGdvY"
      },
      "source": [
        "**Answers:**\n",
        "\n",
        "Write your answers here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-O-D6DeEbvzM"
      },
      "source": [
        "Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xVU7n8TVGeG6"
      },
      "outputs": [],
      "source": [
        "### install the necessary packages\n",
        "!pip install datasets transformers evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmNviGH4MtDq"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "### Load the provided test dataset from the link above.\n",
        "\n",
        "### Shuffle the data and take 1000 reviews from it as your test subset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCrsPlPAbzDt"
      },
      "source": [
        "Load the pre-trained model or train your own."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAy0n7xpKXIe"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "### Getting the pipeline of the model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwot0GQVb3Ko"
      },
      "source": [
        "Evaluate the model on the test set. *(This may take a few moments)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5idjR5EKY-y"
      },
      "outputs": [],
      "source": [
        "from evaluate import evaluator\n",
        "\n",
        "### Load the evaluator\n",
        "\n",
        "\n",
        "### Compute and print the accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ydsx9gd9chvv"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "Write your answers here"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "F95dcB4zKOnw",
        "_ic8VY0SNXX5",
        "jAbNAfRIKXrW",
        "cXzhi8BwId5o",
        "Fj3_U-QZSauf",
        "FWtanyk2ODGq",
        "JPpZIA28OFUh",
        "wjuP6HopOYkf",
        "o2GpLP26Tu32",
        "XGZO3Ul4ObmG",
        "JP8vXlFcdmry",
        "GRRbwnO2hZic",
        "P2VJtVl9hgH5",
        "ecrC1JLuOK6d",
        "jD1wfWsxZe-j",
        "1Z2Hl4mHEywF"
      ],
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}