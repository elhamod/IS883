{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elhamod/IS883/blob/main/Prompt_Engineering_Agents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exploring the power of `LangChain`\n",
        "\n",
        "## Overview\n",
        "This notebook has been created for IS883 at Questrom School of Business - Boston University. It is designed to guide students through using `LangChain` to harness the power of LLMs.\n",
        "\n",
        "### Created By\n",
        "- **Author:** Mohannad Elhamod\n",
        "- **Position:** Clinical Assistant Profressor\n",
        "- **Institution:** Questrom School of Business - Boston University\n",
        "\n",
        "\n",
        "\n",
        "*Note: This notebook is intended for educational purposes and is part of the coursework for IS883. Unauthorized distribution or use is not permitted.*\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyX90VY4i7Y9"
      },
      "source": [
        "# Using LangChain for using language models for different use-cases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QOIueVJiup4"
      },
      "source": [
        "## A recap of using OpenAI API:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfJfAyrYGKl3"
      },
      "source": [
        "Let's remember how we used OpenAI API for chat: [GitHub](https://github.com/elhamod/IS883/blob/main/Open_Api_Guide.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2saCIkn6GlHZ",
        "outputId": "20bcdfa0-0777-4185-c80a-c0b917525f32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/77.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m71.7/77.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed openai-0.28.1\n"
          ]
        }
      ],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jc3H9URqGdGr",
        "outputId": "fe6bde9f-2e90-46ce-bf7c-705dc3b51082"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jq2_v0flj0Mi"
      },
      "source": [
        "Load your OpenAI API key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wG7zOShyGfar"
      },
      "outputs": [],
      "source": [
        "config_ini_location = '/content/drive/MyDrive/Colab Notebooks/IS883/OpenAI guide/config.ini' # Change this to point to the location of your config.ini file.\n",
        "\n",
        "import configparser\n",
        "\n",
        "config = configparser.ConfigParser()\n",
        "config.read(config_ini_location)\n",
        "openai_api_key = config['OpenAI']['API_KEY']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LS8DVfUJj3Rc"
      },
      "source": [
        "Send a prompt and get a response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "si8CKtS6GJ1a",
        "outputId": "e84b811c-650a-449e-f8cc-adabfef26076"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hola, ¿cómo estás?\n"
          ]
        }
      ],
      "source": [
        "import openai\n",
        "\n",
        "# Initialize the OpenAI API with your API key\n",
        "openai.api_key = openai_api_key\n",
        "\n",
        "# You can set up your API key by harcdcoding it here. It is a hacky and bad practice as others will see your secret key clearly and use your account. But, can be used for trying something quick and dirty\n",
        "# openai.api_key = 'YOUR_OPENAI_API_KEY'\n",
        "\n",
        "response = openai.Completion.create(\n",
        "  engine=\"text-davinci-003\",\n",
        "  prompt=\"Translate the following English text to Spanish: 'Hello, how are you?'\",\n",
        "  max_tokens=50\n",
        ")\n",
        "\n",
        "print(response.choices[0].text.strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwQEZ_EZvag5"
      },
      "source": [
        "#Langchain Intro"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jESCVMOwGyDB"
      },
      "source": [
        "Now, let's use another package, [`langchain`](https://python.langchain.com/docs/get_started/introduction), that uses OpenAI API and allows for more advanced capabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "et8bawp-Hl6E",
        "outputId": "dfc1ad64-2c67-4acd-e471-d5cf9bf8a380"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.0.321-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.22)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.6)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.1-py3-none-any.whl (27 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.43 (from langchain)\n",
            "  Downloading langsmith-0.0.49-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.1.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, jsonpointer, typing-inspect, langsmith, jsonpatch, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.6.1 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.321 langsmith-0.0.49 marshmallow-3.20.1 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "y_6zZ1-cHgtM"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKERwcNfvgC5"
      },
      "source": [
        "Let's create the model/chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "6fR78FsAvjqW"
      },
      "outputs": [],
      "source": [
        "chat = ChatOpenAI(openai_api_key=openai_api_key) #temperature=0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9_NAQUtvlUQ"
      },
      "source": [
        "Let's create the prompt \"template\" to translate from English to Spanish.\n",
        "\n",
        "Templates provide a means for standardizing the prompt and improving it *(e.g., through prompt engineering)* by decoupling its format from the parameters *(in this casem the text to be translated and the style of the target translation)*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "82iB5bQBHnsw"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "template_string = \"\"\"Translate the text \\\n",
        "that is delimited by triple backticks \\\n",
        "into a style that is {style}. \\\n",
        "text: ```{text}```\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(template_string)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AO5LXwBQv5Px"
      },
      "source": [
        "Let's define the parameters of the translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "hswaYCstv5Uy"
      },
      "outputs": [],
      "source": [
        "customer_style = \"\"\"formal Spanish\"\"\"\n",
        "\n",
        "customer_email = \"\"\"\n",
        "Hello, how are you?\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nB7RCDMdktel"
      },
      "source": [
        "Now, let's substitute the parameters into the template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhfUv2w8ktnJ",
        "outputId": "b5a4fca3-3852-4c8a-a88c-f86bab20a80a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Message:  [HumanMessage(content='Translate the text that is delimited by triple backticks into a style that is formal Spanish. text: ```\\nHello, how are you?\\n```\\n')]\n"
          ]
        }
      ],
      "source": [
        "customer_messages = prompt_template.format_messages(\n",
        "                    style=customer_style,\n",
        "                    text=customer_email)\n",
        "\n",
        "\n",
        "print(\"Message: \", customer_messages)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPAtcPnzvsjp"
      },
      "source": [
        "Let's query!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMvnL1vvvwuX",
        "outputId": "a667410d-550e-4f07-ff0a-5a49212631b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Response:  Hola, ¿cómo estás?\n"
          ]
        }
      ],
      "source": [
        "customer_response = chat(customer_messages)\n",
        "\n",
        "\n",
        "print(\"Response: \", customer_response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tt0k-GF0wHO5"
      },
      "source": [
        "## Extracting output from response\n",
        "\n",
        "Let's say I would like to extract some information from a travel log I have come across, and save that information as a dictionary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkS-aVTllVaA"
      },
      "source": [
        "Notice below how specific my prompt is:\n",
        "- I was specific about the format of the output.\n",
        "- I was specific about the date format.\n",
        "- I was specific about invalid/unknown price values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "FOV1YRezdvHY"
      },
      "outputs": [],
      "source": [
        "travel_template = \"\"\"\\\n",
        "For the following text, extract the following information:\n",
        "\n",
        "source: Where I am travelling from.\n",
        "\n",
        "destination: Where I am travelling to.\n",
        "\n",
        "airline: Which airline I am travelling with.\n",
        "\n",
        "date: The date of travel in the format mm/dd/yyyy. Make sure the date is within the next 12 months.\n",
        "\n",
        "price: How much I paid for the ticket. If price is not available, return -1\n",
        "\n",
        "Format the output as JSON\n",
        "\n",
        "text: {text}\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(travel_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "dy9KxjigeWhb"
      },
      "outputs": [],
      "source": [
        "essay = \"\"\"\\\n",
        "On Veteran's day, I woke up reluctantly in my bed in Boston. I did not want to go to work.\\\n",
        "I decided to give myself time off and go on a vacation. I went on Expedia and bought a ticket to \\\n",
        "Houston to see my family. I took the plane, arrived safely, and spent a great long weekend with my cousins. \\\n",
        "It was the best $500 I had spent in a while. The only downside is that JetBlue has lost my luggage. :(\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_jO_ZPtwVpj"
      },
      "source": [
        "Query!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSXIaqMrfP-l",
        "outputId": "3734c02d-8f87-4ab8-bfa3-e8c97a5885db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content='{\"source\": \"Boston\", \"destination\": \"Houston\", \"airline\": \"JetBlue\", \"date\": \"11/11/2022\", \"price\": 500}'\n"
          ]
        }
      ],
      "source": [
        "messages = prompt_template.format_messages(text=essay)\n",
        "chat = ChatOpenAI(openai_api_key=openai_api_key)\n",
        "response = chat(messages)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTqiHhsvwXik"
      },
      "source": [
        "Extract the price from this dictionary (i.e., JSON)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3JrvG58gMCc",
        "outputId": "b38651a6-ab52-4427-c966-998274408ddd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "500"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "json_object = json.loads(response.content)\n",
        "json_object['price']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrMcs0PSlyF0"
      },
      "source": [
        "Try to repeat the code above after removing the sentence on the price. See what happens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0j7lxbvwbPM"
      },
      "source": [
        "#Using roles in OpenAI API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JP1UYTYKg7xS"
      },
      "source": [
        "Roles provide context so that the model can answer more specifically and accurately.\n",
        "\n",
        "- System role: defines that overarching context of the conversation.\n",
        "- User role: The human agent.\n",
        "- AI role: The AI bot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "9Sv65YWWwoYg"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts.chat import (\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WM_hX-rQmJ1v"
      },
      "source": [
        "Define the system role here to set the tone and atmosphere of the conversation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "XRDAnJskhf-I"
      },
      "outputs": [],
      "source": [
        "chat = ChatOpenAI(openai_api_key=openai_api_key)\n",
        "\n",
        "# System role\n",
        "template = (\n",
        "    \"to someone in {degree}. Make sure you never over-complicate things for someone of that degree.\"\n",
        "    # \"You are a teacher that is explaining advanced computer science concepts. But, you know nothing about computer science.\"\n",
        ")\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(template)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXGLfU2vmVZC"
      },
      "source": [
        "Now, start the conversation by writing a human message."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "LTXXMu8imUtk"
      },
      "outputs": [],
      "source": [
        "# Human role\n",
        "human_template = \"Explain to me what {concept} is.\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "bGRoEG_ZmeYG"
      },
      "outputs": [],
      "source": [
        "chat_prompt = ChatPromptTemplate.from_messages(\n",
        "    [system_message_prompt, human_message_prompt]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxNxGurIw-Ls",
        "outputId": "ee82dd38-51d4-45d9-f9f3-88853caf0db7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "An RNN, which stands for Recurrent Neural Network, is like a super smart computer brain that can learn patterns and make predictions. It's like having a friend who remembers everything you tell them!\n",
            "\n",
            "Imagine you have a storybook with lots of sentences. When you read one sentence, you can understand it because you remember the sentences that came before. That's how an RNN works too!\n",
            "\n",
            "It takes in information, like words or numbers, one at a time. Then it uses what it has learned from the previous information to understand and remember the current information. This helps it figure out patterns and make predictions about what might come next.\n",
            "\n",
            "So, just like you remember what happened in a story to understand the next part, an RNN remembers what it has seen before to understand and predict what's coming next. It's like a really clever memory machine!\n"
          ]
        }
      ],
      "source": [
        "print(chat(\n",
        "    chat_prompt.format_prompt(\n",
        "        degree=\"first grade\", concept=\"RNN\" #\"Computer science graduate program\"\n",
        "    ).to_messages()\n",
        ").content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxNnht2TxKQR"
      },
      "source": [
        "#Having a conversation with AI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "oKGyCjdjmfgi"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts.chat import (\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.prompts import MessagesPlaceholder\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uysklylSnoR_"
      },
      "source": [
        "In order to have a fully-fledged conversation with AI, we will use a `memory`: This object will keep track of the conversation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "YqqC-H4KoBlV"
      },
      "outputs": [],
      "source": [
        "chat = ChatOpenAI(openai_api_key=openai_api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "mUibfx32mKrr"
      },
      "outputs": [],
      "source": [
        "memory = ConversationBufferMemory()\n",
        "\n",
        "# Defines a system message (You can copy-paste this when using System messages with a conversation chain)\n",
        "template =  \"You are my buttler Alfred. You are talking to Batman\"\n",
        "system_message = SystemMessagePromptTemplate.from_template(template=template)\n",
        "PROMPT = PromptTemplate(input_variables=['history', 'input'], template=template + '.\\n\\nCurrent conversation:\\n{history}\\nHuman: {input}\\nAI:')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "4XA5KwQ1oKTn"
      },
      "outputs": [],
      "source": [
        "# Defines a conversation\n",
        "conversation = ConversationChain(\n",
        "    llm=chat,\n",
        "    prompt=PROMPT,\n",
        "    # verbose=True,\n",
        "    memory=memory,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "VLBdFYqrm8SM",
        "outputId": "fe8a4f0f-08c3-4e0a-d8d1-b85eeafe5494"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Your name is Bruce Wayne, sir.'"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation.predict(input=\"What is my name?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "I-jNVNOAnV2l",
        "outputId": "9fdaaac8-eccb-4407-9a1a-4406f28d39b7"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'You are 35 years old, sir.'"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation.predict(input=\"How old am I?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "DKBxG-nSniD1",
        "outputId": "bf1d19f0-84db-4dd5-8c7c-4c92395f61f8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Apologies, sir. You are 39 years old.'"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation.predict(input=\"No, my age is 39\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlLcQ9UfoNv8"
      },
      "source": [
        "You can print the memory buffer to see the conversation history."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTH1F3IWnqER",
        "outputId": "34ddf03e-bf83-42ae-e8d7-b31b66e39a18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Human: What is my name?\n",
            "AI: Your name is Bruce Wayne, sir.\n",
            "Human: How old am I?\n",
            "AI: You are 35 years old, sir.\n",
            "Human: No, my age is 39\n",
            "AI: Apologies, sir. You are 39 years old.\n"
          ]
        }
      ],
      "source": [
        "print(memory.buffer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p24Ol4ItoTuM"
      },
      "source": [
        "Check if the AI kept track of the updated age"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "RMk-gflH4KSk",
        "outputId": "3aa1ee40-d090-4cec-a773-ce49d2daa2ed"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'You were born on February 19, 1982, sir.'"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation.predict(input=\"When was I born?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "cUjoJRzh4TCy",
        "outputId": "24c171ee-7357-4270-e000-5ab347cfdf7f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Your son is Damian Wayne, sir.'"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation.predict(input=\"Who is my son?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNn-685i45Dd",
        "outputId": "9105f965-20dd-4be8-9925-25ceeb906b20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Human: What is my name?\n",
            "AI: Your name is Bruce Wayne, sir.\n",
            "Human: How old am I?\n",
            "AI: You are 35 years old, sir.\n",
            "Human: No, my age is 39\n",
            "AI: Apologies, sir. You are 39 years old.\n",
            "Human: When was I born?\n",
            "AI: You were born on February 19, 1982, sir.\n",
            "Human: Who is my son?\n",
            "AI: Your son is Damian Wayne, sir.\n"
          ]
        }
      ],
      "source": [
        "print(memory.buffer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sifaLJVJxfqq"
      },
      "source": [
        "For more information (e.g., number of tokens and cost of API call), you could wrap the call with `get_openai_callback()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxiP6DXj5wxT",
        "outputId": "1e022460-55ad-4ae8-dfbd-5eec22374ac6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I am currently in the Batcave, sir.\n",
            "Tokens Used: 142\n",
            "\tPrompt Tokens: 131\n",
            "\tCompletion Tokens: 11\n",
            "Successful Requests: 1\n",
            "Total Cost (USD): $0.0002185\n"
          ]
        }
      ],
      "source": [
        "from langchain.callbacks import get_openai_callback\n",
        "\n",
        "with get_openai_callback() as cb:\n",
        "    print(conversation.predict(input=\"Where are you now?\"))\n",
        "    print(cb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymiXPhFhq1-Y"
      },
      "source": [
        "Notice that I can request a specific reponse length. The longer the request/response, the more cost is incurred."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRouvFbx6O2W",
        "outputId": "a9dd7708-66f4-443d-c628-6e8055befc40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In Gotham's shadowed streets they tread,\n",
            "A hero and a villain, their paths entwined.\n",
            "One fights for justice, the other for chaos,\n",
            "Their destinies forever intertwined.\n",
            "\n",
            "Batman, the Dark Knight, a symbol of hope,\n",
            "A guardian of justice, with a heart of steel.\n",
            "He prowls the night, his cape billowing,\n",
            "His mission to protect and to reveal.\n",
            "\n",
            "Joker, the Clown Prince of Crime, a twisted soul,\n",
            "With laughter as his weapon, and chaos as his goal.\n",
            "He dances with madness, a macabre delight,\n",
            "Leaving a trail of destruction in the dead of night.\n",
            "\n",
            "Their battles are legendary, a never-ending dance,\n",
            "A clash of ideologies, a fight to the death.\n",
            "Batman, the protector, with gadgets and skill,\n",
            "Joker, the agent of chaos, with a maniacal thrill.\n",
            "\n",
            "Through Gotham's alleys, they chase and they fight,\n",
            "Each encounter a symphony of darkness and light.\n",
            "The Batmobile roars, as the Joker's laughter echoes,\n",
            "A never-ending game, where neither can let go.\n",
            "\n",
            "In Arkham Asylum, their destinies collide,\n",
            "A twisted reflection, on opposite sides.\n",
            "Batman's resolve tested, his limits pushed,\n",
            "Joker's mind unraveled, his sanity crushed.\n",
            "\n",
            "Yet in the depths of their eternal war,\n",
            "A connection forms, a bond forevermore.\n",
            "For they are two sides of the same tarnished coin,\n",
            "In a city that thrives on chaos and join.\n",
            "\n",
            "So the adventures continue, day after day,\n",
            "Batman and Joker, forever locked in play.\n",
            "In a battle of wits, of strength, and of might,\n",
            "A never-ending dance in the eternal night.\n",
            "\n",
            "But through it all, Batman remains true,\n",
            "A symbol of hope, for Gotham and for you.\n",
            "And Joker, though twisted, a reminder of choice,\n",
            "To embrace the darkness, or to find a new voice.\n",
            "\n",
            "In Gotham's shadows, their legends will endure,\n",
            "The adventures of Batman and Joker, forever pure.\n",
            "For in their clash, we find reflections of our own,\n",
            "Both hero and villain, forever known.\n",
            "Tokens Used: 583\n",
            "\tPrompt Tokens: 161\n",
            "\tCompletion Tokens: 422\n",
            "Successful Requests: 1\n",
            "Total Cost (USD): $0.0010855\n"
          ]
        }
      ],
      "source": [
        "with get_openai_callback() as cb:\n",
        "    print(conversation.predict(input=\"Write a poem about the adventures of Batman and Joker in 400 words\"))\n",
        "    print(cb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_I9P_Jj2631-"
      },
      "source": [
        "Sometimes, I want to limit the context of the conversation to the last 3 exchanges (e.g., the conversation may get long and the earlier parts maybe irrelevant and might confuse the AI)\n",
        "\n",
        "Thus, I want to limit the buffer to a window of 3 exchanges?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "RZ4WZEq77QcY"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "Qgq-sVmMr4TM"
      },
      "outputs": [],
      "source": [
        "max_number_of_exchanges=2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "VbAcjmBZ6_vN"
      },
      "outputs": [],
      "source": [
        "chat = ChatOpenAI(openai_api_key=openai_api_key)\n",
        "\n",
        "memory = ConversationBufferWindowMemory(k=max_number_of_exchanges)\n",
        "\n",
        "# Set the system message\n",
        "template =  \"You are my buttler Alfred. You are talking to Batman\"\n",
        "system_message = SystemMessagePromptTemplate.from_template(template=template)\n",
        "PROMPT = PromptTemplate(input_variables=['history', 'input'], template=template + '.\\n\\nCurrent conversation:\\n{history}\\nHuman: {input}\\nAI:')\n",
        "\n",
        "conversation = ConversationChain(\n",
        "    llm=chat,\n",
        "    prompt=PROMPT,\n",
        "    verbose=True,\n",
        "    memory=memory,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nH8C-HS57Rgx",
        "outputId": "d4185422-8239-40d7-a4f6-2abf332ad7ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou are my buttler Alfred. You are talking to Batman.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: What's my name?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Your name is Batman, sir.\n"
          ]
        }
      ],
      "source": [
        "print(conversation.predict(input=\"What's my name?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfRd2ACc7Xom",
        "outputId": "622fba8b-6aa1-4044-e040-0b519060e675"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou are my buttler Alfred. You are talking to Batman.\n",
            "\n",
            "Current conversation:\n",
            "Human: What's my name?\n",
            "AI: Your name is Batman, sir.\n",
            "Human: Where am I?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "You are in the Batcave, sir.\n"
          ]
        }
      ],
      "source": [
        "print(conversation.predict(input=\"Where am I?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NG_iwdIh7tcV",
        "outputId": "df3e91c0-3b54-415c-ecb9-d62a4fe8a738"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou are my buttler Alfred. You are talking to Batman.\n",
            "\n",
            "Current conversation:\n",
            "Human: What's my name?\n",
            "AI: Your name is Batman, sir.\n",
            "Human: Where am I?\n",
            "AI: You are in the Batcave, sir.\n",
            "Human: I am now in Metropolis.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Apologies, sir. You are currently in Metropolis.\n"
          ]
        }
      ],
      "source": [
        "print(conversation.predict(input=\"I am now in Metropolis.\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awGErgPB7fAE",
        "outputId": "79aaf083-af82-4556-c1c3-84732387bb66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou are my buttler Alfred. You are talking to Batman.\n",
            "\n",
            "Current conversation:\n",
            "Human: Where am I?\n",
            "AI: You are in the Batcave, sir.\n",
            "Human: I am now in Metropolis.\n",
            "AI: Apologies, sir. You are currently in Metropolis.\n",
            "Human: Can you send me Robin?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "I'm sorry, sir, but Robin is currently unavailable. He is assisting Batman in Gotham City.\n"
          ]
        }
      ],
      "source": [
        "print(conversation.predict(input=\"Can you send me Robin?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hq3sSTHQ1aFf",
        "outputId": "9d1f12b7-146a-4e74-c7ee-c113bb80baa7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou are my buttler Alfred. You are talking to Batman.\n",
            "\n",
            "Current conversation:\n",
            "Human: I am now in Metropolis.\n",
            "AI: Apologies, sir. You are currently in Metropolis.\n",
            "Human: Can you send me Robin?\n",
            "AI: I'm sorry, sir, but Robin is currently unavailable. He is assisting Batman in Gotham City.\n",
            "Human: What's for dinner tomorrow?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Shall I prepare your favorite beef Wellington, sir?\n"
          ]
        }
      ],
      "source": [
        "print(conversation.predict(input=\"What's for dinner tomorrow?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IiYliURG7obF",
        "outputId": "382e7b74-6c5c-46e1-8b90-5efe428dd229"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou are my buttler Alfred. You are talking to Batman.\n",
            "\n",
            "Current conversation:\n",
            "Human: Can you send me Robin?\n",
            "AI: I'm sorry, sir, but Robin is currently unavailable. He is assisting Batman in Gotham City.\n",
            "Human: What's for dinner tomorrow?\n",
            "AI: Shall I prepare your favorite beef Wellington, sir?\n",
            "Human: Where am I?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "You are currently in Wayne Manor, sir.\n"
          ]
        }
      ],
      "source": [
        "print(conversation.predict(input=\"Where am I?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSLuH28d8p3Y"
      },
      "source": [
        "##Sequential chains\n",
        "\n",
        "At times, answering complex questions requires breaking them down into a sequence of steps (e.g., an algorithm for solving a mathematical problem)\n",
        "\n",
        "We can do this by *chaining* questions (i.e., The output of a question will be the input of another question)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMKux8KrZjJC"
      },
      "source": [
        "![picture](https://miro.medium.com/v2/resize:fit:828/format:webp/1*hdx24fJuQwWm1fT-ULGQhg.jpeg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "06sz9AqQ8sAV"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import SimpleSequentialChain\n",
        "from langchain.chains import LLMChain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xIRjm8gtVP7"
      },
      "source": [
        "In  this example, let's\n",
        "\n",
        "\n",
        "1.   Get the names of the 10 countires within a continent.\n",
        "2.   Sort those countries by area."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "yHWGrBkhtR3S"
      },
      "outputs": [],
      "source": [
        "chat = ChatOpenAI(openai_api_key=openai_api_key, temperature=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sl45PyV7tTFQ"
      },
      "source": [
        "The first chain: Get the names of the 10 countires within a continent.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "sW_5TUOHA2CM"
      },
      "outputs": [],
      "source": [
        "first_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Give me the names of 10 different countries in {input}\"\n",
        ")\n",
        "\n",
        "chain_one = LLMChain(llm=chat, prompt=first_prompt, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biG7I5L8twGb"
      },
      "source": [
        "The second chain: Sort those countries by area."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "IvZMztPhA3Qp"
      },
      "outputs": [],
      "source": [
        "second_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Order the names in the following list by descending order by country area size: {list}\"\n",
        ")\n",
        "\n",
        "chain_two = LLMChain(llm=chat, prompt=second_prompt, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgJu_XhLt1K6"
      },
      "source": [
        "Let's put the chains together, substitute into the template, and run the query!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 868
        },
        "id": "e1foO4-XA4vR",
        "outputId": "46dbfd27-1e45-41c9-98e4-15743a58c57c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mHuman: Give me the names of 10 different countries in North America\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[36;1m\u001b[1;3m1. United States\n",
            "2. Canada\n",
            "3. Mexico\n",
            "4. Guatemala\n",
            "5. Belize\n",
            "6. Honduras\n",
            "7. El Salvador\n",
            "8. Costa Rica\n",
            "9. Nicaragua\n",
            "10. Panama\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mHuman: Order the names in the following list by descending order by country area size: 1. United States\n",
            "2. Canada\n",
            "3. Mexico\n",
            "4. Guatemala\n",
            "5. Belize\n",
            "6. Honduras\n",
            "7. El Salvador\n",
            "8. Costa Rica\n",
            "9. Nicaragua\n",
            "10. Panama\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[33;1m\u001b[1;3m1. Canada\n",
            "2. United States\n",
            "3. Mexico\n",
            "4. Guatemala\n",
            "5. Honduras\n",
            "6. Nicaragua\n",
            "7. Costa Rica\n",
            "8. Panama\n",
            "9. El Salvador\n",
            "10. Belize\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1. Canada\\n2. United States\\n3. Mexico\\n4. Guatemala\\n5. Honduras\\n6. Nicaragua\\n7. Costa Rica\\n8. Panama\\n9. El Salvador\\n10. Belize'"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "overall_simple_chain = SimpleSequentialChain(chains=[chain_one, chain_two],\n",
        "                                             verbose=True\n",
        "                                            )\n",
        "continent = \"North America\"\n",
        "overall_simple_chain.run(continent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsP6LK1FDEVf"
      },
      "source": [
        "Here is a somewhat more complicated chain. Consider the following math problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "Uw4B9wCZDD5o"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import SequentialChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "AFzQgIK0E5rd"
      },
      "outputs": [],
      "source": [
        "chat = ChatOpenAI(openai_api_key=openai_api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "dc3z_brIFHW4"
      },
      "outputs": [],
      "source": [
        "problem = \"\\\n",
        "John has some amount of apples, Sarah has double that amount, \\\n",
        "and Mohannad has 3 apples. If they altogether have 12 apples, how many does John have?\\\n",
        "\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QyhL_NSuRVz"
      },
      "source": [
        "First step: convert the problem into an equation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "5LIC1NhmE2KH"
      },
      "outputs": [],
      "source": [
        "first_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Convert the following problem into an equation in terms of x, where x is the number of apples John has:\"\n",
        "    \"\\n\\n{problem}. Only give an equation\"\n",
        ")\n",
        "chain_one = LLMChain(llm=chat, prompt=first_prompt,\n",
        "                     output_key=\"Equation\", verbose=True\n",
        "                    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paABWAv7uWvS"
      },
      "source": [
        "Second step: solve the equation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "q1zKvpixBqWz"
      },
      "outputs": [],
      "source": [
        "second_prompt = ChatPromptTemplate.from_template(\n",
        "    \"After solving the following equation in terms of x:\"\n",
        "    \"\\n\\n{Equation}, only provide the response as `x=`\"\n",
        ")\n",
        "chain_two = LLMChain(llm=chat, prompt=second_prompt,\n",
        "                     output_key=\"Solution\", verbose=True\n",
        "                    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bniX8XgQubgW"
      },
      "source": [
        "Third step: Narrate the solution in a language appropriate to a 6 year-old"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "702m9_MxG8U_"
      },
      "outputs": [],
      "source": [
        "third_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Now, narrate how you solved this {problem} to a 6 year-old\"\n",
        ")\n",
        "chain_three = LLMChain(llm=chat, prompt=third_prompt,\n",
        "                     output_key=\"narration\"\n",
        "                    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ijM3trJul_3"
      },
      "source": [
        "Put the chains together and run the query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "JXA8fYtMHVR6"
      },
      "outputs": [],
      "source": [
        "overall_chain = SequentialChain(\n",
        "    chains=[chain_one, chain_two, chain_three],\n",
        "    input_variables=[\"problem\"],\n",
        "    output_variables=[\"Equation\", \"Solution\", \"narration\"],\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2XbZDkDuprX"
      },
      "source": [
        "If you are interested in debugging or seeing the details of every single API call, turn debugging on.\n",
        "\n",
        "By turning `langchain.debug` on, you get a lot more information about the API calls and responses. This would be useful when debugging. Otherwise, turn it off."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7dV6BqSH-IO",
        "outputId": "e22a1b53-4233-4c6b-a224-b72a7db58ecd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:SequentialChain] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"problem\": \"John has some amount of apples, Sarah has double that amount, and Mohannad has 3 apples. If they altogether have 12 apples, how many does John have?\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:SequentialChain > 2:chain:LLMChain] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"problem\": \"John has some amount of apples, Sarah has double that amount, and Mohannad has 3 apples. If they altogether have 12 apples, how many does John have?\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:SequentialChain > 2:chain:LLMChain > 3:llm:ChatOpenAI] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"Human: Convert the following problem into an equation in terms of x, where x is the number of apples John has:\\n\\nJohn has some amount of apples, Sarah has double that amount, and Mohannad has 3 apples. If they altogether have 12 apples, how many does John have?. Only give an equation\"\n",
            "  ]\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:SequentialChain > 2:chain:LLMChain > 3:llm:ChatOpenAI] [916ms] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \"x + 2x + 3 = 12\",\n",
            "        \"generation_info\": {\n",
            "          \"finish_reason\": \"stop\"\n",
            "        },\n",
            "        \"message\": {\n",
            "          \"lc\": 1,\n",
            "          \"type\": \"constructor\",\n",
            "          \"id\": [\n",
            "            \"langchain\",\n",
            "            \"schema\",\n",
            "            \"messages\",\n",
            "            \"AIMessage\"\n",
            "          ],\n",
            "          \"kwargs\": {\n",
            "            \"content\": \"x + 2x + 3 = 12\",\n",
            "            \"additional_kwargs\": {}\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": {\n",
            "    \"token_usage\": {\n",
            "      \"prompt_tokens\": 69,\n",
            "      \"completion_tokens\": 11,\n",
            "      \"total_tokens\": 80\n",
            "    },\n",
            "    \"model_name\": \"gpt-3.5-turbo\"\n",
            "  },\n",
            "  \"run\": null\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:SequentialChain > 2:chain:LLMChain] [919ms] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"Equation\": \"x + 2x + 3 = 12\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:SequentialChain > 4:chain:LLMChain] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"problem\": \"John has some amount of apples, Sarah has double that amount, and Mohannad has 3 apples. If they altogether have 12 apples, how many does John have?\",\n",
            "  \"Equation\": \"x + 2x + 3 = 12\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:SequentialChain > 4:chain:LLMChain > 5:llm:ChatOpenAI] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"Human: After solving the following equation in terms of x:\\n\\nx + 2x + 3 = 12, only provide the response as `x=`\"\n",
            "  ]\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:SequentialChain > 4:chain:LLMChain > 5:llm:ChatOpenAI] [568ms] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \"x=3\",\n",
            "        \"generation_info\": {\n",
            "          \"finish_reason\": \"stop\"\n",
            "        },\n",
            "        \"message\": {\n",
            "          \"lc\": 1,\n",
            "          \"type\": \"constructor\",\n",
            "          \"id\": [\n",
            "            \"langchain\",\n",
            "            \"schema\",\n",
            "            \"messages\",\n",
            "            \"AIMessage\"\n",
            "          ],\n",
            "          \"kwargs\": {\n",
            "            \"content\": \"x=3\",\n",
            "            \"additional_kwargs\": {}\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": {\n",
            "    \"token_usage\": {\n",
            "      \"prompt_tokens\": 37,\n",
            "      \"completion_tokens\": 3,\n",
            "      \"total_tokens\": 40\n",
            "    },\n",
            "    \"model_name\": \"gpt-3.5-turbo\"\n",
            "  },\n",
            "  \"run\": null\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:SequentialChain > 4:chain:LLMChain] [571ms] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"Solution\": \"x=3\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:SequentialChain > 6:chain:LLMChain] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"problem\": \"John has some amount of apples, Sarah has double that amount, and Mohannad has 3 apples. If they altogether have 12 apples, how many does John have?\",\n",
            "  \"Equation\": \"x + 2x + 3 = 12\",\n",
            "  \"Solution\": \"x=3\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:SequentialChain > 6:chain:LLMChain > 7:llm:ChatOpenAI] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"Human: Now, narrate how you solved this John has some amount of apples, Sarah has double that amount, and Mohannad has 3 apples. If they altogether have 12 apples, how many does John have? to a 6 year-old\"\n",
            "  ]\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:SequentialChain > 6:chain:LLMChain > 7:llm:ChatOpenAI] [10.72s] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \"Okay, let's solve this problem together! So, we know that John has some apples, Sarah has double that amount, and Mohannad has 3 apples. And when we add up all the apples they have, it's 12 apples in total.\\n\\nNow, let's imagine that John has x number of apples. That means Sarah has double that amount, which is 2 times x. And Mohannad has 3 apples. \\n\\nSo, if we add up all the apples they have, it's John's apples + Sarah's apples + Mohannad's apples, which equals 12 apples.\\n\\nNow, let's write an equation to solve it. It will be like this: \\n\\nx + 2x + 3 = 12\\n\\nNow, we need to solve this equation. Let's start by combining like terms. \\n\\nWe have 3x + 3 = 12.\\n\\nTo get the value of x, we need to isolate it on one side of the equation. So, we can subtract 3 from both sides. \\n\\n3x = 12 - 3, which is 9.\\n\\nNow, to find the value of x, we need to divide both sides by 3. \\n\\nx = 9 ÷ 3, which is 3.\\n\\nSo, John has 3 apples! \\n\\nI hope this helps you understand how we found the answer. If you have any more questions, feel free to ask!\",\n",
            "        \"generation_info\": {\n",
            "          \"finish_reason\": \"stop\"\n",
            "        },\n",
            "        \"message\": {\n",
            "          \"lc\": 1,\n",
            "          \"type\": \"constructor\",\n",
            "          \"id\": [\n",
            "            \"langchain\",\n",
            "            \"schema\",\n",
            "            \"messages\",\n",
            "            \"AIMessage\"\n",
            "          ],\n",
            "          \"kwargs\": {\n",
            "            \"content\": \"Okay, let's solve this problem together! So, we know that John has some apples, Sarah has double that amount, and Mohannad has 3 apples. And when we add up all the apples they have, it's 12 apples in total.\\n\\nNow, let's imagine that John has x number of apples. That means Sarah has double that amount, which is 2 times x. And Mohannad has 3 apples. \\n\\nSo, if we add up all the apples they have, it's John's apples + Sarah's apples + Mohannad's apples, which equals 12 apples.\\n\\nNow, let's write an equation to solve it. It will be like this: \\n\\nx + 2x + 3 = 12\\n\\nNow, we need to solve this equation. Let's start by combining like terms. \\n\\nWe have 3x + 3 = 12.\\n\\nTo get the value of x, we need to isolate it on one side of the equation. So, we can subtract 3 from both sides. \\n\\n3x = 12 - 3, which is 9.\\n\\nNow, to find the value of x, we need to divide both sides by 3. \\n\\nx = 9 ÷ 3, which is 3.\\n\\nSo, John has 3 apples! \\n\\nI hope this helps you understand how we found the answer. If you have any more questions, feel free to ask!\",\n",
            "            \"additional_kwargs\": {}\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": {\n",
            "    \"token_usage\": {\n",
            "      \"prompt_tokens\": 57,\n",
            "      \"completion_tokens\": 297,\n",
            "      \"total_tokens\": 354\n",
            "    },\n",
            "    \"model_name\": \"gpt-3.5-turbo\"\n",
            "  },\n",
            "  \"run\": null\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:SequentialChain > 6:chain:LLMChain] [10.73s] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"narration\": \"Okay, let's solve this problem together! So, we know that John has some apples, Sarah has double that amount, and Mohannad has 3 apples. And when we add up all the apples they have, it's 12 apples in total.\\n\\nNow, let's imagine that John has x number of apples. That means Sarah has double that amount, which is 2 times x. And Mohannad has 3 apples. \\n\\nSo, if we add up all the apples they have, it's John's apples + Sarah's apples + Mohannad's apples, which equals 12 apples.\\n\\nNow, let's write an equation to solve it. It will be like this: \\n\\nx + 2x + 3 = 12\\n\\nNow, we need to solve this equation. Let's start by combining like terms. \\n\\nWe have 3x + 3 = 12.\\n\\nTo get the value of x, we need to isolate it on one side of the equation. So, we can subtract 3 from both sides. \\n\\n3x = 12 - 3, which is 9.\\n\\nNow, to find the value of x, we need to divide both sides by 3. \\n\\nx = 9 ÷ 3, which is 3.\\n\\nSo, John has 3 apples! \\n\\nI hope this helps you understand how we found the answer. If you have any more questions, feel free to ask!\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:SequentialChain] [12.22s] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"Equation\": \"x + 2x + 3 = 12\",\n",
            "  \"Solution\": \"x=3\",\n",
            "  \"narration\": \"Okay, let's solve this problem together! So, we know that John has some apples, Sarah has double that amount, and Mohannad has 3 apples. And when we add up all the apples they have, it's 12 apples in total.\\n\\nNow, let's imagine that John has x number of apples. That means Sarah has double that amount, which is 2 times x. And Mohannad has 3 apples. \\n\\nSo, if we add up all the apples they have, it's John's apples + Sarah's apples + Mohannad's apples, which equals 12 apples.\\n\\nNow, let's write an equation to solve it. It will be like this: \\n\\nx + 2x + 3 = 12\\n\\nNow, we need to solve this equation. Let's start by combining like terms. \\n\\nWe have 3x + 3 = 12.\\n\\nTo get the value of x, we need to isolate it on one side of the equation. So, we can subtract 3 from both sides. \\n\\n3x = 12 - 3, which is 9.\\n\\nNow, to find the value of x, we need to divide both sides by 3. \\n\\nx = 9 ÷ 3, which is 3.\\n\\nSo, John has 3 apples! \\n\\nI hope this helps you understand how we found the answer. If you have any more questions, feel free to ask!\"\n",
            "}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'problem': 'John has some amount of apples, Sarah has double that amount, and Mohannad has 3 apples. If they altogether have 12 apples, how many does John have?',\n",
              " 'Equation': 'x + 2x + 3 = 12',\n",
              " 'Solution': 'x=3',\n",
              " 'narration': \"Okay, let's solve this problem together! So, we know that John has some apples, Sarah has double that amount, and Mohannad has 3 apples. And when we add up all the apples they have, it's 12 apples in total.\\n\\nNow, let's imagine that John has x number of apples. That means Sarah has double that amount, which is 2 times x. And Mohannad has 3 apples. \\n\\nSo, if we add up all the apples they have, it's John's apples + Sarah's apples + Mohannad's apples, which equals 12 apples.\\n\\nNow, let's write an equation to solve it. It will be like this: \\n\\nx + 2x + 3 = 12\\n\\nNow, we need to solve this equation. Let's start by combining like terms. \\n\\nWe have 3x + 3 = 12.\\n\\nTo get the value of x, we need to isolate it on one side of the equation. So, we can subtract 3 from both sides. \\n\\n3x = 12 - 3, which is 9.\\n\\nNow, to find the value of x, we need to divide both sides by 3. \\n\\nx = 9 ÷ 3, which is 3.\\n\\nSo, John has 3 apples! \\n\\nI hope this helps you understand how we found the answer. If you have any more questions, feel free to ask!\"}"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import langchain\n",
        "langchain.debug = True # Useful for debugging the stages of the chain\n",
        "\n",
        "overall_chain(problem)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMk54JuFyRjj"
      },
      "source": [
        "##Interacting with external data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EW9BW2ueN9H7"
      },
      "source": [
        "Let's load a PDF and summarize the first page"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6NXnWeFOA5U",
        "outputId": "2a8ba051-2e5d-45cf-c9c5-94cf501bed87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-3.16.4-py3-none-any.whl (276 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.6/276.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-3.16.4\n"
          ]
        }
      ],
      "source": [
        "!pip install pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "1XuxLj0eu8-I"
      },
      "outputs": [],
      "source": [
        "pdf_path = \"/content/drive/MyDrive/MohannadCV.pdf\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "cr-mqduMN8xM"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(pdf_path)\n",
        "pages = loader.load_and_split() # split the pdf into pages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "fi0hItddOVDg"
      },
      "outputs": [],
      "source": [
        "# get the first page\n",
        "p = pages[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wsVq3K8P5-k",
        "outputId": "0a1eae72-2efb-405c-d8ce-a22bc83d7c32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Read the following page and summarize it in 50 words: Mohannad Elhamod\n",
            "/mobile_phone(206) 902 6177 •/envelopeelhamod@vt.edu •/globewordpress.cs.vt.edu/elhamod\n",
            "/linkedinmohannadelhamod\n",
            "Education\n",
            "Virginia Tech, Blacksburg, VA May 2023\n",
            "Ph.D. in Computer Science\n",
            "GPA: 3.83\n",
            "McGill University, Montreal, QC April 2012\n",
            "M.Eng in Computer Engineering\n",
            "GPA: 3.83\n",
            "Jordan University of Science and Technology, Irbid, Jordan June 2007\n",
            "B.Sc in Computer Engineering\n",
            "GPA: 84.4%. Rated Excellent\n",
            "Research Interests\n",
            "Interested in Knowledge-Guided Machine Learning, and ML interpretability and visualization tools. Involved in\n",
            "interdisciplinary projects that span a variety of scientiﬁc domains, including business, physics, and biology.\n",
            "Research Experience\n",
            "Durability, Damage Tolerance and Reliability Branch, NASA June 2022 — August 2022\n",
            "Research Intern\n",
            "/circle_blankTesting, developing and applying deep learning to demonstrate the use of physics-informed generative adversarial networks\n",
            "to learn and quantify the variability in aerospace materials, ultimately generating an inﬁnite number of virtual tests for\n",
            "certiﬁcation purposes.\n",
            "Science-Guided Machine Learning Lab, Virginia Tech August 2019 — Present\n",
            "Graduate Research Assistant\n",
            "/circle_blankSupervised by Prof. Anuj Karpatne.\n",
            "/circle_blankCoPhy-PGNN: Learning Physics-guided Neural Networks with Competing Loss Functions for Solving Eigenvalue\n",
            "Problems :\n",
            "-Investigating a deep learning approach to the computationally expensive eigen-decomposition numerical solvers, with\n",
            "applications in quantum physics and electromagnetic propagation.\n",
            "/circle_blankGuiding Neural Networks Using Taxonomies and Phylogenies for Species Modelling :\n",
            "-Tackling the problem of species modelling in the paucity of labelled training data by infusing taxonomic and phylogenetic\n",
            "information into model training. Such modeling learns biologically-valid visual features that improve downstream tasks\n",
            "such as classiﬁcation, specimen image generation, and trait discovery.\n",
            "Visual Surveillance Group, McGill University January 2010 — April 2012\n",
            "Graduate Research Assistant\n",
            "/circle_blankSupervised by Prof. Martin D. Levine.\n",
            "/circle_blankThesis: Real-Time Automated Annotation of Surveillance Scenes :\n",
            "-Developing a real-time video surveillance system that detects semantically deﬁned activities of interest.\n",
            "/circle_blankUnconstrained real-time face recognition using face morphing and SVM classiﬁers.\n",
            "Work Experience\n",
            "Microsoft – Azure Monitoring April 2016 – July 2019\n",
            "Software Engineer II\n",
            "/circle_blankApplication Insights/Azure Monitoring team.\n",
            "/circle_blankWorked on Metrics Explorer , a cloud computing feature that allows millions of customers to visualize and analyze their data.\n",
            "/circle_blankLead Azure Monitoring ’s product accessibility compliance eﬀorts to provide a seamless experience for people with disabilities.\n",
            "Microsoft – Microsoft Edge March 2013 – April 2016\n",
            "Software Engineer I\n",
            "/circle_blankWorked on core features on the Windows Phone platform, such as InPrivate experience and tabs center.\n",
            "/circle_blankLead the product’s accessibility compliance eﬀorts to provide a seamless experience for people with disabilities.\n"
          ]
        }
      ],
      "source": [
        "message = \"Read the following page and summarize it in 50 words: \"+ p.page_content\n",
        "print(message)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "RHmXlruyicL_"
      },
      "outputs": [],
      "source": [
        "langchain.debug = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "FKe-uGkJOhpF",
        "outputId": "6d5ed5c8-475d-446e-c6c2-fadd44262b2d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Mohannad Elhamod is a Ph.D. student in Computer Science at Virginia Tech. He has a strong research interest in Knowledge-Guided Machine Learning and ML interpretability and visualization tools. He has worked on various interdisciplinary projects in domains like business, physics, and biology. He has research experience as a Research Intern at NASA and as a Graduate Research Assistant at Virginia Tech and McGill University. He also has work experience as a Software Engineer at Microsoft in the Azure Monitoring and Microsoft Edge teams.'"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat = ChatOpenAI(openai_api_key=openai_api_key)\n",
        "chat.predict(message)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWxhw64MyY04"
      },
      "source": [
        "# Agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lP7dLWB8vITC"
      },
      "source": [
        "In class, we have seen how plugins work. Agents are `LangChain`'s plug-in functionality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYcqL0wzvkxB"
      },
      "source": [
        "Let's see how we can use the wikipedia agent in `LangChain`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4acv-eR5cK6",
        "outputId": "c0690133-7b91-45b8-9329-8558de8700d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.11.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2023.7.22)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.5)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11678 sha256=624380ce6a321432f67ee097ff24c65c9e5c9b1c9bc13b7ed8b8b47c5089b14b\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U wikipedia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmGRJPa9yfU6"
      },
      "source": [
        "Let's ask a question about GPT4 in Wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWqM4Msn3tf-",
        "outputId": "8a56c646-1629-46e4-bde5-3b46e4ae4073"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mI should use Wikipedia to find information about the release date of ChatGPT 4.\n",
            "Action: Wikipedia\n",
            "Action Input: \"ChatGPT 4 release date\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mPage: ChatGPT\n",
            "Summary: ChatGPT, which stands for Chat Generative Pre-trained Transformer, is a large language model-based chatbot developed by OpenAI and launched on November 30, 2022, which enables users to refine and steer a conversation towards a desired length, format, style, level of detail, and language. Successive prompts and replies, known as prompt engineering, are considered at each conversation stage as a context.ChatGPT is built upon either GPT-3.5 or GPT-4—members of OpenAI's proprietary series of generative pre-trained transformer (GPT) models, based on the transformer architecture developed by Google—and is fine-tuned for conversational applications using a combination of supervised and reinforcement learning techniques. ChatGPT was released as a freely available research preview, but due to its popularity, OpenAI now operates the service on a freemium model. It allows users on its free tier to access the GPT-3.5-based version. In contrast, the more advanced GPT-4 based version and priority access to newer features are provided to paid subscribers under the commercial name \"ChatGPT Plus\".\n",
            "By January 2023, it had become what was then the fastest-growing consumer software application in history, gaining over 100 million users and contributing to OpenAI's valuation growing to $29 billion. Within months, Google, Baidu, and Meta accelerated the development of their competing products: Bard, Ernie Bot, and LLaMA. Microsoft launched its Bing Chat based on OpenAI's GPT-4. It raised concern among some observers over the potential of ChatGPT and similar programs to displace or atrophy human intelligence, enable plagiarism, or fuel misinformation.\n",
            "\n",
            "Page: GPT-4\n",
            "Summary: Generative Pre-trained Transformer 4 (GPT-4) is a multimodal large language model created by OpenAI, and the fourth in its series of GPT foundation models. It was initially released on March 14, 2023, and has been made publicly available via the paid chatbot product ChatGPT Plus, and via OpenAI's API.  As a transformer-based model, GPT-4 uses a paradigm where pre-training using both public data and \"data licensed from third-party providers\" is used to predict the next token. After this step, the model was then fine-tuned with reinforcement learning feedback from humans and AI for human alignment and policy compliance.: 2 Observers reported that the iteration of ChatGPT using GPT-4 was an improvement on the previous iteration based on GPT-3.5, with the caveat that GPT-4 retains some of the problems with earlier revisions. GPT-4 is also capable of taking images as input, though this feature has not been made available since launch. OpenAI has declined to reveal various technical details and statistics about GPT-4, such as the precise size of the model.\n",
            "\n",
            "Page: GPT-3\n",
            "Summary: Generative Pre-trained Transformer 3 (GPT-3) is a large language model released by OpenAI in 2020. Like its predecessor GPT-2, it is a decoder-only transformer model of deep neural network, which uses attention in place of previous recurrence- and convolution-based architectures. Attention mechanisms allow the model to selectively focus on segments of input text it predicts to be the most relevant. It uses a 2048-tokens-long context and then-unprecedented size of 175 billion parameters, requiring 800GB to store. The model demonstrated strong zero-shot and few-shot learning on many tasks.Microsoft announced on September 22, 2020, that it had licensed \"exclusive\" use of GPT-3; others can still use the public API to receive output, but only Microsoft has access to GPT-3's underlying model.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI have found the information I need. \n",
            "Final Answer: ChatGPT 4 was released on March 14, 2023.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': 'When was ChatGPT 4 released?',\n",
              " 'output': 'ChatGPT 4 was released on March 14, 2023.'}"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.agents import load_tools, initialize_agent\n",
        "\n",
        "chat = ChatOpenAI(openai_api_key=openai_api_key)\n",
        "tools = load_tools([\"wikipedia\"], llm=chat) #\"llm-math\" is another possible tool for math.\n",
        "\n",
        "agent= initialize_agent(\n",
        "    tools,\n",
        "    chat,\n",
        "    verbose = True)\n",
        "\n",
        "msg = \"When was ChatGPT 4 released?\"\n",
        "\n",
        "agent(msg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLJi9sIiyk3c"
      },
      "source": [
        "What if we don't use Wikipedia agent?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "QAN0cxMl5gSU",
        "outputId": "fe9b0027-5eaa-445e-a5d9-9e72d6553e04"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'As of October 2021, OpenAI has released ChatGPT as ChatGPT Plus and ChatGPT API. However, OpenAI has not released a specific version called ChatGPT 4.'"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.schema import HumanMessage\n",
        "\n",
        "msgs = [HumanMessage(content=msg)]\n",
        "chat(msgs).content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFgoudJeyoPO"
      },
      "source": [
        "## We can also define our own agent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCMTVbeV7nZA"
      },
      "source": [
        "Define your own tool. The descriprion is what really tells the agent which tool to use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "XN12GcJg7tli"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import tool\n",
        "from datetime import date\n",
        "\n",
        "@tool\n",
        "def time(text: str) -> str:\n",
        "    \"\"\"Returns todays date, use this for any \\\n",
        "    questions related to knowing todays date. \\\n",
        "    The input should always be an empty string, \\\n",
        "    and this function will always return todays \\\n",
        "    date - any date mathmatics should occur \\\n",
        "    outside this function.\"\"\" #This is the desciption the agent uses to determine whether to use the time tool.\n",
        "    return str(date.today())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "bNsv5zPP7xFQ"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import AgentType\n",
        "\n",
        "agent= initialize_agent(\n",
        "    [time] + tools,\n",
        "    chat,\n",
        "    handle_parsing_errors=True,\n",
        "    agent=AgentType.OPENAI_FUNCTIONS,\n",
        "    verbose = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlERw2OSwD8p",
        "outputId": "5e8c6b69-f30b-4998-a2c5-2bf67a0ab2dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `time` with `{'text': ''}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m2023-10-24\u001b[0m\u001b[32;1m\u001b[1;3mToday's date is October 24, 2023.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': 'What is the date today?',\n",
              " 'output': \"Today's date is October 24, 2023.\"}"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent(\"What is the date today?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2QT_xuOwGgn",
        "outputId": "ed599cf0-e390-47a6-9be4-726fdc424ca0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `time` with `{'text': ''}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m2023-10-24\u001b[0m\u001b[32;1m\u001b[1;3mThe date yesterday was October 24, 2023.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': 'What was the date yesterday?',\n",
              " 'output': 'The date yesterday was October 24, 2023.'}"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent(\"What was the date yesterday?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ru547H-Z70qV",
        "outputId": "9d199401-3195-460b-ef1e-93156179cc5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mAs an AI, I cannot provide personalized medical advice. However, in general, consuming a date should not cause any harm. Dates are a nutritious fruit and are commonly consumed as a part of a healthy diet. If you have any specific concerns or experience any unusual symptoms, it is recommended to consult a healthcare professional.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': 'I ate a date. Will I be OK?',\n",
              " 'output': 'As an AI, I cannot provide personalized medical advice. However, in general, consuming a date should not cause any harm. Dates are a nutritious fruit and are commonly consumed as a part of a healthy diet. If you have any specific concerns or experience any unusual symptoms, it is recommended to consult a healthcare professional.'}"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent(\"I ate a date. Will I be OK?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z79_3l8KwMHs",
        "outputId": "80ce6731-a8e7-4cec-a304-a5647870d238"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mA good venue for a romantic date in Boston could be the Top of the Hub. It is a restaurant located on the 52nd floor of the Prudential Tower, offering stunning panoramic views of the city. The ambiance is elegant and romantic, and the menu features a variety of delicious dishes. It's a great place to enjoy a romantic dinner with your partner.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': 'What would be a good venue for a romantic date in Boston?',\n",
              " 'output': \"A good venue for a romantic date in Boston could be the Top of the Hub. It is a restaurant located on the 52nd floor of the Prudential Tower, offering stunning panoramic views of the city. The ambiance is elegant and romantic, and the menu features a variety of delicious dishes. It's a great place to enjoy a romantic dinner with your partner.\"}"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent(\"What would be a good venue for a romantic date in Boston?\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPiL2ym4gEn6EvZ2c20IrQR",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
