{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elhamod/IS883/blob/main/Week10/IS883_2024_Agents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IS883 Week10: Tools and Agents"
      ],
      "metadata": {
        "id": "XOBPqcToTqFE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Use Google Colab for this assignment.\n",
        "\n",
        "2. **You are NOT allowed to use external or embedded Gen AI for this assignment (except where specifically instructed). However, you may use Google search and other online resources. As per the syllabus, you are required to cite your usage. You are also responsible for understanding the solution and defending it when asked in class.**\n",
        "\n",
        "3. For each question, fill in the answer in the cell(s) right below it. The answer could be code or text. You can add as many cells as you need for clarity.\n",
        "\n",
        "4. **Your submission on Blackboard should be the downloaded notebook (i.e., ipynb file). It should be prepopulated with your solution (i.e., the TA and/or instructor need not rerun the notebook to inspect the output). The code, when executed by the TA and/or instructor, should run with no runtime errors.**"
      ],
      "metadata": {
        "id": "FoUggC_UTa92"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. In-Class Work"
      ],
      "metadata": {
        "id": "4mBWoSxpUNKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain_community langchain_openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RV7f5V1lX7j9",
        "outputId": "f3c9629f-be56-43ec-c437-bbf3b95aa20d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.7)\n",
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.10/dist-packages (0.3.7)\n",
            "Requirement already satisfied: langchain_openai in /usr/local/lib/python3.10/dist-packages (0.2.8)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.35)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.18)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.2)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.143)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.4.0)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.6.1)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.54.0 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (1.54.4)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (0.8.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.17.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.23.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.15->langchain) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.11)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.54.0->langchain_openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.54.0->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.54.0->langchain_openai) (0.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.54.0->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.54.0->langchain_openai) (4.66.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.9.11)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.54.0->langchain_openai) (1.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Get the OpenAI API key\n",
        "from google.colab import userdata\n",
        "openai_api_key = userdata.get('MyOpenAIKey')"
      ],
      "metadata": {
        "id": "qp-lE1g54XNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Tools\n"
      ],
      "metadata": {
        "id": "QSNXa30eUPQS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.1.1 Using a Calculator for Accurate Computations."
      ],
      "metadata": {
        "id": "tAUWUyNc5498"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try to get an answer to the following finantial problem:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "In your bank account, you have $110,345.45. Because you left them in a saving account with a high annual interest rate of %6.46, how much money will you have in a year?\n",
        "```\n",
        "\n",
        "The correct answer is $110,345.45 * 1.0646 = \\$117473.76607\n"
      ],
      "metadata": {
        "id": "HziH4cJmVvzk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"In your bank account, you have $110,345.45. Because you left them in a saving account with a high annual interest rate of %6.46, how much money will you have in a year?\""
      ],
      "metadata": {
        "id": "Mi99uEnZUfkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from IPython.display import Markdown\n",
        "\n",
        "### Create the chat agent\n",
        "chat = ChatOpenAI(openai_api_key=openai_api_key, model=\"gpt-4o\")\n",
        "\n",
        "### Get the answer\n",
        "display(Markdown(chat.invoke(question).content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "SYxUZwW_E4qE",
        "outputId": "bf6db82a-ae78-4eca-8270-bdc07f35ab7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "To calculate the amount of money you will have in a year with an annual interest rate of 6.46%, you can use the formula for compound interest. Assuming the interest is compounded annually, the formula is:\n\n\\[ A = P(1 + r)^n \\]\n\nWhere:\n- \\( A \\) is the amount of money accumulated after n years, including interest.\n- \\( P \\) is the principal amount (initial investment).\n- \\( r \\) is the annual interest rate (decimal).\n- \\( n \\) is the number of years the money is invested for.\n\nGiven:\n- \\( P = 110,345.45 \\)\n- \\( r = 6.46\\% = 0.0646 \\)\n- \\( n = 1 \\)\n\nPlug these values into the formula:\n\n\\[ A = 110,345.45 \\times (1 + 0.0646)^1 \\]\n\n\\[ A = 110,345.45 \\times 1.0646 \\]\n\n\\[ A \\approx 117,475.78 \\]\n\nSo, you will have approximately $117,475.78 in your account after one year."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:** Is the answer correct? What do you think is the cause for what you observe?"
      ],
      "metadata": {
        "id": "szJ9wuJDYReA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ChatGPT only predicts tokens based on the text it has seen during training. It does not do any true calculations. As such, if this specific example is unlikely to have appeared in  training dataset, then the result is unlikely to be correct."
      ],
      "metadata": {
        "id": "IsAkLQ2eYbgE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to upgrade the LLM's capabilities, we will use [`LLMMathChain`](https://medium.com/data-science-in-your-pocket/mathematics-using-llms-using-langchains-ca23bbd1a38b), which is a chain that uses a calculator tool, into our framework.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZzYTVov4Y3H_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import LLMMathChain\n",
        "from langchain_openai import OpenAI\n",
        "\n",
        "### You can turn on debugging using this code. You will be able to see the intermediate requests and responses.\n",
        "import langchain\n",
        "langchain.debug = False\n",
        "\n",
        "### Create the LLM\n",
        "llm = OpenAI(openai_api_key=openai_api_key)\n",
        "\n",
        "### Wrap the LLM with a chain that has access to the math tool.\n",
        "llm_math = LLMMathChain.from_llm(llm) #, verbose=True\n",
        "\n",
        "### Run the chain\n",
        "llm_math.invoke(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZK48hwR2XDGR",
        "outputId": "306c1d8b-1e2f-4a78-8119-b48684e93eff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'In your bank account, you have $110,345.45. Because you left them in a saving account with a high annual interest rate of %6.46, how much money will you have in a year?',\n",
              " 'answer': 'Answer: 117473.76607'}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:** What are the steps `LangChain` took to obtain the answer this time?\n",
        "\n",
        "*Hint: You may need some diagnostics...*"
      ],
      "metadata": {
        "id": "YZ_BLxcRctJ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The LLM adds a prompt as a prefix to your question:\n",
        "\n",
        "\n",
        "```\n",
        "    \"Translate a math problem into a expression that can be executed using Python's numexpr library. Use the output of running this code to answer the question.\n",
        "    \\n\n",
        "    \\nQuestion: ${Question with math problem.}\n",
        "    \\n```text\n",
        "    \\n${single line mathematical expression that solves the problem}\n",
        "    \\n```\n",
        "    \\n...numexpr.evaluate(text)...\n",
        "    \\n\n",
        "    ```output\n",
        "    \\n${Output of running the code}\n",
        "    \\n```\n",
        "    \\nAnswer: ${Answer}\n",
        "    \\n\n",
        "    \\nBegin.\n",
        "    \\n\n",
        "    \\nQuestion: What is 37593 * 67?\n",
        "    \\n```text\n",
        "    \\n37593 * 67\n",
        "    \\n```\n",
        "    \\n...numexpr.evaluate(\\\"37593 * 67\\\")...\n",
        "    \\n```output\n",
        "    \\n2518731\n",
        "    \\n```\n",
        "    \\nAnswer: 2518731\n",
        "    \\n\n",
        "    \\nQuestion: 37593^(1/5)\n",
        "    \\n```text\\n37593**(1/5)\n",
        "    \\n```\n",
        "    \\n...numexpr.evaluate(\\\"37593**(1/5)\\\")...\n",
        "    \\n```\n",
        "    \\n8.222831614237718\n",
        "    \\n```\n",
        "    \\nAnswer: 8.222831614237718\n",
        "    \\n\n",
        "    \\nQuestion: In your bank account, you have $110,345.45. Because you left them in a saving account with a high annual interest rate of %6.46, how much money will you have in a year?\"\n",
        "```\n",
        "\n",
        "- It obtains the expression:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "text\n",
        "\\n110345.45 * 1.0646\n",
        "\\n```\n",
        "\\n...numexpr.evaluate(\\\"110345.45 * 1.0646\\\")...\n",
        "\\n\n",
        "```\n",
        "\n",
        "- It *executes* the expression as code (not through the LLM itself), and returns the answer."
      ],
      "metadata": {
        "id": "a4gSm9e8ctFe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.1.2 Tool Binding. Use-Case: Websearching Using Google Serper.\n",
        "\n"
      ],
      "metadata": {
        "id": "5foaGFZeA223"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's say we want a smart personal assistant bot that can browse the internet and be helpful.\n",
        "\n",
        "We will use [Google Serper](https://python.langchain.com/docs/integrations/tools/google_serper/), a *free* search tool.\n",
        "\n",
        "`LangChain` has some built-in [tools](https://python.langchain.com/docs/integrations/tools/) and [toolkits](https://python.langchain.com/v0.1/docs/integrations/toolkits/) that provide access to some useful functionality and APIs.\n",
        "\n",
        "The following [link](https://python.langchain.com/docs/how_to/tools_chain/) shows how to use tools in chains."
      ],
      "metadata": {
        "id": "v80I8OcOCsnw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Based on the weather today in Miami, do you think it is a good idea to go on a picnic?\"\n",
        "# question = \"I am at Questrom and hungry. give me 3 good noodle places near me?\"\n",
        "# question = \"What phone number should I call to talk to Mohannad Elhamod at Questrom?\"\n",
        "# question = \"What is 1+1\""
      ],
      "metadata": {
        "id": "8vf1y3molEoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
        "from langchain_core.tools import Tool\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
        "import os\n",
        "\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "        (\"placeholder\", \"{agent_scratchpad}\"), # To be used by the agent for intermediate operations.\n",
        "    ]\n",
        ")\n",
        "\n",
        "chat = ChatOpenAI(openai_api_key=openai_api_key, model=\"gpt-4o-mini\")\n",
        "\n",
        "# Setting up the Serper tool\n",
        "os.environ[\"SERPER_API_KEY\"] = userdata.get('SERPER_API')\n",
        "search = GoogleSerperAPIWrapper()\n",
        "tools = [\n",
        "    Tool(\n",
        "        name=\"GoogleSerper\",\n",
        "        func=search.run,\n",
        "        description=\"Useful for when you need to look up some information on the internet.\",\n",
        "    )\n",
        "]\n",
        "\n",
        "# Defining the agent\n",
        "agent = create_tool_calling_agent(chat, tools, prompt)\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True) #, verbose=True\n",
        "\n",
        "print(\"Vanilla LLM answer:\", chat(question).content)\n",
        "\n",
        "# Run the agent\n",
        "print(\"*****\")\n",
        "print(\"Agent answer:\", agent_executor.invoke({\"input\": question})[\"output\"])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETmxyJvWEqoL",
        "outputId": "163366a4-1e65-4e8b-ff89-64853c0dcc84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-002150cb95e7>:33: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  print(\"Vanilla LLM answer:\", chat(question).content)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vanilla LLM answer: I don't have access to real-time weather data, so I can't provide specific advice based on today's conditions in Miami. However, when considering a picnic, you should check the current temperature, the chance of rain, humidity levels, and wind conditions. A pleasant day with mild temperatures and low humidity is ideal for a picnic. If the forecast indicates rain or extreme heat, it might be better to postpone your plans. Always remember to bring necessary items like sunscreen, water, and insect repellent!\n",
            "*****\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `GoogleSerper` with `Miami weather today`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3m77°F\u001b[0m\u001b[32;1m\u001b[1;3mThe weather in Miami today is 77°F. Generally, this temperature can be pleasant for a picnic, but other factors such as humidity, wind, and the likelihood of rain can also play a significant role. If the weather is sunny and not too humid, it could be a great day for a picnic. Would you like to know more about the specific weather conditions such as humidity or chance of rain?\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Agent answer: The weather in Miami today is 77°F. Generally, this temperature can be pleasant for a picnic, but other factors such as humidity, wind, and the likelihood of rain can also play a significant role. If the weather is sunny and not too humid, it could be a great day for a picnic. Would you like to know more about the specific weather conditions such as humidity or chance of rain?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.1.3 Custom Tool Binding. Use-Case: What's the time?"
      ],
      "metadata": {
        "id": "VYuGSnMVBFWJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's say we want out LLM to be aware of today's date. We want to implement our own date-checking function. This is done using `@tool` decorator.\n",
        "\n",
        "A tool expects some input parameters, if any, and returns some output. Here is some overview of [how tool calling is done](https://python.langchain.com/docs/concepts/tool_calling/#:~:text=A%20key%20principle%20of%20tool,result%20%3D%20llm_with_tools.). Here is a resource on how to build [custom tools](https://python.langchain.com/v0.1/docs/use_cases/tool_use/quickstart/).\n",
        "\n",
        "Reference: [`create_tool_calling_agent`](https://api.python.langchain.com/en/latest/agents/langchain.agents.tool_calling_agent.base.create_tool_calling_agent.html)\n",
        "\n",
        "Reference: [custom tools](https://python.langchain.com/docs/how_to/custom_tools/)"
      ],
      "metadata": {
        "id": "QiSNddrpQk84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What's the date today?\"\n",
        "# question = \"Give a singer who is alive whose birthdate is next month\"\n",
        "# question = \"Give a celebrity whose birthdate is in 6 weeks\"\n",
        "# question = \"When was Michael Jackson born?\"\n",
        "# question = \"What day of the week is in 3 days?\"\n",
        "# question = \"What year is it?\"\n",
        "# question = \"I ate a date. Will I be OK?\"\n",
        "# question = \"What would be a good venue for a romantic date in Boston?\""
      ],
      "metadata": {
        "id": "v7w3f3zhUHNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import AgentExecutor, create_tool_calling_agent, tool\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "import langchain\n",
        "langchain.debug = False\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "        (\"placeholder\", \"{agent_scratchpad}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Defining the tools\n",
        "from datetime import date, timedelta, datetime\n",
        "@tool\n",
        "def datetoday() -> str:\n",
        "    \"\"\"Returns today's date, use this for any \\\n",
        "    questions that need today's date to be answered. \\\n",
        "    This tool takes no argumetns but returns a string with today's date.\"\"\" #This is the desciption the agent uses to determine whether to use the time tool.\n",
        "    return \"Today is \" + str(date.today())\n",
        "\n",
        "tools = [datetoday]\n",
        "\n",
        "\n",
        "# Defining the agent\n",
        "agent = create_tool_calling_agent(chat, tools, prompt)\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True) #\n",
        "\n",
        "\n",
        "print(\"Vanilla LLM answer:\", chat([HumanMessage(content=question)]).content)\n",
        "\n",
        "# # Run the agent\n",
        "print(\"*****\")\n",
        "print(\"Agent answer:\", agent_executor.invoke({\"input\": question})[\"output\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rwoca7-L91Z",
        "outputId": "9aa4dcb2-039c-4568-d6f2-ae75693352a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vanilla LLM answer: Today's date is October 3, 2023.\n",
            "*****\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m\n",
            "Invoking: `datetoday` with `{}`\n",
            "\n",
            "\n",
            "\u001b[0m\u001b[36;1m\u001b[1;3mToday is 2024-11-18\u001b[0m\u001b[32;1m\u001b[1;3mToday is November 18, 2024.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Agent answer: Today is November 18, 2024.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.2 Chain of Thought (CoT)"
      ],
      "metadata": {
        "id": "7rCYFJ4szjl8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we have seen in class before, breaking down a complex task into steps is useful as it generates intermediate steps, yielding a better and more accurate final answer.\n",
        "\n",
        "Here, we will take this idea further and apply *Chain of Thought (CoT)*, a prompt engineering technique that asks the model to \"reason\" through its answer by providing intermediate outputs.\n",
        "\n",
        "You may read this [reference](https://www.promptingguide.ai/techniques/cot) for more information."
      ],
      "metadata": {
        "id": "5Q5m9k-yFJeA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.2.1 Do Objects Float. Yes or No?"
      ],
      "metadata": {
        "id": "SFgXNIesXmDF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Would an avocado float in medical alcohol?\n",
        "\n",
        "The answer lies in object densities:\n",
        "* ethanol (grain alcohol):\t0.810\n",
        "*  [Avocado density](https://www.sciencedirect.com/science/article/pii/S0260877422002734): 1.02\n",
        "\n",
        "\n",
        "\n",
        "Let's try it out. We will use the `PromptTemplate` format to promote flexibilty (i.e., we may be interested in trying different liquids or objects).\n",
        "\n"
      ],
      "metadata": {
        "id": "Az6wftVTWCFc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "obj = \"avocado\"\n",
        "liquid = \"medical alcohol\""
      ],
      "metadata": {
        "id": "mwNQ7uBBW4MD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain.callbacks import get_openai_callback\n",
        "\n",
        "instruction = \"Only provide the answer as a single yes or no without an explanation: \\n\"\n",
        "riddle = \"\"\"\n",
        "Would a {obj} float in {liquid}?\n",
        "\"\"\"\n",
        "\n",
        "chat = ChatOpenAI(openai_api_key=openai_api_key, model=\"gpt-4o-mini\")\n",
        "\n",
        "riddle_prompt_template =PromptTemplate(input_variables=[\"obj\", \"liquid\"], template=instruction + riddle)\n",
        "simple_chain = LLMChain(llm=chat, prompt=riddle_prompt_template)\n",
        "\n",
        "# print(\"Answer with no reasoning\")\n",
        "avg_cost = 0\n",
        "for i in range(10):\n",
        "  with get_openai_callback() as cb:\n",
        "    print(\"Answer: \", simple_chain.invoke({\"obj\":obj, \"liquid\":liquid})[\"text\"], \"Cost: $\", cb.total_cost)\n",
        "    avg_cost += cb.total_cost\n",
        "\n",
        "avg_cost = avg_cost/10\n",
        "print(\"Average cost: $\", avg_cost)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpl5nisMWjiR",
        "outputId": "c7cdf717-a105-469b-8f83-25e0a22e235e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-1568a1882fc3>:15: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  simple_chain = LLMChain(llm=chat, prompt=riddle_prompt_template)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer:  No. Cost: $ 5.7e-06\n",
            "Answer:  No. Cost: $ 5.7e-06\n",
            "Answer:  No. Cost: $ 5.7e-06\n",
            "Answer:  No. Cost: $ 5.7e-06\n",
            "Answer:  Yes. Cost: $ 5.7e-06\n",
            "Answer:  Yes. Cost: $ 5.7e-06\n",
            "Answer:  Yes. Cost: $ 5.7e-06\n",
            "Answer:  Yes. Cost: $ 5.7e-06\n",
            "Answer:  No. Cost: $ 5.7e-06\n",
            "Answer:  No. Cost: $ 5.7e-06\n",
            "Average cost: $ 5.7000000000000005e-06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:**\n",
        "\n",
        "- What happens if I remove the instruction? Is that a problem?"
      ],
      "metadata": {
        "id": "lWr697NEXyNg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.2.2 Do Objects Float? Can Gen AI figure it out?"
      ],
      "metadata": {
        "id": "s-jvgQp-YHAZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's let `GPT-4o` do what it needs to do to get the answer without restraints, and then extract the answer and return it. We can do this using `LLMChain`s"
      ],
      "metadata": {
        "id": "PVKwP0gIZgZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import SimpleSequentialChain\n",
        "\n",
        "chat = ChatOpenAI(openai_api_key=openai_api_key, model=\"gpt-4o-mini\")\n",
        "\n",
        "# riddle prompt where object and liquid are substituted.\n",
        "riddle_prompt =PromptTemplate(input_variables=[\"obj\", \"liquid\"], template=riddle).format_prompt(obj=obj, liquid=liquid)\n",
        "\n",
        "# The first chain solves the riddle.\n",
        "solver_prompt = PromptTemplate(\n",
        "    input_variables=[\"riddle\"], template=\"Solve the following riddle: {riddle}\"\n",
        ")\n",
        "solver_chain = LLMChain(llm=chat, prompt=solver_prompt)\n",
        "\n",
        "# The second chain extracts the final answer.\n",
        "extractor_prompt = PromptTemplate(\n",
        "    input_variables=[\"input\"], template=\"Given the input, extract the answer as a yes or no with no explanation or additional text. Input: {input}\"\n",
        ")\n",
        "extractor_chain = LLMChain(llm=chat, prompt=extractor_prompt)\n",
        "\n",
        "# The overall chain\n",
        "ss_chain = SimpleSequentialChain(chains=[solver_chain, extractor_chain]) # ,verbose=True\n",
        "\n",
        "print(\"Answer with optional reasoning\")\n",
        "avg_cost = 0\n",
        "for i in range(10):\n",
        "  with get_openai_callback() as cb:\n",
        "    result = ss_chain.invoke(riddle_prompt)\n",
        "    print(\"Answer: \", result[\"output\"], \"Cost: $\", cb.total_cost)\n",
        "    avg_cost += cb.total_cost\n",
        "\n",
        "avg_cost = avg_cost/10\n",
        "print(\"Average cost: $\", avg_cost)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OR2Z9f2vZf8_",
        "outputId": "ce2ea3db-3cce-4ec2-c3d5-e709d34f4bf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer with optional reasoning\n",
            "Answer:  No Cost: $ 9.944999999999999e-05\n",
            "Answer:  No Cost: $ 0.0001362\n",
            "Answer:  Yes Cost: $ 0.00012119999999999999\n",
            "Answer:  No Cost: $ 0.0001317\n",
            "Answer:  No Cost: $ 8.895e-05\n",
            "Answer:  Yes Cost: $ 0.00014144999999999997\n",
            "Answer:  Yes Cost: $ 0.00011295\n",
            "Answer:  Yes Cost: $ 0.0001467\n",
            "Answer:  No Cost: $ 0.00010995\n",
            "Answer:  No Cost: $ 0.00015644999999999998\n",
            "Average cost: $ 0.0001245\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:**\n",
        "\n",
        "What happens if we upgrade the model?"
      ],
      "metadata": {
        "id": "t2MSfO4vcy-e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.2.3 Do Objects Float? Can we nudge Gen AI to figure it out?"
      ],
      "metadata": {
        "id": "F66B1GDQd1_-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far, we had to either increase cost or accept low performance. Can we do better by telling the model how to solve the problem?"
      ],
      "metadata": {
        "id": "UyA2E5ftd-HS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "instructions = \"\"\"\n",
        "- Follow these steps:\n",
        "1) what is the density of {obj}?\n",
        "1) what is the density of {liquid}?\n",
        "2) is density of  {obj} is less than a  {liquid} then yes, otherwise no.\n",
        "\"\"\"\n",
        "\n",
        "chat = ChatOpenAI(openai_api_key=openai_api_key, model=\"gpt-4o-mini\")\n",
        "\n",
        "# chain for substituting\n",
        "riddle_prompt =PromptTemplate(input_variables=[\"obj\", \"liquid\"], template=riddle + instructions).format_prompt(obj=obj, liquid=liquid)\n",
        "\n",
        "# The first chain solves the riddle.\n",
        "solver_prompt = PromptTemplate(\n",
        "    input_variables=[\"riddle\"], template=\"Solve the following riddle: {riddle}. \"\n",
        ")\n",
        "solver_chain = LLMChain(llm=chat, prompt=solver_prompt)\n",
        "\n",
        "# The second chain extracts the final answer.\n",
        "extractor_prompt = PromptTemplate(\n",
        "    input_variables=[\"input\"], template=\"given the input, extract the answer as a yes or no with no explanation or additional text. Input: {input}\"\n",
        ")\n",
        "extractor_chain = LLMChain(llm=chat, prompt=extractor_prompt)\n",
        "\n",
        "# The overall chain\n",
        "ss_chain = SimpleSequentialChain(chains=[solver_chain, extractor_chain]) # ,verbose=True\n",
        "\n",
        "print(\"Answer with explicit reasoning\")\n",
        "avg_cost = 0\n",
        "for i in range(10):\n",
        "  with get_openai_callback() as cb:\n",
        "    result = ss_chain.invoke(riddle_prompt)\n",
        "    print(\"Answer: \", result[\"output\"], \"Cost: $\", cb.total_cost)\n",
        "    avg_cost += cb.total_cost\n",
        "\n",
        "avg_cost = avg_cost/10\n",
        "print(\"Average cost: $\", avg_cost)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wo4bdEbQeZDR",
        "outputId": "9ccfde64-e656-4c99-a96a-d2afd1f22c75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer with explicit reasoning\n",
            "Answer:  No Cost: $ 0.00015795\n",
            "Answer:  No. Cost: $ 0.00016605\n",
            "Answer:  No Cost: $ 0.00013245\n",
            "Answer:  No Cost: $ 0.00015945\n",
            "Answer:  No Cost: $ 0.00015194999999999998\n",
            "Answer:  No Cost: $ 0.00015869999999999998\n",
            "Answer:  No Cost: $ 0.00016545\n",
            "Answer:  No Cost: $ 0.00015045\n",
            "Answer:  No Cost: $ 0.00015644999999999998\n",
            "Answer:  No Cost: $ 0.0001437\n",
            "Average cost: $ 0.00015426\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are many forms of *CoT* with mixed results. You may consult the following link for more [info](https://www.prompthub.us/blog/chain-of-thought-prompting-guide#automatic-chain-of-thought-prompting). You may try different forms and see how well they work.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "-VkNjIWTAlor"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 ReAct Framework"
      ],
      "metadata": {
        "id": "cn3gfD0XFqDh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sometimes, answering a question may require *thinking* and then *acting* by using some tools. That is, the model may need to take several steps and obtain intermediate answers, just like in *CoT*. Some of these steps may require taking actions (i.e., using tools). This framework is called **ReAct (Reasoning-Acting)**.\n",
        "\n",
        "To create such a ReAct agent, we use the [`create_react_agent`](https://python.langchain.com/api_reference/langchain/agents/langchain.agents.react.agent.create_react_agent.html#langchain.agents.react.agent.create_react_agent) function.\n",
        "\n"
      ],
      "metadata": {
        "id": "Rt5SyufdQ-E9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's say our agent needs to answer a complex question that may require being broken down into several steps. For example, consider the following questions which may occur in a customer facing chatbot."
      ],
      "metadata": {
        "id": "YKQ9i2dwSK5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# question = \"Given today's date, look at the price of Microsoft stock. If I had bought 1 stock 3 years ago, how much money would I make today by selling?\" # Given today's date,\n",
        "\n",
        "\n",
        "question = \"Create a task in my project with name LLMtest. The task's type is bug with name as tomorrow's date and assignee is mentormohannad@gmail.com\"\n",
        "# question = \"How many tasks are there with 2024-11-14 as a name in project with name LLMtest\"\n",
        "# question = \"How many tasks are there with yesterday's date as a name in project with name LLMtest\"\n",
        "# question = \"Who is the assignee of the task with yesterday's date as a name in project with name LLMtest\""
      ],
      "metadata": {
        "id": "qMexUX02Vrou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems to answer such questions, we need ReAct to reason and act using:\n",
        "\n",
        "1. A tool that is good at calculations.\n",
        "2. A tool that can search the internet.\n",
        "3. A tool that can find today's date.\n",
        "4. A tool that can read and create Jira issues.\n",
        "  *   For Jira, you simply need to [sign up](https://www.atlassian.com/try/cloud/signup?bundle=jira-software&edition=free&skipBundles=true) first, create a project and [get a token](https://id.atlassian.com/manage-profile/security) for authentication. After that, the [Jira toolkit](https://python.langchain.com/docs/integrations/tools/jira/) can be used.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AMOSCeI7WatI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --quiet  atlassian-python-api\n",
        "!pip install -qU langchain-community langchain_openai"
      ],
      "metadata": {
        "id": "v-oTjmtchuBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import hub\n",
        "from langchain_community.llms import OpenAI\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.agents import AgentExecutor, create_react_agent, create_tool_calling_agent, tool\n",
        "from langchain_community.tools.google_finance import GoogleFinanceQueryRun\n",
        "from langchain import LLMMathChain\n",
        "\n",
        "### Get the OpenAI API key\n",
        "from google.colab import userdata\n",
        "openai_api_key = userdata.get('MyOpenAIKey')\n",
        "\n",
        "# LLM\n",
        "chat = ChatOpenAI(openai_api_key=openai_api_key, model=\"gpt-4o-mini\")"
      ],
      "metadata": {
        "id": "toHLcLrKVrpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "## tool 1: getting today's date. Already defined above.\n",
        "from datetime import date, timedelta, datetime\n",
        "@tool\n",
        "def datetoday(dummy: str) -> str:\n",
        "    \"\"\"Returns today's date, use this for any \\\n",
        "    questions that need today's date to be answered. \\\n",
        "    This tool returns a string with today's date.\"\"\" #This is the desciption the agent uses to determine whether to use the time tool.\n",
        "    return \"Today is \" + str(date.today())"
      ],
      "metadata": {
        "id": "S2wWLFUqNQ2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## tool 2: Calculator.\n",
        "# We can simply define a tool that uses the LLMChain above\n",
        "llm = OpenAI(openai_api_key=openai_api_key)\n",
        "llm_math = LLMMathChain.from_llm(llm)\n",
        "@tool\n",
        "def calculator_tool(input_message: str) -> str:\n",
        "    \"\"\"Will not be used for any date arithmetics. Takes input message and performs the necessary calculations.\n",
        "    Returns the result.\"\"\" #This is the desciption the agent uses to determine whether to use the time tool.\n",
        "    return \"The answer to \" + input_message + \" is \" + llm_math.invoke(input_message)[\"answer\"]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5e4FLPaNQ4o",
        "outputId": "ac91f943-931c-47e1-c1a6-9fc518d06097"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-fe299cf11e23>:3: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n",
            "  llm = OpenAI(openai_api_key=openai_api_key)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## tool 3: Internet Search.\n",
        "\n",
        "from langchain_core.tools import Tool\n",
        "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
        "import os\n",
        "\n",
        "# Setting up the Serper tool\n",
        "os.environ[\"SERPER_API_KEY\"] = userdata.get('SERPER_API')\n",
        "search = GoogleSerperAPIWrapper()\n",
        "serper_tool = Tool(\n",
        "        name=\"GoogleSerper\",\n",
        "        func=search.run,\n",
        "        description=\"Useful for when you need to look up some information on the internet.\",\n",
        "    )\n"
      ],
      "metadata": {
        "id": "kABxvk3yQs7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## toolkit 1\n",
        "import os\n",
        "from langchain_community.agent_toolkits.jira.toolkit import JiraToolkit\n",
        "from langchain_community.utilities.jira import JiraAPIWrapper\n",
        "\n",
        "os.environ[\"JIRA_API_TOKEN\"] = userdata.get('JIRA_API_TOKEN')\n",
        "os.environ[\"JIRA_USERNAME\"] = \"mentormohannad@gmail.com\" #Email used to sign in to JIRA\n",
        "os.environ[\"JIRA_INSTANCE_URL\"] = \"https://mohannadelhamod.atlassian.net/\" #URL used to reach your free JIRA instance\n",
        "os.environ[\"JIRA_CLOUD\"] = \"True\"\n",
        "\n",
        "jira = JiraAPIWrapper()\n",
        "toolkit = JiraToolkit.from_jira_api_wrapper(jira)\n",
        "\n",
        "# DON'T CHANGE: Some fixes and improvements needed.\n",
        "for idx, tool in enumerate(toolkit.tools):\n",
        "    toolkit.tools[idx].name = toolkit.tools[idx].name.replace(\" \", \"_\")\n",
        "toolkit.tools[2].description = toolkit.tools[2].description + \". Don't set priority for issues. Make sure to specify the project ID.\" # Future work: could set the assignee to {'accountId':'account_id'}\n"
      ],
      "metadata": {
        "id": "idVwaJamNagf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's put the tools together"
      ],
      "metadata": {
        "id": "s69_uwB6OUuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [\n",
        "    datetoday,\n",
        "    calculator_tool,\n",
        "    serper_tool\n",
        "] + toolkit.get_tools()"
      ],
      "metadata": {
        "id": "k5DAFkJYOWzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at the different tools available"
      ],
      "metadata": {
        "id": "YdpsaE6wNykw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, tool in enumerate(tools):\n",
        "    print(f\"Tool {idx}: {tool.name}\")\n",
        "    print(\"Description:\", tool.description)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwxgsCTk5sJk",
        "outputId": "6fe42d5f-a6a3-45a1-b0aa-37ecb5075c60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tool 0: datetoday\n",
            "Description: Returns today's date, use this for any     questions that need today's date to be answered.     This tool returns a string with today's date.\n",
            "Tool 1: calculator_tool\n",
            "Description: Will not be used for any date arithmetics. Takes input message and performs the necessary calculations.\n",
            "    Returns the result.\n",
            "Tool 2: GoogleSerper\n",
            "Description: Useful for when you need to look up some information on the internet.\n",
            "Tool 3: JQL_Query\n",
            "Description: \n",
            "    This tool is a wrapper around atlassian-python-api's Jira jql API, useful when you need to search for Jira issues.\n",
            "    The input to this tool is a JQL query string, and will be passed into atlassian-python-api's Jira `jql` function,\n",
            "    For example, to find all the issues in project \"Test\" assigned to the me, you would pass in the following string:\n",
            "    project = Test AND assignee = currentUser()\n",
            "    or to find issues with summaries that contain the word \"test\", you would pass in the following string:\n",
            "    summary ~ 'test'\n",
            "    \n",
            "Tool 4: Get_Projects\n",
            "Description: \n",
            "    This tool is a wrapper around atlassian-python-api's Jira project API, \n",
            "    useful when you need to fetch all the projects the user has access to, find out how many projects there are, or as an intermediary step that involv searching by projects. \n",
            "    there is no input to this tool.\n",
            "    \n",
            "Tool 5: Create_Issue\n",
            "Description: \n",
            "    This tool is a wrapper around atlassian-python-api's Jira issue_create API, useful when you need to create a Jira issue. \n",
            "    The input to this tool is a dictionary specifying the fields of the Jira issue, and will be passed into atlassian-python-api's Jira `issue_create` function.\n",
            "    For example, to create a low priority task called \"test issue\" with description \"test description\", you would pass in the following dictionary: \n",
            "    {{\"summary\": \"test issue\", \"description\": \"test description\", \"issuetype\": {{\"name\": \"Task\"}}, \"priority\": {{\"name\": \"Low\"}}}}\n",
            "    . Don't set priority for issues. Make sure to specify the project ID.\n",
            "Tool 6: Catch_all_Jira_API_call\n",
            "Description: \n",
            "    This tool is a wrapper around atlassian-python-api's Jira API.\n",
            "    There are other dedicated tools for fetching all projects, and creating and searching for issues, \n",
            "    use this tool if you need to perform any other actions allowed by the atlassian-python-api Jira API.\n",
            "    The input to this tool is a dictionary specifying a function from atlassian-python-api's Jira API, \n",
            "    as well as a list of arguments and dictionary of keyword arguments to pass into the function.\n",
            "    For example, to get all the users in a group, while increasing the max number of results to 100, you would\n",
            "    pass in the following dictionary: {{\"function\": \"get_all_users_from_group\", \"args\": [\"group\"], \"kwargs\": {{\"limit\":100}} }}\n",
            "    or to find out how many projects are in the Jira instance, you would pass in the following string:\n",
            "    {{\"function\": \"projects\"}}\n",
            "    For more information on the Jira API, refer to https://atlassian-python-api.readthedocs.io/jira.html\n",
            "    \n",
            "Tool 7: Create_confluence_page\n",
            "Description: This tool is a wrapper around atlassian-python-api's Confluence \n",
            "atlassian-python-api API, useful when you need to create a Confluence page. The input to this tool is a dictionary \n",
            "specifying the fields of the Confluence page, and will be passed into atlassian-python-api's Confluence `create_page` \n",
            "function. For example, to create a page in the DEMO space titled \"This is the title\" with body \"This is the body. You can use \n",
            "<strong>HTML tags</strong>!\", you would pass in the following dictionary: {{\"space\": \"DEMO\", \"title\":\"This is the \n",
            "title\",\"body\":\"This is the body. You can use <strong>HTML tags</strong>!\"}} \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take look a look at the ReAct prompt and how it directs the model to *\"think\"*..."
      ],
      "metadata": {
        "id": "Q5PpOzn7c9_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = hub.pull(\"hwchase17/react\")\n",
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rwelgmiOaf6",
        "outputId": "0a625c46-7ec1-4901-d6f7-7029c120056b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langsmith/client.py:241: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['agent_scratchpad', 'input', 'tool_names', 'tools'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'react', 'lc_hub_commit_hash': 'd15fe3c426f1c4b3f37c9198853e4a86e20c425ca7f4752ec0c9b0e97ca7ea4d'}, template='Answer the following questions as best you can. You have access to the following tools:\\n\\n{tools}\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [{tool_names}]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: {input}\\nThought:{agent_scratchpad}')"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's see the thinking and acting in motion!"
      ],
      "metadata": {
        "id": "JJT2iRrZOZR2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### You can turn on debugging using this code. You will be able to see the intermediate requests and responses.\n",
        "import langchain\n",
        "langchain.debug = False\n",
        "\n",
        "# The following prompt is designed for ReAct framework.\n",
        "prompt = hub.pull(\"hwchase17/react\")\n",
        "agent = create_react_agent(chat, tools, prompt) # Now we have a ReAct agent!\n",
        "\n",
        "# We may still use create_tool_calling_agent and get good result. But, it is not following the standard ReAct framework per se.\n",
        "# More on agent types can be found here: https://python.langchain.com/v0.1/docs/modules/agents/agent_types/\n",
        "# from langchain_core.prompts import ChatPromptTemplate\n",
        "# prompt = ChatPromptTemplate.from_messages(\n",
        "#     [\n",
        "#         (\"system\", \"You are a helpful assistant.\"),\n",
        "#         (\"human\", \"{input}\"),\n",
        "#         (\"placeholder\", \"{agent_scratchpad}\"), # To be used by the agent for intermediate operations.\n",
        "#     ]\n",
        "# )\n",
        "# agent = create_tool_calling_agent(chat, tools, prompt)\n",
        "\n",
        "\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose= True, stream_runnable=False)\n",
        "agent_executor.invoke({\"input\": question}, handle_parsing_errors=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEdvX6OmOdal",
        "outputId": "b3a4bda1-b5f4-41b7-bf1b-7065bca4378e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langsmith/client.py:241: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mI need to create a task in the user's project with specific details. First, I need to retrieve the user's projects to find the appropriate project ID. Then I'll get tomorrow's date to set it as the task's name. Finally, I will create the task with the specified details. \n",
            "\n",
            "Action: Get_Projects\n",
            "Action Input: None\u001b[0m\u001b[33;1m\u001b[1;3mFound 1 projects:\n",
            "[{'id': '10000', 'key': 'SCRUM', 'name': 'LLMtest', 'type': 'software', 'style': 'next-gen'}]\u001b[0m\u001b[32;1m\u001b[1;3mI have retrieved the project ID for the project named \"LLMtest.\" Now, I need to get tomorrow's date, which I will use as the task's name. \n",
            "\n",
            "Action: datetoday\n",
            "Action Input: dummy\u001b[0m\u001b[36;1m\u001b[1;3mToday is 2024-11-18\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:atlassian.jira:Creating issue \"2024-11-19\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3mTomorrow's date is 2024-11-19. I will now create the bug task with the name \"2024-11-19,\" and assign it to the specified email address.\n",
            "\n",
            "Action: Create_Issue\n",
            "Action Input: {\"summary\": \"2024-11-19\", \"description\": \"\", \"issuetype\": {\"name\": \"Bug\"}, \"project\": {\"id\": \"10000\"}, \"assignee\": {\"emailAddress\": \"mentormohannad@gmail.com\"}}\u001b[0m\u001b[38;5;200m\u001b[1;3m{'id': '10014', 'key': 'SCRUM-15', 'self': 'https://mohannadelhamod.atlassian.net/rest/api/2/issue/10014'}\u001b[0m\u001b[32;1m\u001b[1;3mI have successfully created the bug task with the name \"2024-11-19\" in the project \"LLMtest,\" assigned to the user with the email address mentormohannad@gmail.com. \n",
            "\n",
            "Final Answer: The task has been created successfully with the name \"2024-11-19\" in the project \"LLMtest.\"\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': \"Create a task in my project with name LLMtest. The task's type is bug with name as tomorrow's date and assignee is mentormohannad@gmail.com\",\n",
              " 'output': 'The task has been created successfully with the name \"2024-11-19\" in the project \"LLMtest.\"'}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Homework"
      ],
      "metadata": {
        "id": "EJIUrg_yUuJt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.1 Creating and Dating Files"
      ],
      "metadata": {
        "id": "Gl891Vbjb70L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write an agent that solves the following question correctly:\n",
        "\n",
        "> Create a file named *\\<some filename\\>*.txt and contains today's date.\n",
        "\n",
        "Once it executes, the agent will be able to create the file with the correct name and content. It should create the file under `/content`. It will show up on the left side of your Google notebook under *files*. **(5 Points)**\n",
        "\n",
        "You will need to find and integrate the right tool from this [list](https://python.langchain.com/docs/integrations/tools/) to be able to create and write a file.\n",
        "\n",
        "You will also need to use the `datetoday` tool we defined previously to get today's date.\n",
        "\n",
        "Make sure you allow the agent to use multiple tools."
      ],
      "metadata": {
        "id": "dFY32l5XUx1Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer**"
      ],
      "metadata": {
        "id": "MPwWcmlvcEib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import AgentExecutor, create_tool_calling_agent, tool\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.messages import HumanMessage"
      ],
      "metadata": {
        "id": "ncCQGeMphSG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"hello.txt\" # Your code should respect this desired filename"
      ],
      "metadata": {
        "id": "ROGOAApQgVm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here write an instruction prompt that effectively execute the desired functionality. Remember that `filename` is a *variable*. **(4 Points)**"
      ],
      "metadata": {
        "id": "WRFDAZQwhBiL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "instruction = ### Fill in code here. This prompt will be used in the prompt template."
      ],
      "metadata": {
        "id": "DCMMfIYFhB9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat = ChatOpenAI(openai_api_key=openai_api_key, model=\"gpt-4o-mini\")\n",
        "\n",
        "# Create the PromptTemplate from the instruction you defined above.\n",
        "instruction_template =PromptTemplate(\n",
        "    input_variables= ### Fill in code here\n",
        "    template=instruction\n",
        "    )"
      ],
      "metadata": {
        "id": "NugPgsnUU2PW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the first tool that gets today's date"
      ],
      "metadata": {
        "id": "cOHRtlwAhpTG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import date, timedelta, datetime\n",
        "@tool\n",
        "def datetoday() -> str:\n",
        "    \"\"\"Returns today's date, use this for any \\\n",
        "    questions that need today's date to be answered. \\\n",
        "    This tool takes no argumetns but returns a string with today's date.\"\"\" #This is the desciption the agent uses to determine whether to use the time tool.\n",
        "    return \"Today is \" + str(date.today())"
      ],
      "metadata": {
        "id": "M4fHhojwhpZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instantiate the second tool that is able to create and write the file. **(7 Points)**"
      ],
      "metadata": {
        "id": "i3MVPHGuhyNo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Fill in code here to get the tool. DO NOT create a custom tool!"
      ],
      "metadata": {
        "id": "6OcPP4ouhyV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Putting it all together.\n",
        "When you run the code, *make sure it shows the intermedate tool calls*. **(4 Points)**"
      ],
      "metadata": {
        "id": "pRODohkRidVM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tools = ### Fill here: Since both tools should be available, place them into a list.\n",
        "\n",
        "# Defining the agent\n",
        "### You may need to make minor changes here.\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "        (\"placeholder\", \"{agent_scratchpad}\"),\n",
        "    ]\n",
        ")\n",
        "agent = create_tool_calling_agent(chat, tools, prompt)\n",
        "agent_executor = AgentExecutor(agent=agent, tools=tools) # , verbose=True\n",
        "\n",
        "formatted_instruction = ### Fill code here\n",
        "print(\"Agent answer:\", agent_executor.invoke({\"input\": formatted_instruction})[\"output\"])"
      ],
      "metadata": {
        "id": "HgJX4wn9fifc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}