{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elhamod/IS883/blob/main/Assignment4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Assignment 4\n",
        "\n",
        "## Overview\n",
        "This notebook has been created for IS883 at Questrom School of Business - Boston University. It is designed to guide students through using `LangChain` to create chat agents, Question-answering from a document, and prompt engineering techniques.\n",
        "\n",
        "### Created By\n",
        "- **Author:** Mohannad Elhamod\n",
        "- **Position:** Clinical Assistant Profressor\n",
        "- **Institution:** Questrom School of Business - Boston University\n",
        "\n",
        "\n",
        "\n",
        "*Note: This notebook is intended for educational purposes and is part of the coursework for IS883. Unauthorized distribution or use is not permitted.*\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyX90VY4i7Y9"
      },
      "source": [
        "#LangChain Frameworks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPkbs259XQ9t"
      },
      "source": [
        "##Q0: Prepation code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6l_D3JUMRsw"
      },
      "source": [
        "Installing necessary packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CR52h7TlXX5P",
        "outputId": "5f7800a3-7731-4b79-bf39-a61e819a0a22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.0.342-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.6)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.3-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-core<0.1,>=0.0.7 (from langchain)\n",
            "  Downloading langchain_core-0.0.7-py3-none-any.whl (177 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.5/177.5 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langsmith<0.1.0,>=0.0.63 (from langchain)\n",
            "  Downloading langsmith-0.0.67-py3-none-any.whl (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.0/47.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.1.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain\n",
        "!pip install pypdf\n",
        "!pip install openai\n",
        "!pip install tiktoken\n",
        "!pip install faiss-cpu\n",
        "!pip install nltk\n",
        "!pip install pandas\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQkEk_toMg10"
      },
      "source": [
        "Retrieve OpenAI API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wLnzuBTXZ_R"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEUeXLsOXb2R"
      },
      "outputs": [],
      "source": [
        "config_ini_location = '/content/drive/MyDrive/Colab Notebooks/IS883/OpenAI guide/config.ini' # Change this to point to the location of your config.ini file.\n",
        "\n",
        "import configparser\n",
        "\n",
        "config = configparser.ConfigParser()\n",
        "config.read(config_ini_location)\n",
        "openai_api_key = config['OpenAI']['API_KEY']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oV2WKm3-jlZZ"
      },
      "source": [
        "For this assignment you will use ``model_name=\"gpt-3.5-turbo-0613\"`` only. **You are NOT allowed to use any other model. You will lose 1 point per question if you violate this requirement.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGe8kKAJbm1G"
      },
      "outputs": [],
      "source": [
        "model_name=\"gpt-3.5-turbo-0613\" # Do Not change this!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maZViuv_at5R"
      },
      "source": [
        "**For debugging purposes for all the questions below, remember that using `verbose`  and `langchain.debug` to print the actual requests and responses is quite useful.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QOIueVJiup4"
      },
      "source": [
        "## Q1:  Question Answering System Using the School's Syllabus Database (4.5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZyoG9F9_pYS"
      },
      "source": [
        " At your school, the department has embarked on a project to utilize language modeling for the development of a question-answering agent. This initiative aims to streamline the access to information for faculty and staff, particularly regarding the extensive array of courses offered at our institution. The data pertaining to these courses is currently dispersed across numerous documents within [the department's syllabus corpus](https://drive.google.com/drive/folders/1dH-t_Ujih4lMMzUOaNOHngvOYLK_gWOp?usp=sharing).\n",
        "\n",
        "Download the corpus to your Google Drive and update the path below.\n",
        "\n",
        "Note: The used syllabus corpus is a subset of [Cal Poly's Syllabus Corpus dataset](https://www.kaggle.com/datasets/mfekadu/syllabus-corpus)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mlvkSddj2b3g"
      },
      "outputs": [],
      "source": [
        "syllabus_corpus_path = \"/content/drive/MyDrive/Colab Notebooks/IS883/IS883_HW4_syllabus_corpus/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-5WoGlMB0HF"
      },
      "source": [
        "First, you will use a [PyPDFDirectoryLoader](https://api.python.langchain.com/en/latest/document_loaders/langchain.document_loaders.pdf.PyPDFDirectoryLoader.html) to create a loader that can load all the PDFs in the directory so they could be used by LangChain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9d5Kv4xOIbmr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9BYOSrmCwdB"
      },
      "source": [
        "Given the extensive data contained within these documents, it's impractical to include them in their entirety in our queries. Including all data at once could exceed the context window's capacity and may result in significant processing costs. To address this challenge, you will employ a methodical approach to manage the data effectively.\n",
        "\n",
        "* Create a [RecursiveCharacterTextSplitter](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter): You will use a `RecursiveCharacterTextSplitter` to divide the documents into more manageable segments. This splitter will break down the documents into chunks.\n",
        "\n",
        "* Configurations: **(0.25 point)**\n",
        "  * Chunk Size Configuration: Set the `chunk_size` to 500 characters. This size ensures that the chunks are large enough to contain meaningful content but small enough to be processed efficiently.\n",
        "\n",
        "  * Creating Overlapping Chunks: Set `chunk_overlap` to 50 characters. This overlap will help prevent the loss of context that might occur at the boundaries of each chunk. It ensures that no critical information is missed or misunderstood due to the chunking process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pH5RHznSCv7j"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NT_gyExVEK2T"
      },
      "source": [
        "Now, using the afortmentioned loader and splitter, perform the splitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzrSsfNiELEm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIkbtCboEWyQ"
      },
      "source": [
        "The next crucial step involves the creation of a data store, essentially a database, that will house the chunks of data you've created. The effectiveness of our question-answering system hinges on its ability to swiftly locate the relevant chunk containing the answer to any given query. To achieve this efficiency, we will employ a sophisticated indexing strategy, rather than relying on a basic brute-force search method.\n",
        "\n",
        "* Build the Data Store with [Facebook AI Similarity Search (FAISS)](https://python.langchain.com/docs/integrations/vectorstores/faiss): Set up your data store using a [FAISS Vector store](https://python.langchain.com/docs/integrations/vectorstores/faiss). FAISS is a library developed by Facebook AI that allows for efficient similarity search and clustering of dense vectors.\n",
        "\n",
        "* Embedding Calculation with `OpenAIEmbeddings`: For each chunk of data in your store, calculate an embedding using `OpenAIEmbeddings`. These embeddings are essentially numerical representations of your text data, which can then be compared to the embeddings of incoming queries.\n",
        "\n",
        "* Indexing for Efficient Search: By creating embeddings for each chunk and indexing them in the FAISS Vector store, you will enable the system to quickly find the most relevant chunk in response to a query. This process involves comparing the embedding of the query with the embeddings of the chunks to identify the best match.\n",
        "\n",
        "The combination of `FAISS` and `OpenAIEmbeddings` will significantly enhance the efficiency and accuracy of the question-answering system, allowing for rapid retrieval of information from the extensive syllabus corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1KJfaIHJW9L"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSdBNAFuFog9"
      },
      "source": [
        "With the data store and indexing system in place, you are now equipped to tackle the core functionality of our question-answering system: responding to queries based on the indexed database.\n",
        "\n",
        "* Utilize the [*`similarity_search`*](https://python.langchain.com/docs/integrations/vectorstores/faiss) function to identify the chunk that is most relevant or most similar to the posed question. This function will compare the embedding of the query with those of the indexed chunks to find the best match. **(0.25 point)**\n",
        "\n",
        "* Display Source Information: Once you have identified the most relevant answer, output additional details indicating where this chunk is located. Specifically, provide information about *the page number and the document from which this chunk was extracted*. **(0.5 point)**\n",
        "\n",
        "To gain a deeper understanding of how similarity search operates, refer to the provided articles and references. These resources will offer a more detailed conceptual insight into the workings of similarity search algorithms and their applications in systems like ours.\n",
        "\n",
        "[Resource 1.](https://www.pinecone.io/learn/what-is-similarity-search/)\n",
        "\n",
        "[Resource 2.](https://python.langchain.com/docs/modules/data_connection/vectorstores/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtXjIjFOFn_s"
      },
      "outputs": [],
      "source": [
        "question = \"Who is the instructor of Linear Algebra III?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCFVrRojMlm5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXWI7xvuH8wx"
      },
      "source": [
        "Next, you will delve deeper into the results to evaluate the system.\n",
        "\n",
        "* Display the Top 5 Matches: Print the top five most relevant chunks in response to your query, *along with their respective similarity scores*. These scores quantify how closely each chunk matches your query, offering a clear metric of relevance. **(0.5 point)**\n",
        "\n",
        "\n",
        "\n",
        "* Examine why certain chunks received higher or lower similarity scores. Analyze the content of each chunk in relation to your query to understand the basis of these scores. **(0.25 point)**\n",
        "\n",
        "  * Discuss whether the model is effectively discerning relevant information or if it appears to be misled by certain elements. Provide suggestions for improvements.\n",
        "\n",
        "[Resource.](https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.faiss.FAISS.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1OwFLeCIqy4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-nN8u1BIpw5"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaV70nbKLHcm"
      },
      "source": [
        "Finally, we are going to use OpenAI API to get the answer to the question based on the relevant chunk. To do that, we will LangChain's *load_qa_chain*. This [article](https://cloudatlas.me/query-your-pdfs-with-openai-langchain-and-faiss-7e8221791c62) should give you an example of how to use it.\n",
        "\n",
        "The final step involves leveraging OpenAI API to obtain answers to your queries based on the top `k` most relevant chunk identified in the previous step **(0.25 point)**. For this, you will use LangChain's [`load_qa_chain`](https://cloudatlas.me/query-your-pdfs-with-openai-langchain-and-faiss-7e8221791c62) functionality.\n",
        "\n",
        "* Utilize `load_qa_chain` to integrate OpenAI API into your question-answering system. This tool will enable you to send the selected chunk as a context to the API and retrieve a \"precise\" answer to your query.\n",
        "\n",
        "* Track the requests sent and the responses received from the OpenAI API. This will give you visibility into the interaction between your system and the API. **(0.25 point)**\n",
        "\n",
        "* Analyze the requests and responses in detail. Discuss how the API processes the chunk and formulates an answer **(0.5 point)**. Evaluate the overall performance of the system in leveraging OpenAI API for answering queries. Consider the relevance and precision of the answers, and how well the system integrates the information from the chunks to generate responses. **(0.5 point)**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7IjXkkYj-4S"
      },
      "outputs": [],
      "source": [
        "temperature ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V366zXMybXGq"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "# Create a reference to the language model\n",
        "llm = ChatOpenAI(openai_api_key=openai_api_key, temperature=temperature, model_name=model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8Xoh61sVyHQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-kEs712YZ8h"
      },
      "source": [
        "**Answer:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfs5j0_zZexD"
      },
      "source": [
        "It's important to analyze and compare the system's performance across various questions.\n",
        "\n",
        "\n",
        "\n",
        "* Compare with First Question: Reflect on the system's response to the following question and compare it with the response to the first question above. Note any differences in accuracy, relevance, or clarity of the answers. **(0.5 point)**\n",
        "\n",
        "* Analyze the causes behind these observations. Consider factors such as the nature of the question, the relevance of the chosen chunk, and how the AI model interprets different types of queries. **(0.25 point)**\n",
        "\n",
        "* Propose Changes: Based on your observations, propose potential changes or adjustments that could improve the system's ability to retrieve more accurate or relevant answers **(0.25 point)**. Evaluate Trade-offs: Discuss the trade-offs associated with the changes you propose. **(0.25 point)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGaVq2vaaB_7"
      },
      "outputs": [],
      "source": [
        "question2 = \"What additional cost does Lean Six Sigma Black Belt Training require?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IW30Jf7baDV8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_T7lnzTbOIL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BykXgjiQOY5"
      },
      "source": [
        "**Answer:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwQEZ_EZvag5"
      },
      "source": [
        "##Q2: AI-Powered \"I went to the beach\" Showdown (4.5 points)\n",
        "\n",
        "In this part, you will explore the realm of AI-driven creativity by setting up an engaging interaction between two chatbots. The aim is to test the extent of the AI's \"perpetual\" creativity in a fun and interactive setup. You will create a scenario where two chatbots complete each other's sentences in a themed showdown revolving around the phrase \"I went to the beach.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yagtjhqZ_75g"
      },
      "outputs": [],
      "source": [
        "first_sentence = \"I went to the beach with my son and daughter.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjG0V8hjZ6R-"
      },
      "outputs": [],
      "source": [
        "temperature = 0.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vn49zEgoyOb5"
      },
      "source": [
        "First, Create the `ChatOpenAI` object. Be sure to use the `model_name` above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gN0K_hdXx85r"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models.openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(temperature=temperature,openai_api_key=openai_api_key, model_name=model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ial_XtOMyVa3"
      },
      "source": [
        "The next step is to construct the first AI participant, referred to as **AI_1**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKASVCVeqFs0"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
        "\n",
        "# Create the prompt\n",
        "template = \"\"\"We are going to take turns building a story. The story should not be repetitive. It should keep adding new events.\n",
        "Each turn, I will first give a sentance of no more than 20 words, and then you will give a sentence of a similar length. We repeat the cycle indefinitely.\n",
        "\n",
        "{chat_history}\n",
        "Human: {human_input}\n",
        "AI:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"chat_history\", \"human_input\"], template=template\n",
        ")\n",
        "\n",
        "# create the memory of the conversation\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "\n",
        "# create the chain\n",
        "llm_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt,\n",
        "    verbose=True,\n",
        "    memory=memory,\n",
        ")\n",
        "\n",
        "# get the first AI reponse\n",
        "response1 = llm_chain.predict(human_input=first_sentence)\n",
        "print(\"AI_1:\", response1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kx64F_r29GG"
      },
      "source": [
        "Let's look at the conversation's memory from **AI_1**'s perspective. This should print the entire conversation **(0.25 point)**. It should look like this:\n",
        "\n",
        "```\n",
        "I went to the beach with my son and daughter.\n",
        "We built sandcastles and played in the waves, laughing and splashing each other <<or something similar...>>\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "byBD-eBI27-b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBFt7EXZzhVJ"
      },
      "source": [
        "After successfully setting up **AI_1**, the next step is to develop the second participant in the creative exchange, **AI_2**. This AI will have roles that are opposite to those of **AI_1** (i.e., \"Human\" here is \"AI\" there and vice versa.).\n",
        "\n",
        "* Setting Up `ConversationBufferMemory` (memory2): Establish a new `ConversationBufferMemory`, named `memory2`, for **AI_2**. This memory will be responsible for storing the conversation as perceived and processed by **AI_2**.\n",
        "* Initializing Memory with First Sentence: Incorporate an `AIMessage`: Ensure that `memory2` already contains an `AIMessage` corresponding to the first sentence of the conversation corresponding to **AI_1**'s first response. This is crucial for setting the context from **AI_2**’s perspective. Utilize the [`add_ai_message`](https://python.langchain.com/docs/modules/memory/) function to add this initial message to `memory2`.\n",
        "\n",
        "Consult the [provided reference](https://python.langchain.com/docs/modules/memory/conversational_customization) for a thorough understanding of message manipulation and conversation customization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbap-VodxgoW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpMH_CEX3VVa"
      },
      "source": [
        "After setting up the foundational elements for **AI_2**, your next task is to build its conversational chain, using `LLMChain` and use it to generate the next sentence in the \"I went to the beach\" showdown. **(0.5 point)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0KiuToD2zHz2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sr53tW-J22O3"
      },
      "source": [
        "At the end of your setup, `memory2.chat_memory` should have the following messages:\n",
        "\n",
        "\n",
        "Conclude the setup for **AI_2** by verifying that the content of `memory2.chat_memory` correctly reflects the conversation **(0.25 point)**. It should contain the following sequence of messages:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "I went to the beach with my son and daughter.\n",
        "<<AI_1's response>>.\n",
        "<<AI_2's reponse>>.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "giCs7rPu24rq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OU0t7roOx5iw"
      },
      "source": [
        "Now, let's create a loop where **AI_1** and **AI_2** engage in a series of sentence completions. You'll also be tracking the total cost of the entire conversation using the `get_openai_callback` function.\n",
        "\n",
        "\n",
        "\n",
        "* Create a loop that runs for 10 iterations. In each iteration, **AI_1** and **AI_2** will take turns completing each other’s sentences, contributing to the ongoing conversation. **(0.5 point)**\n",
        "\n",
        "* Within the loop, after each AI has generated its response, print the responses along with the iteration number in the following format:\n",
        "```\n",
        "iteration X\n",
        "AI_1: <<AI_1's response>>\n",
        "AI_2: <<AI_2's response>>\n",
        "```\n",
        "Replace X with the current iteration number, and ensure the responses are accurately captured. **(0.25 point)**\n",
        "\n",
        "* Use the [`get_openai_callback`](https://python.langchain.com/docs/modules/model_io/llms/token_usage_tracking) to track the **total** cost for the entire conversation, as opposed to per-iteration costs. **(0.25 point)**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BlKwZwLx6Ja"
      },
      "outputs": [],
      "source": [
        "iterations=10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0sawVRb5jB7Y"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utkeHaL16JY4"
      },
      "source": [
        "For optimizing the token usage and cost while preserving the essence of the conversation, you'll need to adapt your code to incorporate [`ConversationSummaryBufferMemory`](https://python.langchain.com/docs/modules/memory/types/summary_buffer). This memory type will summarize the conversation when a predefined `max_token_limit=200` is reached, thus reducing the token count sent to the language model in each iteration.\n",
        "\n",
        "* While keeping the existing language model object (`llm`) intact, recreate the chains for **AI_1** and **AI_2** to use the new `ConversationSummaryBufferMemory`. **(0.25 point)**\n",
        "* Repeat the same experiment above (i.e., conducting a 10-round conversation) using the new chains. **(0.5 point)**\n",
        "\n",
        "For a better understanding of the different types of memory and their functionalities, refer to [the following article](https://www.pinecone.io/learn/series/langchain/langchain-conversational-memory/).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYneQgmSMGLP"
      },
      "outputs": [],
      "source": [
        "max_token_limit="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VWtrqdVnKorR"
      },
      "outputs": [],
      "source": [
        "# Define AI_1 and produce its first sentence.\n",
        "\n",
        "\n",
        "# Define AI_2 and produce its first sentence.\n",
        "\n",
        "\n",
        "\n",
        "# Get the conversation going.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iiR-pR3u7pix"
      },
      "source": [
        "Answer the following questions:\n",
        "\n",
        "* Having run the conversational AI using both standard memory and `ConversationSummaryBufferMemory`, compare the total cost incurred with each type of memory. Which memory type results in low cost? Discuss the causes behind your findings. **(0.5 point)**\n",
        "\n",
        "* [Measure the time](https://www.programiz.com/python-programming/examples/elapsed-time) taken to execute the conversation loop with each type of memory. Determine which memory type takes longer to execute and provide reasons for this. **(0.25 point)**\n",
        "\n",
        "* Observe the final conversations for both types of memory. Provide evidence from the code's output showing how the summary is generated, and how the conversation is condensed once the `max_token_limit` is reached. **(0.25 point)**\n",
        "\n",
        "* Describe the process of summary generation. Assess the effectiveness of the summary in representing the original conversation. **(0.5 point)**\n",
        "\n",
        "* What prompt engineering framework was used in `ConversationSummaryBufferMemory`? Provide evidence. **(0.25 point)**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WA3_zj6zOsjk"
      },
      "source": [
        "**Answer:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBMXXSxuNGX9"
      },
      "source": [
        "##Q3 (BONUS): Riddle Me This... **(4.5 points)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDwpy8qJOzBg"
      },
      "source": [
        "In this question, you will get to have some fun with math riddles and explore the impact of different prompt engineering frameworks on solving them using AI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXt3n2vA4yww"
      },
      "outputs": [],
      "source": [
        "riddle  = \"A man left 100 dollars to be divided between his two sons Alfred and Benjamin. If one third of Alfred’s legacy was taken from one-fourth of Benjamin’s, the remainder would be 11 dollars. How much is Alfred's legacy?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VaGhcnQ8TlPp"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import AgentType, initialize_agent, load_tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2YF3Vb8S1gw"
      },
      "source": [
        "**Solution 0: Zero-shot learning**.\n",
        "\n",
        "* Create an [OpenAI](https://api.python.langchain.com/en/latest/llms/langchain.llms.openai.OpenAI.html) language model object.\n",
        "* Decide on a temperature value for the model. Explain your choice.\n",
        "* Use the language model to get a response to the riddle. No prompt engineering at this point.\n",
        "* Note AI's answer to the riddle. Is it correct or not? Discuss your observations on how well the model responded to the riddle. **(0.25 point)**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWc19HXPvL1C"
      },
      "outputs": [],
      "source": [
        "temperature="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwhdDJPlTQUm"
      },
      "outputs": [],
      "source": [
        "## Create the llm\n",
        "from langchain.llms import OpenAI\n",
        "llm = OpenAI(temperature=, openai_api_key=openai_api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptwq9aPuvQY1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWS_8ZE0ga6j"
      },
      "source": [
        "**Answer**:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6Pl8n1tPMSw"
      },
      "source": [
        "**Solution 1: Zero-shot learning with a calculator**.\n",
        "\n",
        "\n",
        "Let's try enhancing the zero-shot learning approach by integrating a calculator tool with the language model. This setup aims to improve the accuracy and effectiveness of solving riddles, especially those involving mathematical elements.\n",
        "\n",
        "* Using [`load_tools`](https://python.langchain.com/docs/modules/agents/tools/), load the `llm-math` tool, which will serve as a calculator. This tool can be integrated with the language model to handle any mathematical computations required in the riddle-solving process.\n",
        "\n",
        "* Using [`initialize_agent`](https://python.langchain.com/docs/modules/agents/agent_types/openai_functions_agent#using-openaifunctionsagent), create a **zero-shot agent** with the `initialize_agent` function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmDia7l9v9s0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sttXw_7ySona"
      },
      "source": [
        "\n",
        "Invoke the agent with the riddle.\n",
        "\n",
        "\n",
        "* Show evidence that the calculator tool was utilized to obtain the response by examining the intermediate steps.  **(0.5 point)**\n",
        "* Evaluate whether the answer provided by the agent is correct. **(0.25 point)**\n",
        "* Reflect on whether you expected the calculator to aid in solving this riddle. Discuss if the calculator tool has actually contributed to finding the solution. Include in your dicussion an explanation of how and why (or why not). **(0.5 point)**\n",
        "\n",
        "* What prompt engineering framework was used here? Provide evidence. **(0.25 point)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-q-z0pvwE_q"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHNsAy32NAw2"
      },
      "source": [
        "**Answer:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x12N7QsZYPE0"
      },
      "source": [
        "**Solution 3: Few-shot Learning**.\n",
        "\n",
        "Now, you'll provide the model with examples to learn from. This approach aims to improve AI's ability to solve riddles by presenting it with similar problems and their solutions.\n",
        "\n",
        "* Create three examples of riddles that are similar to the one you are given. For each example riddle, only provide a clear and correct *final* solution. This will serve as a learning reference for the AI. **(0.25 point)**\n",
        "* Create a `PromptTemplate` that formats the example riddles and their solutions appropriately. Utilize the prompt to create a `FewShotPromptTemplate`. This template will be used to present the example riddles and their solutions to the AI in a way that facilitates learning. Refer to [this reference](https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/few_shot_examples) for guidance.\n",
        "* Introduce slight variations in the answers to reduce *social* biases. Consider adding other variations that could enhance learning. Discuss your implementation. **(0.25 point)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6q99Wr5Yr2x"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0ZQKnHqhsq_"
      },
      "source": [
        "**Answer**:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDSfNehXbp-D"
      },
      "source": [
        "Now, call the `llm` object with the template after properly substituting the riddle into it.\n",
        "\n",
        "\n",
        "* Explain how you obtained the used examples. **(0.25 point)**\n",
        "* Show evidence that the template was utilized to obtain the response by examining the intermediate steps.  **(0.25 point)**\n",
        "* Evaluate whether the answer provided by the framework is correct. **(0.25 point)**\n",
        "* Reflect on whether you expected the framework to aid in solving this riddle. Discuss if it has actually contributed to finding the solution. Include in your dicussion an explanation of how and why (or why not). **(0.5 point)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJxV9JAibuvK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mS8-RjOcNMf"
      },
      "source": [
        "**Answer:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rRbicTzTxST"
      },
      "source": [
        "**Solution 3: Chain of Thought (CoT)**.\n",
        "\n",
        "\n",
        "Finally, you'll guide the model through a step-by-step process, breaking down the solution into clear, logical steps. This CoT approach helps the AI model understand the reasoning process needed to arrive at the correct answer.\n",
        "\n",
        "\n",
        "* Create a detailed example of a similar riddle and logically break it down into steps. The steps involve translating the problem into an alogrithm with equation solving. Clearly outline each step. **(0.25 point)**\n",
        "* Ensure that each step in the solution is explained clearly and simply. Avoid complex jargon or overly technical language. Make sure the steps follow a logical sequence that is easy to understand and replicate.\n",
        "* Use the same `PromptTemplate` object from the previous solution to maintain consistency in formatting.\n",
        "* Create a new `FewShotPromptTemplate` that incorporates the detailed example you've created. This template will guide the AI model in applying a similar chain of thought to solve other riddles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQtM-uM97N2K"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxAPdIyFXNDS"
      },
      "source": [
        "Now, call the `llm` object with the template after properly substituting the riddle into it.\n",
        "\n",
        "\n",
        "* Evaluate whether the answer provided by the framework is correct. **(0.25 point)**\n",
        "* Reflect on whether you expected the framework to aid in solving this riddle. Discuss if it has actually contributed to finding the solution. Include in your dicussion an explanation of how and why (or why not). **(0.5 point)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2u2FmoeS_wwA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neU7tYXTWonF"
      },
      "source": [
        "**Answer:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LpOT0SpvstK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
