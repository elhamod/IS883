{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elhamod/IS883/blob/main/Assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGwQeMHQqY_3"
      },
      "source": [
        "#**Using Transformers for language modeling**\n",
        "\n",
        "In this assignment, you will experiment with using transformers to solve two different language modeling roblems: Text generation and translation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUd9MZg95E79"
      },
      "source": [
        "- Some packages you may need. You are free to use alternative ones, but this should make your task simpler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUKJ_GcK5Ela"
      },
      "outputs": [],
      "source": [
        "# You only need to run this once when you load the notebook to install required packages. You can comment this cell out once you run it.\n",
        "\n",
        "# !pip install torch\n",
        "!pip install datasets\n",
        "!pip install apache_beam mwparserfromhell\n",
        "!pip install transformers[torch]\n",
        "!pip install sentence_transformers\n",
        "!pip install evaluate\n",
        "!pip install accelerate -U"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " - Connect to Google Drive"
      ],
      "metadata": {
        "id": "3a33z_MdOdvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "qEyFXeXQOWP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8CUJw2KAMSC"
      },
      "source": [
        "- Check if GPU is available. If so, it should print `cuda`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EmBx6OTIAJFb"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOEHB0JR1wyc"
      },
      "source": [
        "##**Part 1: Using a Transformer to model Wikipedia text**\n",
        "\n",
        "You will use a GPT2 Transformer to model the data [simple Wikipedia dataset](https://huggingface.co/datasets/wikipedia/viewer/20220301.simple/train). Our goal is to generate Wikipedia-sounding articles that sound novel but also believable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWMD0fOF5Zuh"
      },
      "source": [
        "- Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWVmDdrq5Y9e"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "wikipedia_simple_dataset = load_dataset(\"wikipedia\", \"20220301.simple\")\n",
        "\n",
        "print(\"dataset structure is\", wikipedia_simple_dataset)\n",
        "\n",
        "print(\"an example of a training sequence is\", wikipedia_simple_dataset[\"train\"][\"text\"][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdKDU7ao5n9i"
      },
      "source": [
        "- Split the dataset into a training set (the first 300 articles) and a the test set (the last 60 articles)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_2Kl4fl5p-z"
      },
      "outputs": [],
      "source": [
        "# Check the total number of rows in the dataset\n",
        "total_rows = len(wikipedia_simple_dataset[\"train\"])\n",
        "\n",
        "# Define the indices for splitting the dataset\n",
        "train_end_idx = 300  # The end index for the training set\n",
        "test_start_idx = total_rows - 60  # The start index for the test set\n",
        "\n",
        "# Ensure the dataset has at least 12k rows\n",
        "if total_rows < 360:\n",
        "    raise ValueError(\"The dataset has fewer than 360 rows.\")\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "train_dataset = wikipedia_simple_dataset[\"train\"].select(range(train_end_idx))\n",
        "test_dataset = wikipedia_simple_dataset[\"train\"].select(range(test_start_idx, total_rows))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-v4vZaLA25PT"
      },
      "source": [
        "1. **(1 point)** Start from a *pretrained* GPT2 transformer with a context of 512 tokens with padding, such that:\n",
        "  - Print the training and test losses every epoch.\n",
        "  - Save the model that performs best on the **test set** as `best_model`\n",
        "  - Train for 10 epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Create the tokenizer and tokenize the dataset"
      ],
      "metadata": {
        "id": "uvgypv9xPb6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_len = 512"
      ],
      "metadata": {
        "id": "XQcpC8QeCc86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYlnFUk2-DyT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Create the model"
      ],
      "metadata": {
        "id": "f6CsE3KGSTkN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRE91bfa-Ewb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Create a perplexity metric and a `compute_metric` function to measure the perplexity."
      ],
      "metadata": {
        "id": "Kq5ojgD0S1Ha"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8WdV9AZ_KlV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Train the model"
      ],
      "metadata": {
        "id": "Pu3_DYhvTMZQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N341Rjsd-J0y"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Save the best model to your Google Drive to path `/content/drive/MyDrive/IS883_HW2/best_model_wiki`\n",
        "\n"
      ],
      "metadata": {
        "id": "bL-MPDXxOB-I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKtUwgzoTIy2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6: Now load the model back and assign it to best_model"
      ],
      "metadata": {
        "id": "Zm9gJwavMTb9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbuIS2VO-X1_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EocqT3zA54Ec"
      },
      "source": [
        "2. **(1 point)** Write a function that generates text using `best_model`. This function takes the following parameters:\n",
        "\n",
        "  - *temperature*: has a default value 1.0.\n",
        "  - *max_gen_tokens*: specifies the maximum number of tokens in the generated text. Default value is 40.\n",
        "  - *prefix*: default value `tokenizer.bos_token` (i.e., beginning of sentence token).\n",
        "\n",
        "Each time the function is called, it generates 5 possible unique texts. Also, use sampling to avoid generating identical texts.\n",
        "\n",
        "Use the function and generate some texts with different temperatures and prefixes. Comment on the quality of the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xaqapc5R8nlm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPJl5QHT7WAq"
      },
      "source": [
        "Call the function here to generate 5 different texts. The texts should not be identical."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-VaR48j7fgC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9vmWB717-9g"
      },
      "source": [
        "3. **(1 point)** Calculate the perplexity of `best_model` on the test set.\n",
        "\n",
        "Generally, a perplexity lower than 30 is desired. Have you been able to achieve it? If not, would you expect more hyper-parameter tuning to solve the issue? Elaborate and reflect on your answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2jSQ7zc7uI_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "31J5OdtUZFpD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ki_j2ew-7fsd"
      },
      "source": [
        "4. **(1.5 point)** Now, train a new GPT2. This model `model_from_scratch` is identical to `best_model`, except that it is trained **from scratch**.\n",
        "Once done:\n",
        "\n",
        "  - Calculate the perplexity on the test set.\n",
        "  - Generate some texts.\n",
        "  - Which model is better `best_model` or `model_from_scratch`? Justify and reflect on your answers."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the model and train."
      ],
      "metadata": {
        "id": "5RsiHbgvNP4h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjCD_uyq3sZF"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate perplexity"
      ],
      "metadata": {
        "id": "GO6O_OVzNQ5I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSYdW3PB42Uh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate texts"
      ],
      "metadata": {
        "id": "I7p6qlCWNawz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MtfKfSAQMUcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MUQplolNYu21"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Delete your model and clear `cuda` cache for next experiment.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "U5GtoKeFHw2f"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f5-xwuX1OI76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear GPU cache\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "b1SPEo1kHwhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUHcTinh5nuA"
      },
      "source": [
        "##**Part 2: Using an language models for translation**\n",
        "\n",
        "Here, you will use an *appropriate* language model of your choice and train it on a dataset that has English-to-French song translations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9vHUlIu6Dse"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"Nicolas-BZRD/Original_Songs_Lyrics_with_French_Translation\")\n",
        "\n",
        "# Define a function to check if either 'original_version' or 'french_version' are None\n",
        "def filter_rows(example):\n",
        "    return example['original_version'] is not None and example['french_version'] is not None\n",
        "\n",
        "# Filter the dataset\n",
        "dataset = dataset.filter(filter_rows)\n",
        "\n",
        "print(\"An example row from this dataset\")\n",
        "dataset['train'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyGkzA4O7A2F"
      },
      "source": [
        "  - Split the dataset into a training set (the first 300 songs) and a test set (the last 60 songs).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFkxCeoj7HC7"
      },
      "outputs": [],
      "source": [
        "# Check the total number of rows in the dataset\n",
        "total_rows = len(dataset[\"train\"])\n",
        "\n",
        "# Ensure the dataset has at least 22k rows\n",
        "if total_rows < 660:\n",
        "    raise ValueError(\"The dataset has fewer than 360 rows.\")\n",
        "\n",
        "# Define the indices for splitting the dataset\n",
        "train_end_idx = 300  # The end index for the training set\n",
        "test_start_idx = total_rows - 60  # The start index for the test set\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "train_dataset = dataset[\"train\"].select(range(train_end_idx))\n",
        "test_dataset = dataset[\"train\"].select(range(test_start_idx, total_rows))\n",
        "\n",
        "# Print the number of rows in training and test sets\n",
        "print(f\"Number of rows in training set: {len(train_dataset)}\")\n",
        "print(f\"Number of rows in test set: {len(test_dataset)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRLgeakE6n1L"
      },
      "source": [
        "1. **(1.5 point)** Choose a good **pre-trained** model for this task. Explain your criteria for choosing this model. It is highly recommended to select one from [HuggingFace official pre-trained models](https://huggingface.co/docs/transformers/index) or [HuggingFace user pre-trained models](https://huggingface.co/models)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the tokenizer. Use a `max_length` of 512. Remove all columns unnecessary for the translation.\n"
      ],
      "metadata": {
        "id": "YUhKjvcGax2b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_length=512"
      ],
      "metadata": {
        "id": "fgOleSadDBOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUX7pl9H69CI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the model"
      ],
      "metadata": {
        "id": "6xNjzT4ta5qt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7eHqT67ta6yN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model.\n",
        "\n",
        "  2. **(0.5 point)** You might find that your notebook runs out of memory or takes too long to train. What hyper-parameter could you change to address that?"
      ],
      "metadata": {
        "id": "VzYcAFQx7pD5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_EaMnWYUSVoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUjSUgpT9dRk"
      },
      "source": [
        "3. **(1 point)** Translate the following two sentences. Would your model make a good English-to-French translator? Justify your answer.\n",
        "\n",
        "  - \"Just let me hear some of that rock and roll music\"\n",
        "  - \"If you wanna dance with me\\nI've got no kick against modern jazz\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKJ9VoliEZ5p"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fE9Tt2i6IPHX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. **(0.5 point)** What would be a good metric for measuring the performance of this model? Could you calculate it for this pair of model and dataset? If yes, show your results and discuss them. If no, elaborate on the reason and how you would go about solving it."
      ],
      "metadata": {
        "id": "dIirjmbSjYOF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7lL7c7JhJiO7"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}