{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOjGvnPk4G337yzGukS7eMf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elhamod/IS883/blob/main/RNN_in_class_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNN on \"tiny-shakespear\"\n",
        "\n",
        "The following code trains an RNN on the [tiny-shakespear dataset](https://www.tensorflow.org/datasets/catalog/tiny_shakespeare)."
      ],
      "metadata": {
        "id": "lMLqTdJzFfqk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install tensorflow"
      ],
      "metadata": {
        "id": "C6vJ33saISIB"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
        "import numpy as np\n",
        "\n",
        "## Fetch the dataset.\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "## Convert the text to a sequence of integer indices representing characters.\n",
        "chars = sorted(set(text))\n",
        "char2idx = {u:i for i, u in enumerate(chars)}\n",
        "idx2char = np.array(chars)\n",
        "text_as_int = np.array([char2idx[c] for c in text])\n",
        "\n",
        "# Create training examples and targets\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "# Define and create the RNN\n",
        "vocab_size = len(chars)\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024\n",
        "model = Sequential([\n",
        "    Embedding(vocab_size, embedding_dim, batch_input_shape=[BATCH_SIZE, None]),\n",
        "    SimpleRNN(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
        "    Dense(vocab_size)\n",
        "])\n",
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "# Train the model\n",
        "model.compile(optimizer='adam', loss=loss)\n",
        "history = model.fit(dataset, epochs=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHNAe1PKHAh2",
        "outputId": "b3292f9d-7766-4c85-d24b-7ec8b0c5a7bc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "172/172 [==============================] - 27s 101ms/step - loss: 2.8490\n",
            "Epoch 2/10\n",
            "172/172 [==============================] - 17s 89ms/step - loss: 2.0390\n",
            "Epoch 3/10\n",
            "172/172 [==============================] - 18s 88ms/step - loss: 1.8431\n",
            "Epoch 4/10\n",
            "172/172 [==============================] - 17s 89ms/step - loss: 1.7085\n",
            "Epoch 5/10\n",
            "172/172 [==============================] - 19s 94ms/step - loss: 1.6169\n",
            "Epoch 6/10\n",
            "172/172 [==============================] - 17s 88ms/step - loss: 1.5523\n",
            "Epoch 7/10\n",
            "172/172 [==============================] - 17s 87ms/step - loss: 1.5043\n",
            "Epoch 8/10\n",
            "172/172 [==============================] - 17s 90ms/step - loss: 1.4673\n",
            "Epoch 9/10\n",
            "172/172 [==============================] - 18s 90ms/step - loss: 1.4377\n",
            "Epoch 10/10\n",
            "172/172 [==============================] - 18s 93ms/step - loss: 1.4121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate some text\n",
        "\n",
        "model.save_weights('./shakespeare_rnn_weights')  # Save the weights to the current directory\n",
        "\n",
        "# Define the model with batch size of 1 for text generation\n",
        "model_gen = Sequential([\n",
        "    Embedding(vocab_size, embedding_dim, batch_input_shape=[1, None]),\n",
        "    SimpleRNN(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
        "    Dense(vocab_size)\n",
        "])\n",
        "\n",
        "# Load the weights of the trained model\n",
        "model_gen.load_weights(tf.train.latest_checkpoint(\"./\"))\n",
        "\n",
        "# Build the model\n",
        "model_gen.build(tf.TensorShape([1, None]))\n",
        "\n",
        "\n",
        "# Define the seed text and other parameters\n",
        "seed_text = \"\"\n",
        "num_generate = 1000  # Number of characters to generate\n",
        "temperature = 1.0  # Higher value: more random, Lower value: more deterministic\n",
        "\n",
        "# Handle empty string seed by initializing with a random character from the vocabulary\n",
        "if not seed_text:\n",
        "    seed_text = np.random.choice(chars)  # Choose a random character from chars\n",
        "\n",
        "# Convert seed text to numerical representation\n",
        "input_eval = [char2idx[s] for s in seed_text]\n",
        "input_eval = tf.expand_dims(input_eval, 0)  # Add batch dimension\n",
        "\n",
        "# Initialize the list to hold the generated text\n",
        "text_generated = []\n",
        "\n",
        "# Reset the states of the model\n",
        "model_gen.reset_states()\n",
        "\n",
        "# Use model_gen for text generation\n",
        "for i in range(num_generate):\n",
        "    predictions = model_gen(input_eval)\n",
        "    predictions = tf.squeeze(predictions, 0)  # remove the batch dimension\n",
        "\n",
        "    # Use a categorical distribution to sample the character returned by the model\n",
        "    predictions = predictions / temperature\n",
        "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "    # Pass the predicted character as the next input to the model along with the previous hidden state\n",
        "    input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "# Combine the seed text with the generated text\n",
        "generated_text = seed_text + ''.join(text_generated)\n",
        "print(generated_text)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUuiFExvKI97",
        "outputId": "66228521-4ea4-4f29-866a-64cfa236ff75"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "?\n",
            "\n",
            "GRUMIO:\n",
            "Not once again;\n",
            "And lived, madam:\n",
            "I loved mystand,\n",
            "That land to reath to inveltire my leave foll carrain. O will an' calls to Rone!\n",
            "\n",
            "Second Servingman:\n",
            "Marching a gue offend\n",
            "Thy justice, and all desper you\n",
            "Of prove me come propesst freely.\n",
            "\n",
            "BENVILINE:\n",
            "It is the demation of\n",
            "Tup mine, come fool the ling of retutt was the days is nother?\n",
            "\n",
            "CLIFFORD:\n",
            "We are not to art tappaintation.\n",
            "\n",
            "CAMILLO:\n",
            "Marcius here storm.\n",
            "\n",
            "YORK:\n",
            "I shall I do\n",
            "you weak infains\n",
            "Callageshal-sowinn of too.\n",
            "\n",
            "PAULIGH:\n",
            "Best on a happy said, mine ownet disnowift dast office. What, uncle may s,\n",
            "So more men as we new warm\n",
            "parte:\n",
            "You know, keaven cansless thich.\n",
            "Madam, say's by int. A neitness:\n",
            "Poor signior off?\n",
            "If now the rest,\n",
            "As so desire 'twixt that hoted and this sing hereage.\n",
            "\n",
            "POLIXENES:\n",
            "When your hasse?\n",
            "Up of my years corseowords: the kill's. would make no marriour will\n",
            "Your earthy shall be many in plagues thine.\n",
            "Thou hast said, heir die! sound not\n",
            "by your stoprey,\n",
            "Nay, it thank this sturs\n",
            "Known fear'ly fault is\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Improvements:\n",
        "\n",
        "\n",
        "1. Is the training slow or fast? If slow, how to improve it?\n",
        "2. Try generating text using the model without starting with any context. What do you notice? Why did the text not meet your expectation? How can you improve it? Hints: (context window? tokenization?).  \n",
        "\n"
      ],
      "metadata": {
        "id": "GrE1DLslJJgI"
      }
    }
  ]
}