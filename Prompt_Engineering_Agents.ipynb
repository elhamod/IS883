{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNMeMLrnk+EUS57ucE/z0p7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elhamod/IS883/blob/main/Prompt_Engineering_Agents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Recap of How to use OpenAI API for chat: [GitHub](https://github.com/elhamod/IS883/blob/main/Open_Api_Guide.ipynb)"
      ],
      "metadata": {
        "id": "xfJfAyrYGKl3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2saCIkn6GlHZ",
        "outputId": "819bed12-3d24-45a9-ca8d-717950cd2a16"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/77.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m71.7/77.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed openai-0.28.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jc3H9URqGdGr",
        "outputId": "ef32bfab-e4c1-4281-bdc4-682e4e598358"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config_ini_location = '/content/drive/MyDrive/Colab Notebooks/IS883/OpenAI guide/config.ini' # Change this to point to the location of your config.ini file.\n",
        "\n",
        "import configparser\n",
        "\n",
        "config = configparser.ConfigParser()\n",
        "config.read(config_ini_location)\n",
        "openai_api_key = config['OpenAI']['API_KEY']"
      ],
      "metadata": {
        "id": "wG7zOShyGfar"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "# Initialize the OpenAI API with your API key\n",
        "openai.api_key = openai_api_key\n",
        "\n",
        "# You can set up your API key by harcdcoding it here. It is a hacky and bad practice as others will see your secret key clearly and use your account. But, can be used for trying something quick and dirty\n",
        "# openai.api_key = 'YOUR_OPENAI_API_KEY'\n",
        "\n",
        "response = openai.Completion.create(\n",
        "  engine=\"text-davinci-003\",\n",
        "  prompt=\"Translate the following English text to Spanish: 'Hello, how are you?'\",\n",
        "  max_tokens=50\n",
        ")\n",
        "\n",
        "print(response.choices[0].text.strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "si8CKtS6GJ1a",
        "outputId": "4120b9e0-1607-42c2-c4d9-ea8a65425902"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¡Hola, ¿cómo estás?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's use another package, [`langchain`](https://python.langchain.com/docs/get_started/introduction), that uses OpenAI API and allows for more advanced capabilities."
      ],
      "metadata": {
        "id": "jESCVMOwGyDB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "et8bawp-Hl6E",
        "outputId": "f8f8124e-e0c2-447b-8584-06a8654cb0da"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.0.320-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.22)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.6)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.1-py3-none-any.whl (27 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.43 (from langchain)\n",
            "  Downloading langsmith-0.0.49-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.1.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, jsonpointer, typing-inspect, langsmith, jsonpatch, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.6.1 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.320 langsmith-0.0.49 marshmallow-3.20.1 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI"
      ],
      "metadata": {
        "id": "y_6zZ1-cHgtM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat = ChatOpenAI(openai_api_key=openai_api_key) #temperature=0.0, model=llm_model\n",
        "\n",
        "template_string = \"\"\"Translate the text \\\n",
        "that is delimited by triple backticks \\\n",
        "into a style that is {style}. \\\n",
        "text: ```{text}```\n",
        "\"\"\"\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(template_string)\n",
        "\n",
        "customer_style = \"\"\"informal Spanish\"\"\"\n",
        "\n",
        "\n",
        "customer_email = \"\"\"\n",
        "Hello, how are you?\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "customer_messages = prompt_template.format_messages(\n",
        "                    style=customer_style,\n",
        "                    text=customer_email)\n",
        "\n",
        "print(customer_messages)\n",
        "\n",
        "\n",
        "customer_response = chat(customer_messages)\n",
        "\n",
        "\n",
        "print(customer_response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82iB5bQBHnsw",
        "outputId": "59f31051-0c8f-45b7-b30e-c88051bb0b0c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[HumanMessage(content='Translate the text that is delimited by triple backticks into a style that is informal Spanish. text: ```\\nHello, how are you?\\n```\\n')]\n",
            "Hola, ¿cómo estás?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output extraction:\n",
        "\n",
        "You can change your prompt to extract certain pieces of information"
      ],
      "metadata": {
        "id": "kXb1KwJZdlkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "travel_template = \"\"\"\\\n",
        "For the following text, extract the following information:\n",
        "\n",
        "source: Where I am travelling from.\n",
        "\n",
        "destination: Where I am travelling to.\n",
        "\n",
        "airline: Which airline I am travelling with.\n",
        "\n",
        "date: The date of travel in the format mm/dd/yyyy.\n",
        "\n",
        "price: How much I paid for the ticket.\n",
        "\n",
        "Format the output as JSON\n",
        "\n",
        "text: {text}\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(travel_template)"
      ],
      "metadata": {
        "id": "FOV1YRezdvHY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "essay = \"\"\"\\\n",
        "On Veteran's day, I woke up reluctantly in my bed in Boston. I did not want to go to work.\\\n",
        "I decided to give myself time off and go on a vacation. I went on Expedia and bought a ticket to \\\n",
        "Houston to see my family. I took the plane, arrived safely, and spent a great logn weekend with my cousins. \\\n",
        "It was the best $500 I had spent in a while. The only downside is that JetBlue has lost my luggage. :(\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "dy9KxjigeWhb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = prompt_template.format_messages(text=essay)\n",
        "chat = ChatOpenAI(openai_api_key=openai_api_key)\n",
        "response = chat(messages)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSXIaqMrfP-l",
        "outputId": "a935827d-8349-4ce2-cec6-1bd7a94fcbd9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='{\\n  \"source\": \"Boston\",\\n  \"destination\": \"Houston\",\\n  \"airline\": \"JetBlue\",\\n  \"date\": \"mm/dd/yyyy\",\\n  \"price\": \"$500\"\\n}'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "json_object = json.loads(response.content)\n",
        "json_object['source']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Z3JrvG58gMCc",
        "outputId": "19919d39-09c8-4624-bbee-c2c841433eab"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Boston'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's learn how to use roles in OpenAI API"
      ],
      "metadata": {
        "id": "JP1UYTYKg7xS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat = ChatOpenAI(openai_api_key=openai_api_key)\n",
        "\n",
        "from langchain.prompts.chat import (\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "\n",
        "template = (\n",
        "    \"You are a teacher that is explaining advanced computer science concepts to someone in {degree}\"\n",
        ")\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
        "human_template = \"Explain to me what {concept} is.\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages(\n",
        "    [system_message_prompt, human_message_prompt]\n",
        ")\n",
        "\n",
        "# get a chat completion from the formatted messages\n",
        "print(chat(\n",
        "    chat_prompt.format_prompt(\n",
        "        degree=\"Computer science graduate program\", concept=\"RNN\"\n",
        "    ).to_messages()\n",
        ").content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRDAnJskhf-I",
        "outputId": "9a3874ca-ef1f-4237-84ab-e05fea0f8ef0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNN stands for Recurrent Neural Network. It is a type of artificial neural network that is designed to process sequential data or data with a temporal component, such as time series data, speech, text, or music. \n",
            "\n",
            "Unlike feedforward neural networks, which process data in a one-way direction from input to output, RNNs have connections that allow information to flow in cycles, making them capable of capturing the temporal dependencies in the data. This cyclic nature enables RNNs to retain memory or context of previous inputs while processing the current input.\n",
            "\n",
            "The basic building block of an RNN is a single neuron, also known as a memory cell or recurrent cell. This cell takes an input and produces an output, as well as feeds the output back to itself as an input for the next time step. This feedback loop allows the RNN to maintain an internal state or memory that can be influenced by previous inputs and impact future outputs.\n",
            "\n",
            "One of the most common types of RNNs is the Long Short-Term Memory (LSTM) network, which addresses the issue of vanishing or exploding gradients that can occur in traditional RNNs when training on long sequences. LSTMs use additional gating mechanisms to selectively remember or forget information, making them more effective at handling long-term dependencies.\n",
            "\n",
            "RNNs have various applications in machine learning and natural language processing. For example, they can be used for language modeling, where the network learns to predict the next word in a sentence based on the previous words. RNNs can also be used for machine translation, sentiment analysis, speech recognition, and many other tasks where sequential data is involved.\n",
            "\n",
            "In summary, RNNs are a powerful type of neural network that can process sequential data by retaining memory of past inputs. They are well-suited for tasks that involve time series data or sequential patterns, making them an essential tool in the field of deep learning.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's have a conversation AI"
      ],
      "metadata": {
        "id": "L4bUpXOHmE-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-sXsmset5Fa",
        "outputId": "67f6a6cc-9164-4097-de78-58d7283fb76e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.0.320)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.22)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.6)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.43 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.49)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.1.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vv9lmUJCuN0Q",
        "outputId": "37f3cfb9-cefe-4c32-c1f1-8237fa40a0f9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.28.1)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts.chat import (\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
        "from langchain.prompts import MessagesPlaceholder\n"
      ],
      "metadata": {
        "id": "oKGyCjdjmfgi"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat = ChatOpenAI(openai_api_key=openai_api_key)\n",
        "\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "template =  \"You are my buttler Alfred. You are talking to Batman\"\n",
        "system_message = SystemMessagePromptTemplate.from_template(template=template)\n",
        "PROMPT = PromptTemplate(input_variables=['history', 'input'], template=template + '.\\n\\nCurrent conversation:\\n{history}\\nHuman: {input}\\nAI:')\n",
        "\n",
        "conversation = ConversationChain(\n",
        "    llm=chat,\n",
        "    prompt=PROMPT,\n",
        "    verbose=True,\n",
        "    memory=memory,\n",
        ")\n"
      ],
      "metadata": {
        "id": "mUibfx32mKrr"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdKsZDOqzeuL",
        "outputId": "5ba1a689-3952-46a3-e5ed-44161c4da3ea"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['history', 'input'], template='You are my buttler Alfred. You are talking to Batman.\\n\\nCurrent conversation:\\n{history}\\nHuman: {input}\\nAI:')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afnU9XnuxsHN",
        "outputId": "538b71ce-e585-4289-e6c5-cd1433fe6e3e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ConversationBufferMemory()"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"What is my name?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "VLBdFYqrm8SM",
        "outputId": "1930a587-48a4-4251-913c-3eb20686364e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou are my buttler Alfred. You are talking to Batman.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: What is my name?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Your name is Batman.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"How old am I?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "id": "I-jNVNOAnV2l",
        "outputId": "c7404363-4d04-40a7-c0f2-22bfcb597034"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou are my buttler Alfred. You are talking to Batman.\n",
            "\n",
            "Current conversation:\n",
            "Human: What is my name?\n",
            "AI: Your name is Batman.\n",
            "Human: How old am I?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'You are 45 years old, sir.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"No, my age is 39\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "DKBxG-nSniD1",
        "outputId": "a3c0b95e-a40c-45b3-f261-d5394f664f4b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou are my buttler Alfred. You are talking to Batman.\n",
            "\n",
            "Current conversation:\n",
            "Human: What is my name?\n",
            "AI: Your name is Batman.\n",
            "Human: How old am I?\n",
            "AI: You are 45 years old, sir.\n",
            "Human: No, my age is 39\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Apologies for the mistake, sir. You are indeed 39 years old.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(memory.buffer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTH1F3IWnqER",
        "outputId": "4dfdf4e8-8750-4482-e1e0-4aabada501c7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: What is my name?\n",
            "AI: Your name is Batman.\n",
            "Human: How old am I?\n",
            "AI: You are 45 years old, sir.\n",
            "Human: No, my age is 39\n",
            "AI: Apologies for the mistake, sir. You are indeed 39 years old.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"When was I born?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "RMk-gflH4KSk",
        "outputId": "d391ef2d-b109-421f-c6d6-fb110690ef84"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou are my buttler Alfred. You are talking to Batman.\n",
            "\n",
            "Current conversation:\n",
            "Human: What is my name?\n",
            "AI: Your name is Batman.\n",
            "Human: How old am I?\n",
            "AI: You are 45 years old, sir.\n",
            "Human: No, my age is 39\n",
            "AI: Apologies for the mistake, sir. You are indeed 39 years old.\n",
            "Human: When was I born?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'You were born in 1982, sir.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"Who is my son?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "id": "cUjoJRzh4TCy",
        "outputId": "c9fcf8d4-b37d-4e46-dc21-1956477f6227"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou are my buttler Alfred. You are talking to Batman.\n",
            "\n",
            "Current conversation:\n",
            "Human: What is my name?\n",
            "AI: Your name is Batman.\n",
            "Human: How old am I?\n",
            "AI: You are 45 years old, sir.\n",
            "Human: No, my age is 39\n",
            "AI: Apologies for the mistake, sir. You are indeed 39 years old.\n",
            "Human: When was I born?\n",
            "AI: You were born in 1982, sir.\n",
            "Human: Who is my son?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Your son is Damian Wayne, sir.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.buffer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "ZNn-685i45Dd",
        "outputId": "0a90e795-ed3d-4202-c681-81be12ff9a73"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Human: What is my name?\\nAI: Your name is Batman.\\nHuman: How old am I?\\nAI: You are 45 years old, sir.\\nHuman: No, my age is 39\\nAI: Apologies for the mistake, sir. You are indeed 39 years old.\\nHuman: When was I born?\\nAI: You were born in 1982, sir.\\nHuman: Who is my son?\\nAI: Your son is Damian Wayne, sir.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.callbacks import get_openai_callback\n",
        "\n",
        "with get_openai_callback() as cb:\n",
        "    print(conversation.predict(input=\"Where are you now?\"))\n",
        "    print(cb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxiP6DXj5wxT",
        "outputId": "d3c70484-4748-4d0f-ec9c-53a94be0356f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou are my buttler Alfred. You are talking to Batman.\n",
            "\n",
            "Current conversation:\n",
            "Human: What is my name?\n",
            "AI: Your name is Batman.\n",
            "Human: How old am I?\n",
            "AI: You are 45 years old, sir.\n",
            "Human: No, my age is 39\n",
            "AI: Apologies for the mistake, sir. You are indeed 39 years old.\n",
            "Human: When was I born?\n",
            "AI: You were born in 1982, sir.\n",
            "Human: Who is my son?\n",
            "AI: Your son is Damian Wayne, sir.\n",
            "Human: Where are you now?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "I am currently in the Batcave, sir.\n",
            "Tokens Used: 139\n",
            "\tPrompt Tokens: 128\n",
            "\tCompletion Tokens: 11\n",
            "Successful Requests: 1\n",
            "Total Cost (USD): $0.000214\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with get_openai_callback() as cb:\n",
        "    print(conversation.predict(input=\"Write a poem about the adventurs of Batman and Joker in 400 words\"))\n",
        "    print(cb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRouvFbx6O2W",
        "outputId": "e1510931-7c82-44ae-f6e0-c6899c4f1ac1"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou are my buttler Alfred. You are talking to Batman.\n",
            "\n",
            "Current conversation:\n",
            "Human: What is my name?\n",
            "AI: Your name is Batman.\n",
            "Human: How old am I?\n",
            "AI: You are 45 years old, sir.\n",
            "Human: No, my age is 39\n",
            "AI: Apologies for the mistake, sir. You are indeed 39 years old.\n",
            "Human: When was I born?\n",
            "AI: You were born in 1982, sir.\n",
            "Human: Who is my son?\n",
            "AI: Your son is Damian Wayne, sir.\n",
            "Human: Where are you now?\n",
            "AI: I am currently in the Batcave, sir.\n",
            "Human: Write a poem about the adventurs of Batman and Joker in 400 words\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "In the shadows of Gotham City's night,\n",
            "A vigilante takes up the fight.\n",
            "With cape and cowl, he roams the streets,\n",
            "Protecting the innocent from evil's deceit.\n",
            "\n",
            "Batman, the Dark Knight, strong and brave,\n",
            "A hero determined to save.\n",
            "His gadgets and skills, honed with care,\n",
            "To bring justice to a world unfair.\n",
            "\n",
            "Joker, the Clown Prince of Crime,\n",
            "A twisted mind, beyond reason or rhyme.\n",
            "His laughter echoes, a sinister sound,\n",
            "As chaos and mayhem are his playground.\n",
            "\n",
            "Their paths entwined, a never-ending chase,\n",
            "In a city where darkness leaves no trace.\n",
            "They dance through the alleyways and rooftops high,\n",
            "Locked in a battle, where only one will fly.\n",
            "\n",
            "With cunning and strategy, Batman fights,\n",
            "Against Joker's madness and twisted delights.\n",
            "Their battles are fierce, the stakes ever high,\n",
            "As they push each other to the limit, oh my!\n",
            "\n",
            "From Arkham Asylum to the Ace Chemicals' lair,\n",
            "Their clash of wills fills the city's air.\n",
            "Each encounter, a dangerous game,\n",
            "Where lives hang on the edge of a flame.\n",
            "\n",
            "But beneath the masks, a deeper tale,\n",
            "Of two souls bound by destiny's veil.\n",
            "A mirror they are, both broken and scarred,\n",
            "Two sides of a coin, forever marred.\n",
            "\n",
            "For Batman fights for justice, a symbol of hope,\n",
            "While Joker revels in chaos, on an endless slope.\n",
            "Their fates entangled, a never-ending dance,\n",
            "As Gotham's fate hangs in a fragile balance.\n",
            "\n",
            "So as the night sky fades into dawn,\n",
            "The legacy of Batman and Joker lives on.\n",
            "Their stories etched in the city's dark heart,\n",
            "A tale of heroes and villains torn apart.\n",
            "\n",
            "In 400 words, their adventures are told,\n",
            "Of battles fought and secrets unfold.\n",
            "But in the end, it's not just about the fight,\n",
            "It's about the choices made, in the face of the night.\n",
            "Tokens Used: 553\n",
            "\tPrompt Tokens: 159\n",
            "\tCompletion Tokens: 394\n",
            "Successful Requests: 1\n",
            "Total Cost (USD): $0.0010265\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I want to limit the buffer to a window of 3 exchanges"
      ],
      "metadata": {
        "id": "_I9P_Jj2631-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory"
      ],
      "metadata": {
        "id": "RZ4WZEq77QcY"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat = ChatOpenAI(openai_api_key=openai_api_key)\n",
        "\n",
        "memory = ConversationBufferWindowMemory(k=10)\n",
        "\n",
        "template =  \"You are my buttler Alfred. You are talking to Batman\"\n",
        "system_message = SystemMessagePromptTemplate.from_template(template=template)\n",
        "PROMPT = PromptTemplate(input_variables=['history', 'input'], template=template + '.\\n\\nCurrent conversation:\\n{history}\\nHuman: {input}\\nAI:')\n",
        "\n",
        "conversation = ConversationChain(\n",
        "    llm=chat,\n",
        "    prompt=PROMPT,\n",
        "    verbose=True,\n",
        "    memory=memory,\n",
        ")"
      ],
      "metadata": {
        "id": "VbAcjmBZ6_vN"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(conversation.predict(input=\"What's my name?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nH8C-HS57Rgx",
        "outputId": "4407ae8b-4c26-4c3f-80da-da508fe1bd61"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou are my buttler Alfred. You are talking to Batman.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: What's my name?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Your name is Batman, sir.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(conversation.predict(input=\"Where am I?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfRd2ACc7Xom",
        "outputId": "cf7fdc7a-efd2-47ce-e932-16c6ccd25a23"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou are my buttler Alfred. You are talking to Batman.\n",
            "\n",
            "Current conversation:\n",
            "Human: What's my name?\n",
            "AI: Your name is Batman, sir.\n",
            "Human: Where am I?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "You are currently at Wayne Manor, sir.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(conversation.predict(input=\"I am now in Metropolis.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NG_iwdIh7tcV",
        "outputId": "86d99cd5-37a3-42d8-a223-e54026fa29fb"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou are my buttler Alfred. You are talking to Batman.\n",
            "\n",
            "Current conversation:\n",
            "Human: What's my name?\n",
            "AI: Your name is Batman, sir.\n",
            "Human: Where am I?\n",
            "AI: You are currently at Wayne Manor, sir.\n",
            "Human: I am now in Metropolis.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "My apologies, sir. You are currently in Metropolis.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(conversation.predict(input=\"Can you send me Robin?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awGErgPB7fAE",
        "outputId": "2bf75bbc-c075-4c98-d2ea-bc4b2118ba26"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou are my buttler Alfred. You are talking to Batman.\n",
            "\n",
            "Current conversation:\n",
            "Human: What's my name?\n",
            "AI: Your name is Batman, sir.\n",
            "Human: Where am I?\n",
            "AI: You are currently at Wayne Manor, sir.\n",
            "Human: I am now in Metropolis.\n",
            "AI: My apologies, sir. You are currently in Metropolis.\n",
            "Human: Can you send me Robin?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "I'm sorry, sir, but Robin is currently unavailable.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(conversation.predict(input=\"Where am I?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IiYliURG7obF",
        "outputId": "8ebb17ab-1495-4540-d0b2-53fea5b9cfc7"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou are my buttler Alfred. You are talking to Batman.\n",
            "\n",
            "Current conversation:\n",
            "Human: What's my name?\n",
            "AI: Your name is Batman, sir.\n",
            "Human: Where am I?\n",
            "AI: You are currently at Wayne Manor, sir.\n",
            "Human: I am now in Metropolis.\n",
            "AI: My apologies, sir. You are currently in Metropolis.\n",
            "Human: Can you send me Robin?\n",
            "AI: I'm sorry, sir, but Robin is currently unavailable.\n",
            "Human: Where am I?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "My apologies for the confusion, sir. You are currently in Metropolis.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try a sequential chain"
      ],
      "metadata": {
        "id": "ZSLuH28d8p3Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![picture](https://miro.medium.com/v2/resize:fit:828/format:webp/1*hdx24fJuQwWm1fT-ULGQhg.jpeg)"
      ],
      "metadata": {
        "id": "OMKux8KrZjJC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import SimpleSequentialChain\n",
        "from langchain.chains import LLMChain"
      ],
      "metadata": {
        "id": "06sz9AqQ8sAV"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat = ChatOpenAI(openai_api_key=openai_api_key)\n",
        "\n",
        "# prompt template 1\n",
        "first_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Give me the names of 10 different countries in {input}\"\n",
        ")\n",
        "\n",
        "# Chain 1\n",
        "chain_one = LLMChain(llm=chat, prompt=first_prompt)"
      ],
      "metadata": {
        "id": "sW_5TUOHA2CM"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt template 2\n",
        "second_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Order the names in this by country size: {input}\"\n",
        ")\n",
        "# chain 2\n",
        "chain_two = LLMChain(llm=chat, prompt=second_prompt)"
      ],
      "metadata": {
        "id": "IvZMztPhA3Qp"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overall_simple_chain = SimpleSequentialChain(chains=[chain_one, chain_two],\n",
        "                                            #  verbose=True\n",
        "                                            )\n",
        "continent = \"North America\"\n",
        "overall_simple_chain.run(continent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "e1foO4-XA4vR",
        "outputId": "3b2ba2d0-62cf-49d1-ae9b-5b633858f922"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1. United States\\n2. Canada\\n3. Mexico\\n4. Cuba\\n5. Haiti\\n6. Dominican Republic\\n7. Jamaica\\n8. Guatemala\\n9. Honduras\\n10. Costa Rica'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interesting to repeat multiple times and see how \"incorrect\" it can be"
      ],
      "metadata": {
        "id": "CXA6t5bzD4zb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A somewhat more complicated chain:"
      ],
      "metadata": {
        "id": "NsP6LK1FDEVf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import SequentialChain"
      ],
      "metadata": {
        "id": "Uw4B9wCZDD5o"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat = ChatOpenAI(openai_api_key=openai_api_key)"
      ],
      "metadata": {
        "id": "AFzQgIK0E5rd"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "problem = \"\\\n",
        "John has some amount of apples, Sarah has double that amount, \\\n",
        "and Mohannad has 3 apples. If they altogether have 12 apples, how man y does John have?\\\n",
        "\""
      ],
      "metadata": {
        "id": "dc3z_brIFHW4"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt template 1: translate to english\n",
        "first_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Convert the following problem into an equation in terms of x, where x is the number of apples John has:\"\n",
        "    \"\\n\\n{problem}. Only give an equation\"\n",
        ")\n",
        "\n",
        "# chain 1: input= Review and output= English_Review\n",
        "chain_one = LLMChain(llm=chat, prompt=first_prompt,\n",
        "                     output_key=\"Equation\", verbose=True\n",
        "                    )\n"
      ],
      "metadata": {
        "id": "5LIC1NhmE2KH"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt template 1: translate to english\n",
        "second_prompt = ChatPromptTemplate.from_template(\n",
        "    \"After solving the following equation in terms of x:\"\n",
        "    \"\\n\\n{Equation}, only provide the response as `x=`\"\n",
        ")\n",
        "\n",
        "# chain 1: input= Review and output= English_Review\n",
        "chain_two = LLMChain(llm=chat, prompt=second_prompt,\n",
        "                     output_key=\"Solution\", verbose=True\n",
        "                    )\n"
      ],
      "metadata": {
        "id": "q1zKvpixBqWz"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt template 1: translate to english\n",
        "third_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Now, narrate how you solved this {problem} to a 6 year old\"\n",
        ")\n",
        "\n",
        "# chain 1: input= Review and output= English_Review\n",
        "chain_three = LLMChain(llm=chat, prompt=third_prompt,\n",
        "                     output_key=\"narration\"\n",
        "                    )"
      ],
      "metadata": {
        "id": "702m9_MxG8U_"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "overall_chain = SequentialChain(\n",
        "    chains=[chain_one, chain_two, chain_three],\n",
        "    input_variables=[\"problem\"],\n",
        "    output_variables=[\"Equation\", \"Solution\", \"narration\"],\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "JXA8fYtMHVR6"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain\n",
        "langchain.debug = True\n",
        "\n",
        "overall_chain(problem)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7dV6BqSH-IO",
        "outputId": "200211b1-5730-46a6-ad9f-44bf893f7ded"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:SequentialChain] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"problem\": \"John has some amount of apples, Sarah has double that amount, and Mohannad has 3 apples. If they altogether have 12 apples, how man y does John have?\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:SequentialChain > 2:chain:LLMChain] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"problem\": \"John has some amount of apples, Sarah has double that amount, and Mohannad has 3 apples. If they altogether have 12 apples, how man y does John have?\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:SequentialChain > 2:chain:LLMChain > 3:llm:ChatOpenAI] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"Human: Convert the following problem into an equation in terms of x, where x is the number of apples John has:\\n\\nJohn has some amount of apples, Sarah has double that amount, and Mohannad has 3 apples. If they altogether have 12 apples, how man y does John have?. Only give an equation\"\n",
            "  ]\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:SequentialChain > 2:chain:LLMChain > 3:llm:ChatOpenAI] [2.58s] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \"Let x be the number of apples John has.\\n\\nSince Sarah has double the amount of apples John has, Sarah has 2x apples.\\n\\nTogether, John, Sarah, and Mohannad have 12 apples.\\n\\nSo, the equation can be written as:\\n\\nx + 2x + 3 = 12\",\n",
            "        \"generation_info\": {\n",
            "          \"finish_reason\": \"stop\"\n",
            "        },\n",
            "        \"message\": {\n",
            "          \"lc\": 1,\n",
            "          \"type\": \"constructor\",\n",
            "          \"id\": [\n",
            "            \"langchain\",\n",
            "            \"schema\",\n",
            "            \"messages\",\n",
            "            \"AIMessage\"\n",
            "          ],\n",
            "          \"kwargs\": {\n",
            "            \"content\": \"Let x be the number of apples John has.\\n\\nSince Sarah has double the amount of apples John has, Sarah has 2x apples.\\n\\nTogether, John, Sarah, and Mohannad have 12 apples.\\n\\nSo, the equation can be written as:\\n\\nx + 2x + 3 = 12\",\n",
            "            \"additional_kwargs\": {}\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": {\n",
            "    \"token_usage\": {\n",
            "      \"prompt_tokens\": 70,\n",
            "      \"completion_tokens\": 63,\n",
            "      \"total_tokens\": 133\n",
            "    },\n",
            "    \"model_name\": \"gpt-3.5-turbo\"\n",
            "  },\n",
            "  \"run\": null\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:SequentialChain > 2:chain:LLMChain] [2.58s] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"Equation\": \"Let x be the number of apples John has.\\n\\nSince Sarah has double the amount of apples John has, Sarah has 2x apples.\\n\\nTogether, John, Sarah, and Mohannad have 12 apples.\\n\\nSo, the equation can be written as:\\n\\nx + 2x + 3 = 12\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:SequentialChain > 4:chain:LLMChain] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"problem\": \"John has some amount of apples, Sarah has double that amount, and Mohannad has 3 apples. If they altogether have 12 apples, how man y does John have?\",\n",
            "  \"Equation\": \"Let x be the number of apples John has.\\n\\nSince Sarah has double the amount of apples John has, Sarah has 2x apples.\\n\\nTogether, John, Sarah, and Mohannad have 12 apples.\\n\\nSo, the equation can be written as:\\n\\nx + 2x + 3 = 12\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:SequentialChain > 4:chain:LLMChain > 5:llm:ChatOpenAI] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"Human: After solving the following equation in terms of x:\\n\\nLet x be the number of apples John has.\\n\\nSince Sarah has double the amount of apples John has, Sarah has 2x apples.\\n\\nTogether, John, Sarah, and Mohannad have 12 apples.\\n\\nSo, the equation can be written as:\\n\\nx + 2x + 3 = 12, only provide the response as `x=`\"\n",
            "  ]\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:SequentialChain > 4:chain:LLMChain > 5:llm:ChatOpenAI] [2.41s] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \"x = 12 - 3 - 2x\\nx + 2x = 12 - 3\\n3x = 9\\nx = 9/3\\nx = 3\",\n",
            "        \"generation_info\": {\n",
            "          \"finish_reason\": \"stop\"\n",
            "        },\n",
            "        \"message\": {\n",
            "          \"lc\": 1,\n",
            "          \"type\": \"constructor\",\n",
            "          \"id\": [\n",
            "            \"langchain\",\n",
            "            \"schema\",\n",
            "            \"messages\",\n",
            "            \"AIMessage\"\n",
            "          ],\n",
            "          \"kwargs\": {\n",
            "            \"content\": \"x = 12 - 3 - 2x\\nx + 2x = 12 - 3\\n3x = 9\\nx = 9/3\\nx = 3\",\n",
            "            \"additional_kwargs\": {}\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": {\n",
            "    \"token_usage\": {\n",
            "      \"prompt_tokens\": 89,\n",
            "      \"completion_tokens\": 41,\n",
            "      \"total_tokens\": 130\n",
            "    },\n",
            "    \"model_name\": \"gpt-3.5-turbo\"\n",
            "  },\n",
            "  \"run\": null\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:SequentialChain > 4:chain:LLMChain] [2.41s] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"Solution\": \"x = 12 - 3 - 2x\\nx + 2x = 12 - 3\\n3x = 9\\nx = 9/3\\nx = 3\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:SequentialChain > 6:chain:LLMChain] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"problem\": \"John has some amount of apples, Sarah has double that amount, and Mohannad has 3 apples. If they altogether have 12 apples, how man y does John have?\",\n",
            "  \"Equation\": \"Let x be the number of apples John has.\\n\\nSince Sarah has double the amount of apples John has, Sarah has 2x apples.\\n\\nTogether, John, Sarah, and Mohannad have 12 apples.\\n\\nSo, the equation can be written as:\\n\\nx + 2x + 3 = 12\",\n",
            "  \"Solution\": \"x = 12 - 3 - 2x\\nx + 2x = 12 - 3\\n3x = 9\\nx = 9/3\\nx = 3\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:SequentialChain > 6:chain:LLMChain > 7:llm:ChatOpenAI] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"Human: Now, narrate how you solved this John has some amount of apples, Sarah has double that amount, and Mohannad has 3 apples. If they altogether have 12 apples, how man y does John have? to a 6 year old\"\n",
            "  ]\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:SequentialChain > 6:chain:LLMChain > 7:llm:ChatOpenAI] [11.15s] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \"Well, let's solve this puzzle step by step!\\n\\nJohn has some apples, but we don't know how many yet. So, let's say John has X apples.\\n\\nSarah has double the amount of apples John has. So, if John has X apples, Sarah will have 2X apples.\\n\\nAnd Mohannad has 3 apples, which means Mohannad has 3 apples.\\n\\nNow, let's add up all the apples they have together. John has X apples, Sarah has 2X apples, and Mohannad has 3 apples. When we add them up, it equals 12 apples.\\n\\nSo, we can write it like this: X + 2X + 3 = 12.\\n\\nNow, let's solve this equation. We can start by combining the X terms. X + 2X is 3X. So, our equation becomes 3X + 3 = 12.\\n\\nTo find out how many apples John has, we need to get X by itself. So, let's subtract 3 from both sides of the equation. We have 3X + 3 - 3 = 12 - 3. This simplifies to 3X = 9.\\n\\nNow, to get X by itself, we need to divide both sides of the equation by 3. We have 3X/3 = 9/3. This simplifies to X = 3.\\n\\nSo, John has 3 apples!\\n\\nI hope that helps, my little friend!\",\n",
            "        \"generation_info\": {\n",
            "          \"finish_reason\": \"stop\"\n",
            "        },\n",
            "        \"message\": {\n",
            "          \"lc\": 1,\n",
            "          \"type\": \"constructor\",\n",
            "          \"id\": [\n",
            "            \"langchain\",\n",
            "            \"schema\",\n",
            "            \"messages\",\n",
            "            \"AIMessage\"\n",
            "          ],\n",
            "          \"kwargs\": {\n",
            "            \"content\": \"Well, let's solve this puzzle step by step!\\n\\nJohn has some apples, but we don't know how many yet. So, let's say John has X apples.\\n\\nSarah has double the amount of apples John has. So, if John has X apples, Sarah will have 2X apples.\\n\\nAnd Mohannad has 3 apples, which means Mohannad has 3 apples.\\n\\nNow, let's add up all the apples they have together. John has X apples, Sarah has 2X apples, and Mohannad has 3 apples. When we add them up, it equals 12 apples.\\n\\nSo, we can write it like this: X + 2X + 3 = 12.\\n\\nNow, let's solve this equation. We can start by combining the X terms. X + 2X is 3X. So, our equation becomes 3X + 3 = 12.\\n\\nTo find out how many apples John has, we need to get X by itself. So, let's subtract 3 from both sides of the equation. We have 3X + 3 - 3 = 12 - 3. This simplifies to 3X = 9.\\n\\nNow, to get X by itself, we need to divide both sides of the equation by 3. We have 3X/3 = 9/3. This simplifies to X = 3.\\n\\nSo, John has 3 apples!\\n\\nI hope that helps, my little friend!\",\n",
            "            \"additional_kwargs\": {}\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": {\n",
            "    \"token_usage\": {\n",
            "      \"prompt_tokens\": 58,\n",
            "      \"completion_tokens\": 309,\n",
            "      \"total_tokens\": 367\n",
            "    },\n",
            "    \"model_name\": \"gpt-3.5-turbo\"\n",
            "  },\n",
            "  \"run\": null\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:SequentialChain > 6:chain:LLMChain] [11.16s] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"narration\": \"Well, let's solve this puzzle step by step!\\n\\nJohn has some apples, but we don't know how many yet. So, let's say John has X apples.\\n\\nSarah has double the amount of apples John has. So, if John has X apples, Sarah will have 2X apples.\\n\\nAnd Mohannad has 3 apples, which means Mohannad has 3 apples.\\n\\nNow, let's add up all the apples they have together. John has X apples, Sarah has 2X apples, and Mohannad has 3 apples. When we add them up, it equals 12 apples.\\n\\nSo, we can write it like this: X + 2X + 3 = 12.\\n\\nNow, let's solve this equation. We can start by combining the X terms. X + 2X is 3X. So, our equation becomes 3X + 3 = 12.\\n\\nTo find out how many apples John has, we need to get X by itself. So, let's subtract 3 from both sides of the equation. We have 3X + 3 - 3 = 12 - 3. This simplifies to 3X = 9.\\n\\nNow, to get X by itself, we need to divide both sides of the equation by 3. We have 3X/3 = 9/3. This simplifies to X = 3.\\n\\nSo, John has 3 apples!\\n\\nI hope that helps, my little friend!\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:SequentialChain] [16.15s] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"Equation\": \"Let x be the number of apples John has.\\n\\nSince Sarah has double the amount of apples John has, Sarah has 2x apples.\\n\\nTogether, John, Sarah, and Mohannad have 12 apples.\\n\\nSo, the equation can be written as:\\n\\nx + 2x + 3 = 12\",\n",
            "  \"Solution\": \"x = 12 - 3 - 2x\\nx + 2x = 12 - 3\\n3x = 9\\nx = 9/3\\nx = 3\",\n",
            "  \"narration\": \"Well, let's solve this puzzle step by step!\\n\\nJohn has some apples, but we don't know how many yet. So, let's say John has X apples.\\n\\nSarah has double the amount of apples John has. So, if John has X apples, Sarah will have 2X apples.\\n\\nAnd Mohannad has 3 apples, which means Mohannad has 3 apples.\\n\\nNow, let's add up all the apples they have together. John has X apples, Sarah has 2X apples, and Mohannad has 3 apples. When we add them up, it equals 12 apples.\\n\\nSo, we can write it like this: X + 2X + 3 = 12.\\n\\nNow, let's solve this equation. We can start by combining the X terms. X + 2X is 3X. So, our equation becomes 3X + 3 = 12.\\n\\nTo find out how many apples John has, we need to get X by itself. So, let's subtract 3 from both sides of the equation. We have 3X + 3 - 3 = 12 - 3. This simplifies to 3X = 9.\\n\\nNow, to get X by itself, we need to divide both sides of the equation by 3. We have 3X/3 = 9/3. This simplifies to X = 3.\\n\\nSo, John has 3 apples!\\n\\nI hope that helps, my little friend!\"\n",
            "}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'problem': 'John has some amount of apples, Sarah has double that amount, and Mohannad has 3 apples. If they altogether have 12 apples, how man y does John have?',\n",
              " 'Equation': 'Let x be the number of apples John has.\\n\\nSince Sarah has double the amount of apples John has, Sarah has 2x apples.\\n\\nTogether, John, Sarah, and Mohannad have 12 apples.\\n\\nSo, the equation can be written as:\\n\\nx + 2x + 3 = 12',\n",
              " 'Solution': 'x = 12 - 3 - 2x\\nx + 2x = 12 - 3\\n3x = 9\\nx = 9/3\\nx = 3',\n",
              " 'narration': \"Well, let's solve this puzzle step by step!\\n\\nJohn has some apples, but we don't know how many yet. So, let's say John has X apples.\\n\\nSarah has double the amount of apples John has. So, if John has X apples, Sarah will have 2X apples.\\n\\nAnd Mohannad has 3 apples, which means Mohannad has 3 apples.\\n\\nNow, let's add up all the apples they have together. John has X apples, Sarah has 2X apples, and Mohannad has 3 apples. When we add them up, it equals 12 apples.\\n\\nSo, we can write it like this: X + 2X + 3 = 12.\\n\\nNow, let's solve this equation. We can start by combining the X terms. X + 2X is 3X. So, our equation becomes 3X + 3 = 12.\\n\\nTo find out how many apples John has, we need to get X by itself. So, let's subtract 3 from both sides of the equation. We have 3X + 3 - 3 = 12 - 3. This simplifies to 3X = 9.\\n\\nNow, to get X by itself, we need to divide both sides of the equation by 3. We have 3X/3 = 9/3. This simplifies to X = 3.\\n\\nSo, John has 3 apples!\\n\\nI hope that helps, my little friend!\"}"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's load a PDF and summarize the first page"
      ],
      "metadata": {
        "id": "EW9BW2ueN9H7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6NXnWeFOA5U",
        "outputId": "c9d6e799-188e-460e-f4b8-a19248a86fed"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-3.16.4-py3-none-any.whl (276 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/276.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.9/276.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.6/276.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-3.16.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"/content/drive/MyDrive/MohannadCV.pdf\")\n",
        "pages = loader.load_and_split()"
      ],
      "metadata": {
        "id": "cr-mqduMN8xM"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p = pages[0]"
      ],
      "metadata": {
        "id": "fi0hItddOVDg"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = \"Read the following page and summarize it in 50 words: \"+ p.page_content"
      ],
      "metadata": {
        "id": "9wsVq3K8P5-k"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "Xb-U3N6rQAyX",
        "outputId": "dbb8c156-f491-4178-d472-adce290ea6f3"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Read the following page and summarize it in 50 words: Mohannad Elhamod\\n/mobile_phone(206) 902 6177 •/envelopeelhamod@vt.edu •/globewordpress.cs.vt.edu/elhamod\\n/linkedinmohannadelhamod\\nEducation\\nVirginia Tech, Blacksburg, VA May 2023\\nPh.D. in Computer Science\\nGPA: 3.83\\nMcGill University, Montreal, QC April 2012\\nM.Eng in Computer Engineering\\nGPA: 3.83\\nJordan University of Science and Technology, Irbid, Jordan June 2007\\nB.Sc in Computer Engineering\\nGPA: 84.4%. Rated Excellent\\nResearch Interests\\nInterested in Knowledge-Guided Machine Learning, and ML interpretability and visualization tools. Involved in\\ninterdisciplinary projects that span a variety of scientiﬁc domains, including business, physics, and biology.\\nResearch Experience\\nDurability, Damage Tolerance and Reliability Branch, NASA June 2022 — August 2022\\nResearch Intern\\n/circle_blankTesting, developing and applying deep learning to demonstrate the use of physics-informed generative adversarial networks\\nto learn and quantify the variability in aerospace materials, ultimately generating an inﬁnite number of virtual tests for\\ncertiﬁcation purposes.\\nScience-Guided Machine Learning Lab, Virginia Tech August 2019 — Present\\nGraduate Research Assistant\\n/circle_blankSupervised by Prof. Anuj Karpatne.\\n/circle_blankCoPhy-PGNN: Learning Physics-guided Neural Networks with Competing Loss Functions for Solving Eigenvalue\\nProblems :\\n-Investigating a deep learning approach to the computationally expensive eigen-decomposition numerical solvers, with\\napplications in quantum physics and electromagnetic propagation.\\n/circle_blankGuiding Neural Networks Using Taxonomies and Phylogenies for Species Modelling :\\n-Tackling the problem of species modelling in the paucity of labelled training data by infusing taxonomic and phylogenetic\\ninformation into model training. Such modeling learns biologically-valid visual features that improve downstream tasks\\nsuch as classiﬁcation, specimen image generation, and trait discovery.\\nVisual Surveillance Group, McGill University January 2010 — April 2012\\nGraduate Research Assistant\\n/circle_blankSupervised by Prof. Martin D. Levine.\\n/circle_blankThesis: Real-Time Automated Annotation of Surveillance Scenes :\\n-Developing a real-time video surveillance system that detects semantically deﬁned activities of interest.\\n/circle_blankUnconstrained real-time face recognition using face morphing and SVM classiﬁers.\\nWork Experience\\nMicrosoft – Azure Monitoring April 2016 – July 2019\\nSoftware Engineer II\\n/circle_blankApplication Insights/Azure Monitoring team.\\n/circle_blankWorked on Metrics Explorer , a cloud computing feature that allows millions of customers to visualize and analyze their data.\\n/circle_blankLead Azure Monitoring ’s product accessibility compliance eﬀorts to provide a seamless experience for people with disabilities.\\nMicrosoft – Microsoft Edge March 2013 – April 2016\\nSoftware Engineer I\\n/circle_blankWorked on core features on the Windows Phone platform, such as InPrivate experience and tabs center.\\n/circle_blankLead the product’s accessibility compliance eﬀorts to provide a seamless experience for people with disabilities.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "langchain.debug = False\n",
        "\n",
        "chat = ChatOpenAI(openai_api_key=openai_api_key)\n",
        "chat.predict(m)\n",
        "\n",
        "# human_message_prompt = HumanMessagePromptTemplate.from_template(m)\n",
        "# chat_prompt = ChatPromptTemplate.from_messages(\n",
        "#     [human_message_prompt]\n",
        "# )\n",
        "\n",
        "# print(chat(\n",
        "#     chat_prompt.format_prompt().to_messages()\n",
        "# ).content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "FKe-uGkJOhpF",
        "outputId": "fdbdcd7a-226c-40ec-c4ee-6df808e6dcb8"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Mohannad Elhamod is a Ph.D. student in Computer Science at Virginia Tech. He has a strong research interest in Knowledge-Guided Machine Learning and ML interpretability and visualization tools. He has research experience in various domains, including aerospace materials and species modeling. He also has work experience as a software engineer at Microsoft, specifically in the Azure Monitoring and Microsoft Edge teams.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's use agents"
      ],
      "metadata": {
        "id": "666V88cx3r2M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Math tool"
      ],
      "metadata": {
        "id": "Krr6NiI94NmQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U wikipedia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4acv-eR5cK6",
        "outputId": "b12666dc-a28e-48f1-b874-fec60d93031a"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.11.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2023.7.22)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.5)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11678 sha256=c9df68ea7c2a290e3775d7a0e936f49ca9fc7e8e9c244578120b8478229741ba\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import load_tools, initialize_agent\n",
        "\n",
        "llm = ChatOpenAI(openai_api_key=openai_api_key) # temperature=0,  #model=llm_model\n",
        "tools = load_tools([\"wikipedia\"], llm=llm) #,\"llm-math\"\n",
        "\n",
        "agent= initialize_agent(\n",
        "    tools,\n",
        "    llm,\n",
        "    # agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    # handle_parsing_errors=True,\n",
        "    verbose = True)\n",
        "\n",
        "# msg = \"What is the 25% of 300?\"\n",
        "msg = \"When was ChatGPT 4 released?\"\n",
        "\n",
        "agent(msg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWqM4Msn3tf-",
        "outputId": "b654ef12-67c9-4193-84d2-68105f24f1c3"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mI should use Wikipedia to find the release date of ChatGPT 4.\n",
            "Action: Wikipedia\n",
            "Action Input: \"ChatGPT 4 release date\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mPage: ChatGPT\n",
            "Summary: ChatGPT, which stands for Chat Generative Pre-trained Transformer, is a large language model-based chatbot developed by OpenAI and launched on November 30, 2022, which enables users to refine and steer a conversation towards a desired length, format, style, level of detail, and language. Successive prompts and replies, known as prompt engineering, are considered at each conversation stage as a context.ChatGPT is built upon either GPT-3.5 or GPT-4—members of OpenAI's proprietary series of generative pre-trained transformer (GPT) models, based on the transformer architecture developed by Google—and is fine-tuned for conversational applications using a combination of supervised and reinforcement learning techniques. ChatGPT was released as a freely available research preview, but due to its popularity, OpenAI now operates the service on a freemium model. It allows users on its free tier to access the GPT-3.5-based version. In contrast, the more advanced GPT-4 based version and priority access to newer features are provided to paid subscribers under the commercial name \"ChatGPT Plus\".\n",
            "By January 2023, it had become what was then the fastest-growing consumer software application in history, gaining over 100 million users and contributing to OpenAI's valuation growing to $29 billion. Within months, Google, Baidu, and Meta accelerated the development of their competing products: Bard, Ernie Bot, and LLaMA. Microsoft launched its Bing Chat based on OpenAI's GPT-4. It raised concern among some observers over the potential of ChatGPT and similar programs to displace or atrophy human intelligence, enable plagiarism, or fuel misinformation.\n",
            "\n",
            "Page: GPT-4\n",
            "Summary: Generative Pre-trained Transformer 4 (GPT-4) is a multimodal large language model created by OpenAI, and the fourth in its series of GPT foundation models. It was initially released on March 14, 2023, and has been made publicly available via the paid chatbot product ChatGPT Plus, and via OpenAI's API.  As a transformer-based model, GPT-4 uses a paradigm where pre-training using both public data and \"data licensed from third-party providers\" is used to predict the next token. After this step, the model was then fine-tuned with reinforcement learning feedback from humans and AI for human alignment and policy compliance.: 2 Observers reported that the iteration of ChatGPT using GPT-4 was an improvement on the previous iteration based on GPT-3.5, with the caveat that GPT-4 retains some of the problems with earlier revisions. GPT-4 is also capable of taking images as input, though this feature has not been made available since launch. OpenAI has declined to reveal various technical details and statistics about GPT-4, such as the precise size of the model.\n",
            "\n",
            "Page: GPT-3\n",
            "Summary: Generative Pre-trained Transformer 3 (GPT-3) is a large language model released by OpenAI in 2020. Like its predecessor GPT-2, it is a decoder-only transformer model of deep neural network, which uses attention in place of previous recurrence- and convolution-based architectures. Attention mechanisms allow the model to selectively focus on segments of input text it predicts to be the most relevant. It uses a 2048-tokens-long context and then-unprecedented size of 175 billion parameters, requiring 800GB to store. The model demonstrated strong zero-shot and few-shot learning on many tasks.Microsoft announced on September 22, 2020, that it had licensed \"exclusive\" use of GPT-3; others can still use the public API to receive output, but only Microsoft has access to GPT-3's underlying model.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThe release date of ChatGPT 4 is November 30, 2022.\n",
            "Final Answer: ChatGPT 4 was released on November 30, 2022.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'When was ChatGPT 4 released?',\n",
              " 'output': 'ChatGPT 4 was released on November 30, 2022.'}"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import HumanMessage\n",
        "\n",
        "msgs = [HumanMessage(content=msg)]\n",
        "llm(msgs).content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "QAN0cxMl5gSU",
        "outputId": "3997d4bd-e10a-4eaa-974d-112240b63d6d"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'As of September 2021, OpenAI has released ChatGPT-4-API, which is the latest version of the ChatGPT model. The specific release date of ChatGPT-4-API is not mentioned.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define your own tool"
      ],
      "metadata": {
        "id": "QCMTVbeV7nZA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The descriprion is what really tells the agent which tool to use."
      ],
      "metadata": {
        "id": "qDfkTNh2VUPy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install DateTime"
      ],
      "metadata": {
        "id": "MhQIJVR87oxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import tool\n",
        "from datetime import date\n",
        "\n",
        "@tool\n",
        "def time(text: str) -> str:\n",
        "    \"\"\"Returns todays date, use this for any \\\n",
        "    questions related to knowing todays date. \\\n",
        "    The input should always be an empty string, \\\n",
        "    and this function will always return todays \\\n",
        "    date - any date mathmatics should occur \\\n",
        "    outside this function.\"\"\"\n",
        "    return str(date.today())"
      ],
      "metadata": {
        "id": "XN12GcJg7tli"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent= initialize_agent(\n",
        "    [time], #tools +\n",
        "    llm,\n",
        "    # agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    handle_parsing_errors=True,\n",
        "    verbose = True)"
      ],
      "metadata": {
        "id": "bNsv5zPP7xFQ"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent(\"I ate a date. Will I be OK?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ru547H-Z70qV",
        "outputId": "582ced7e-34ab-4b4e-f51d-9cf2e1dad885"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThe question is asking about the person's well-being after eating a date. However, this question cannot be answered with the tools given.\u001b[0m\n",
            "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
            "Thought:\u001b[32;1m\u001b[1;3mI cannot answer this question with the given tools.\u001b[0m\n",
            "Observation: Invalid Format: Missing 'Action:' after 'Thought:\n",
            "Thought:\u001b[32;1m\u001b[1;3mI cannot answer this question with the given tools.\n",
            "Final Answer: The question cannot be answered.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'I ate a date. Will I be OK?',\n",
              " 'output': 'The question cannot be answered.'}"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    }
  ]
}