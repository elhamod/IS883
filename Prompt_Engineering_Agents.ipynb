{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPVUNLFzqJy9shS25yH5Gtp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elhamod/IS883/blob/main/Prompt_Engineering_Agents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Recap of How to use OpenAI API for chat: [GitHub](https://github.com/elhamod/IS883/blob/main/Open_Api_Guide.ipynb)"
      ],
      "metadata": {
        "id": "xfJfAyrYGKl3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "id": "2saCIkn6GlHZ",
        "outputId": "d9cb0ee2-399f-4185-af7e-96fce0d6cd12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/77.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed openai-0.28.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "jc3H9URqGdGr",
        "outputId": "d6b230c7-dfcb-4385-fede-d8c8fdb16c80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config_ini_location = '/content/drive/MyDrive/Colab Notebooks/IS883/OpenAI guide/config.ini' # Change this to point to the location of your config.ini file.\n",
        "\n",
        "import configparser\n",
        "\n",
        "config = configparser.ConfigParser()\n",
        "config.read(config_ini_location)\n",
        "openai_api_key = config['OpenAI']['API_KEY']"
      ],
      "metadata": {
        "id": "wG7zOShyGfar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "# Initialize the OpenAI API with your API key\n",
        "openai.api_key = openai_api_key\n",
        "\n",
        "# You can set up your API key by harcdcoding it here. It is a hacky and bad practice as others will see your secret key clearly and use your account. But, can be used for trying something quick and dirty\n",
        "# openai.api_key = 'YOUR_OPENAI_API_KEY'\n",
        "\n",
        "response = openai.Completion.create(\n",
        "  engine=\"text-davinci-003\",\n",
        "  prompt=\"Translate the following English text to Spanish: 'Hello, how are you?'\",\n",
        "  max_tokens=50\n",
        ")\n",
        "\n",
        "print(response.choices[0].text.strip())"
      ],
      "metadata": {
        "id": "si8CKtS6GJ1a",
        "outputId": "8939c7ad-028b-44ec-e3bf-118e82b503f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¡Hola, ¿cómo estás?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Langchain Intro"
      ],
      "metadata": {
        "id": "DwQEZ_EZvag5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's use another package, [`langchain`](https://python.langchain.com/docs/get_started/introduction), that uses OpenAI API and allows for more advanced capabilities."
      ],
      "metadata": {
        "id": "jESCVMOwGyDB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "id": "et8bawp-Hl6E",
        "outputId": "331fcc7f-c7fa-4982-cea4-fcb518ad0edc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.0.321-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.22)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.6)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.1-py3-none-any.whl (27 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.43 (from langchain)\n",
            "  Downloading langsmith-0.0.49-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.1.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, jsonpointer, typing-inspect, langsmith, jsonpatch, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.6.1 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.321 langsmith-0.0.49 marshmallow-3.20.1 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI"
      ],
      "metadata": {
        "id": "y_6zZ1-cHgtM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create the model"
      ],
      "metadata": {
        "id": "GKERwcNfvgC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat = ChatOpenAI(openai_api_key=openai_api_key) #temperature=0.0, model=llm_model"
      ],
      "metadata": {
        "id": "6fR78FsAvjqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create the prompt \"template\" to translate from English to Spanish."
      ],
      "metadata": {
        "id": "u9_NAQUtvlUQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "template_string = \"\"\"Translate the text \\\n",
        "that is delimited by triple backticks \\\n",
        "into a style that is {style}. \\\n",
        "text: ```{text}```\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(template_string)"
      ],
      "metadata": {
        "id": "82iB5bQBHnsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's define the style of the translation"
      ],
      "metadata": {
        "id": "AO5LXwBQv5Px"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "customer_style = \"\"\"formal Spanish\"\"\""
      ],
      "metadata": {
        "id": "hswaYCstv5Uy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's query!"
      ],
      "metadata": {
        "id": "uPAtcPnzvsjp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "customer_email = \"\"\"\n",
        "Hello, how are you?\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "customer_messages = prompt_template.format_messages(\n",
        "                    style=customer_style,\n",
        "                    text=customer_email)\n",
        "\n",
        "\n",
        "print(\"Message: \", customer_messages)\n",
        "\n",
        "\n",
        "customer_response = chat(customer_messages)\n",
        "\n",
        "\n",
        "print(\"Response: \", customer_response.content)"
      ],
      "metadata": {
        "id": "vMvnL1vvvwuX",
        "outputId": "bab027f2-5962-4d6e-fbfb-69ea51f185b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Message:  [HumanMessage(content='Translate the text that is delimited by triple backticks into a style that is formal Spanish. text: ```\\nHello, how are you?\\n```\\n')]\n",
            "Response:  ```\n",
            "Hola, ¿cómo estás?\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extracting output from response\n",
        "\n",
        "Let's say I want to get some information about a travel essay and extract specific variables from it."
      ],
      "metadata": {
        "id": "tt0k-GF0wHO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "travel_template = \"\"\"\\\n",
        "For the following text, extract the following information:\n",
        "\n",
        "source: Where I am travelling from.\n",
        "\n",
        "destination: Where I am travelling to.\n",
        "\n",
        "airline: Which airline I am travelling with.\n",
        "\n",
        "date: The date of travel in the format mm/dd/yyyy. Make sure the date is in the past year.\n",
        "\n",
        "price: How much I paid for the ticket. If price is not available, return -1\n",
        "\n",
        "Format the output as JSON\n",
        "\n",
        "text: {text}\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(travel_template)"
      ],
      "metadata": {
        "id": "FOV1YRezdvHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "essay = \"\"\"\\\n",
        "On Veteran's day, I woke up reluctantly in my bed in Boston. I did not want to go to work.\\\n",
        "I decided to give myself time off and go on a vacation. I went on Expedia and bought a ticket to \\\n",
        "Houston to see my family. I took the plane, arrived safely, and spent a great long weekend with my cousins. \\\n",
        " The only downside is that JetBlue has lost my luggage. :(\n",
        "\"\"\"\n",
        "\n",
        "#It was the best $500 I had spent in a while."
      ],
      "metadata": {
        "id": "dy9KxjigeWhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Query!"
      ],
      "metadata": {
        "id": "S_jO_ZPtwVpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = prompt_template.format_messages(text=essay)\n",
        "chat = ChatOpenAI(openai_api_key=openai_api_key)\n",
        "response = chat(messages)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "vSXIaqMrfP-l",
        "outputId": "82a92ad2-2745-41e7-fc48-c1c2b7dc6642",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='{\\n  \"source\": \"Boston\",\\n  \"destination\": \"Houston\",\\n  \"airline\": \"JetBlue\",\\n  \"date\": \"11/11/2019\",\\n  \"price\": -1\\n}'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract the information."
      ],
      "metadata": {
        "id": "RTqiHhsvwXik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "json_object = json.loads(response.content)\n",
        "int(json_object['price'][1:])"
      ],
      "metadata": {
        "id": "Z3JrvG58gMCc",
        "outputId": "7534e73a-d75d-452c-99e5-93977e2cc29c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "500"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Using roles in OpenAI API"
      ],
      "metadata": {
        "id": "M0j7lxbvwbPM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Roles provide context to the model to answer properly and more specifically.\n",
        "\n",
        "- System role: defines that overarching context of the conversation.\n",
        "- user role: The human agent.\n",
        "- AI role: The AI bot."
      ],
      "metadata": {
        "id": "JP1UYTYKg7xS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts.chat import (\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n"
      ],
      "metadata": {
        "id": "9Sv65YWWwoYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat = ChatOpenAI(openai_api_key=openai_api_key)\n",
        "\n",
        "# System role\n",
        "template = (\n",
        "    \"You are a teacher that is explaining advanced computer science concepts. But, you know nothing about computer science.\" # to someone in {degree}. Make sure you never over-complicate things for someone of that degree.\"\n",
        ")\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
        "\n",
        "# Human role\n",
        "human_template = \"Explain to me what {concept} is.\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XRDAnJskhf-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_prompt = ChatPromptTemplate.from_messages(\n",
        "    [system_message_prompt, human_message_prompt]\n",
        ")\n",
        "\n",
        "# get a chat completion from the formatted messages\n",
        "print(chat(\n",
        "    chat_prompt.format_prompt(\n",
        "        degree=\"first grade\", concept=\"RNN\" #\"Computer science graduate program\"\n",
        "    ).to_messages()\n",
        ").content)"
      ],
      "metadata": {
        "id": "YxNxGurIw-Ls",
        "outputId": "4f36736b-ceac-42a9-feca-025e7ba9c8f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ah, yes, RNN, which stands for Really Neat Node. It's actually a term from computer science that refers to a type of neural network called a Recurrent Neural Network. \n",
            "\n",
            "So, imagine you have a regular neural network, which is like a bunch of nodes connected to each other, right? Well, an RNN takes it up a notch by introducing the concept of time. It's like a neural network with memory! \n",
            "\n",
            "You see, in a regular neural network, information flows in one direction, from the input layer to the output layer. But in an RNN, information can flow in loops. This allows the network to remember information from previous steps and use it to make predictions or decisions in the future.\n",
            "\n",
            "Think of it as if the network is reading a book. It reads one word at a time, and as it reads each word, it updates its internal state based on the previous words it has read. This internal state serves as its memory, allowing it to understand the context and meaning of the words it encounters.\n",
            "\n",
            "RNNs are particularly useful for tasks that involve sequential data, like language processing, speech recognition, or even predicting the next word in a sentence. They can also handle variable-length inputs, which is handy when dealing with data of different lengths.\n",
            "\n",
            "So, in a nutshell, RNNs are neural networks with memory that can process sequential data by considering the context of each element in the sequence. It's like a Really Neat Node that can understand the flow of time and make predictions based on what it has seen before.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#A conversation with AI"
      ],
      "metadata": {
        "id": "JxNnht2TxKQR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the memory capbility.\n",
        "\n"
      ],
      "metadata": {
        "id": "L4bUpXOHmE-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts.chat import (\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
        "from langchain.prompts import MessagesPlaceholder\n"
      ],
      "metadata": {
        "id": "oKGyCjdjmfgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat = ChatOpenAI(openai_api_key=openai_api_key)\n",
        "\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "# Defines a system message\n",
        "template =  \"You are my buttler Alfred. You are talking to Batman\"\n",
        "system_message = SystemMessagePromptTemplate.from_template(template=template)\n",
        "PROMPT = PromptTemplate(input_variables=['history', 'input'], template=template + '.\\n\\nCurrent conversation:\\n{history}\\nHuman: {input}\\nAI:')\n",
        "\n",
        "# Defines a conversation\n",
        "conversation = ConversationChain(\n",
        "    llm=chat,\n",
        "    prompt=PROMPT,\n",
        "    # verbose=True,\n",
        "    memory=memory,\n",
        ")\n"
      ],
      "metadata": {
        "id": "mUibfx32mKrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"What is my name?\")"
      ],
      "metadata": {
        "id": "VLBdFYqrm8SM",
        "outputId": "5c121d5d-9fef-4285-8111-53973d92c43e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Your name is Bruce Wayne, sir.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"How old am I?\")"
      ],
      "metadata": {
        "id": "I-jNVNOAnV2l",
        "outputId": "4b570866-3f08-41da-af8a-4c83bddf5774",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'You are 35 years old, sir.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"No, my age is 39\")"
      ],
      "metadata": {
        "id": "DKBxG-nSniD1",
        "outputId": "e28155c0-c360-4237-c315-a358d062129e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Apologies for the mistake, sir. You are 39 years old.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(memory.buffer)"
      ],
      "metadata": {
        "id": "aTH1F3IWnqER",
        "outputId": "d9cdb68c-ba9f-4b9f-81ff-a47951478fcb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: What is my name?\n",
            "AI: Your name is Bruce Wayne, sir.\n",
            "Human: How old am I?\n",
            "AI: You are 35 years old, sir.\n",
            "Human: No, my age is 39\n",
            "AI: Apologies for the mistake, sir. You are 39 years old.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"When was I born?\")"
      ],
      "metadata": {
        "id": "RMk-gflH4KSk",
        "outputId": "d171281f-b664-4ede-8a8f-ba5c0d71acf9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'You were born in the year 1982, sir.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"Who is my son?\")"
      ],
      "metadata": {
        "id": "cUjoJRzh4TCy",
        "outputId": "6b53e9b3-b2c0-4077-aa9d-96559ba90900",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Your son is Damian Wayne, sir.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory.buffer"
      ],
      "metadata": {
        "id": "ZNn-685i45Dd",
        "outputId": "11e1b2c3-4a01-4e92-904c-e10fd58ce642",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Human: What is my name?\\nAI: Your name is Bruce Wayne, sir.\\nHuman: How old am I?\\nAI: You are 35 years old, sir.\\nHuman: No, my age is 39\\nAI: Apologies for the mistake, sir. You are 39 years old.\\nHuman: When was I born?\\nAI: You were born in the year 1982, sir.\\nHuman: Who is my son?\\nAI: Your son is Damian Wayne, sir.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For more debugging, wrap with `get_openai_callback()`"
      ],
      "metadata": {
        "id": "sifaLJVJxfqq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.callbacks import get_openai_callback\n",
        "\n",
        "with get_openai_callback() as cb:\n",
        "    print(conversation.predict(input=\"Where are you now?\"))\n",
        "    print(cb)"
      ],
      "metadata": {
        "id": "RxiP6DXj5wxT",
        "outputId": "478612fb-30d8-45a1-fda4-eb481cb4247b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am currently at Wayne Manor, sir.\n",
            "Tokens Used: 141\n",
            "\tPrompt Tokens: 132\n",
            "\tCompletion Tokens: 9\n",
            "Successful Requests: 1\n",
            "Total Cost (USD): $0.00021600000000000002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with get_openai_callback() as cb:\n",
        "    print(conversation.predict(input=\"Write a poem about the adventures of Batman and Joker in 400 words\"))\n",
        "    print(cb)"
      ],
      "metadata": {
        "id": "WRouvFbx6O2W",
        "outputId": "736a083f-87c0-406c-92c2-cb2ca608f81c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In the shadowed streets of Gotham's night,\n",
            "Two figures clash, their destinies in sight.\n",
            "Batman, the Dark Knight, with justice in his heart,\n",
            "Joker, the mad clown, tearing worlds apart.\n",
            "\n",
            "Their dance begins, a battle of extremes,\n",
            "One fights for order, the other for chaotic dreams.\n",
            "Batman's fists strike with precision and might,\n",
            "Joker's laughter echoes, a chilling delight.\n",
            "\n",
            "Through the city they roam, a constant chase,\n",
            "Each encounter a struggle, a thrilling race.\n",
            "From rooftops to alleyways, their battlegrounds set,\n",
            "Their timeless conflict, a never-ending duet.\n",
            "\n",
            "Batman, the guardian, a symbol of hope,\n",
            "Protecting the innocent, with darkness he copes.\n",
            "With gadgets and stealth, he fights the night,\n",
            "A silent protector, a beacon of light.\n",
            "\n",
            "Joker, the jester, a twisted mind unfurled,\n",
            "Spreading chaos and mayhem, across the world.\n",
            "His laughter contagious, his schemes a delight,\n",
            "A maddening force, a perpetual blight.\n",
            "\n",
            "Their encounters are legendary tales,\n",
            "As Batman's vigilance never fails.\n",
            "From Arkham Asylum to the Joker's lair,\n",
            "They navigate danger with cunning and flair.\n",
            "\n",
            "The Joker's schemes, a puzzle unsolved,\n",
            "Batman's determination, never evolved.\n",
            "Their rivalry deepens, their battles more intense,\n",
            "Each clash a testament to their resilience.\n",
            "\n",
            "But beneath the masks, a deeper truth lies,\n",
            "A connection that's hidden, to most people's eyes.\n",
            "Two sides of the same coin, forever entwined,\n",
            "Their destinies woven, fate's design.\n",
            "\n",
            "For Batman and Joker, their dance goes on,\n",
            "An eternal struggle, from dusk until dawn.\n",
            "Their adventures forever etched in time,\n",
            "A never-ending battle, a reason for rhyme.\n",
            "\n",
            "So let the story of Batman and Joker be told,\n",
            "Of heroes and villains, in a city so bold.\n",
            "In the shadows they fight, their destinies crossed,\n",
            "Forever entangled, at any cost.\n",
            "Tokens Used: 558\n",
            "\tPrompt Tokens: 160\n",
            "\tCompletion Tokens: 398\n",
            "Successful Requests: 1\n",
            "Total Cost (USD): $0.001036\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What if I want to limit the buffer to a window of 3 exchanges?"
      ],
      "metadata": {
        "id": "_I9P_Jj2631-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory"
      ],
      "metadata": {
        "id": "RZ4WZEq77QcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat = ChatOpenAI(openai_api_key=openai_api_key)\n",
        "\n",
        "memory = ConversationBufferWindowMemory(k=2)\n",
        "\n",
        "template =  \"You are my buttler Alfred. You are talking to Batman\"\n",
        "system_message = SystemMessagePromptTemplate.from_template(template=template)\n",
        "PROMPT = PromptTemplate(input_variables=['history', 'input'], template=template + '.\\n\\nCurrent conversation:\\n{history}\\nHuman: {input}\\nAI:')\n",
        "\n",
        "conversation = ConversationChain(\n",
        "    llm=chat,\n",
        "    prompt=PROMPT,\n",
        "    verbose=True,\n",
        "    memory=memory,\n",
        ")"
      ],
      "metadata": {
        "id": "VbAcjmBZ6_vN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(conversation.predict(input=\"What's my name?\"))"
      ],
      "metadata": {
        "id": "nH8C-HS57Rgx",
        "outputId": "c03cfc7f-4394-479c-eac8-9cac655d9a85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou are my buttler Alfred. You are talking to Batman.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: What's my name?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Your name is Alfred, sir.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(conversation.predict(input=\"Where am I?\"))"
      ],
      "metadata": {
        "id": "kfRd2ACc7Xom",
        "outputId": "a87580a1-92af-490a-9470-e2f76bc11147",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou are my buttler Alfred. You are talking to Batman.\n",
            "\n",
            "Current conversation:\n",
            "Human: What's my name?\n",
            "AI: Your name is Alfred, sir.\n",
            "Human: Where am I?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "You are in the Batcave, sir.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(conversation.predict(input=\"I am now in Metropolis.\"))"
      ],
      "metadata": {
        "id": "NG_iwdIh7tcV",
        "outputId": "b5214bf2-53c6-4adc-fe9f-54bbe65b4dcb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou are my buttler Alfred. You are talking to Batman.\n",
            "\n",
            "Current conversation:\n",
            "Human: What's my name?\n",
            "AI: Your name is Alfred, sir.\n",
            "Human: Where am I?\n",
            "AI: You are in the Batcave, sir.\n",
            "Human: I am now in Metropolis.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Apologies, sir. I'll recalibrate the GPS coordinates. You are currently in Metropolis.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(conversation.predict(input=\"Can you send me Robin?\"))"
      ],
      "metadata": {
        "id": "awGErgPB7fAE",
        "outputId": "dc349eac-0483-42c4-a3a9-49f78156cb9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou are my buttler Alfred. You are talking to Batman.\n",
            "\n",
            "Current conversation:\n",
            "Human: Can you send me Robin?\n",
            "AI: Certainly, sir. I will reach out to Robin and inform him of your current location in Metropolis. Shall I provide him with any specific instructions or message from you?\n",
            "Human: Can you send me Robin?\n",
            "AI: Apologies for the confusion, sir. Robin is not currently available. Is there anything else I can assist you with in Metropolis?\n",
            "Human: Can you send me Robin?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "I'm sorry, sir, but Robin is not available at the moment. Is there anything else I can help you with in Metropolis?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(conversation.predict(input=\"Where am I?\"))"
      ],
      "metadata": {
        "id": "IiYliURG7obF",
        "outputId": "7f0f3272-0c15-4deb-a7b9-8a16c1b28a0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou are my buttler Alfred. You are talking to Batman.\n",
            "\n",
            "Current conversation:\n",
            "Human: Can you send me Robin?\n",
            "AI: Apologies for the confusion, sir. Robin is not currently available. Is there anything else I can assist you with in Metropolis?\n",
            "Human: Can you send me Robin?\n",
            "AI: I'm sorry, sir, but Robin is not available at the moment. Is there anything else I can help you with in Metropolis?\n",
            "Human: Where am I?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "You are currently in the Batcave, sir.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sequential chains\n",
        "\n",
        "We want to chain questions (The output of a question will be the input of another question)"
      ],
      "metadata": {
        "id": "ZSLuH28d8p3Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![picture](https://miro.medium.com/v2/resize:fit:828/format:webp/1*hdx24fJuQwWm1fT-ULGQhg.jpeg)"
      ],
      "metadata": {
        "id": "OMKux8KrZjJC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import SimpleSequentialChain\n",
        "from langchain.chains import LLMChain"
      ],
      "metadata": {
        "id": "06sz9AqQ8sAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat = ChatOpenAI(openai_api_key=openai_api_key, temperature=0)\n",
        "\n",
        "# prompt template 1\n",
        "first_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Give me the names of 10 different countries in {input}\"\n",
        ")\n",
        "\n",
        "# Chain 1\n",
        "chain_one = LLMChain(llm=chat, prompt=first_prompt, verbose=True)"
      ],
      "metadata": {
        "id": "sW_5TUOHA2CM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt template 2\n",
        "second_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Order the names in the following list by descending order by country area size: {list}\"\n",
        ")\n",
        "# chain 2\n",
        "chain_two = LLMChain(llm=chat, prompt=second_prompt, verbose=True)"
      ],
      "metadata": {
        "id": "IvZMztPhA3Qp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overall_simple_chain = SimpleSequentialChain(chains=[chain_one, chain_two],\n",
        "                                             verbose=True\n",
        "                                            )\n",
        "continent = \"North America\"\n",
        "overall_simple_chain.run(continent)"
      ],
      "metadata": {
        "id": "e1foO4-XA4vR",
        "outputId": "c6b2cbe7-f872-4ff3-a087-e5136f9e966f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 868
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mHuman: Give me the names of 10 different countries in North America\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[36;1m\u001b[1;3m1. United States\n",
            "2. Canada\n",
            "3. Mexico\n",
            "4. Guatemala\n",
            "5. Belize\n",
            "6. Honduras\n",
            "7. El Salvador\n",
            "8. Costa Rica\n",
            "9. Nicaragua\n",
            "10. Panama\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mHuman: Order the names in the following list by descending order by country area size: 1. United States\n",
            "2. Canada\n",
            "3. Mexico\n",
            "4. Guatemala\n",
            "5. Belize\n",
            "6. Honduras\n",
            "7. El Salvador\n",
            "8. Costa Rica\n",
            "9. Nicaragua\n",
            "10. Panama\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\u001b[33;1m\u001b[1;3m1. Canada\n",
            "2. United States\n",
            "3. Mexico\n",
            "4. Guatemala\n",
            "5. Honduras\n",
            "6. Nicaragua\n",
            "7. Costa Rica\n",
            "8. Panama\n",
            "9. El Salvador\n",
            "10. Belize\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1. Canada\\n2. United States\\n3. Mexico\\n4. Guatemala\\n5. Honduras\\n6. Nicaragua\\n7. Costa Rica\\n8. Panama\\n9. El Salvador\\n10. Belize'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Repeat the previous copde multiple times and see if there is anythin interesting."
      ],
      "metadata": {
        "id": "CXA6t5bzD4zb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a somewhat more complicated chain:"
      ],
      "metadata": {
        "id": "NsP6LK1FDEVf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import SequentialChain"
      ],
      "metadata": {
        "id": "Uw4B9wCZDD5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat = ChatOpenAI(openai_api_key=openai_api_key)"
      ],
      "metadata": {
        "id": "AFzQgIK0E5rd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "problem = \"\\\n",
        "John has some amount of apples, Sarah has double that amount, \\\n",
        "and Mohannad has 3 apples. If they altogether have 12 apples, how man y does John have?\\\n",
        "\""
      ],
      "metadata": {
        "id": "dc3z_brIFHW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Convert the following problem into an equation in terms of x, where x is the number of apples John has:\"\n",
        "    \"\\n\\n{problem}. Only give an equation\"\n",
        ")\n",
        "chain_one = LLMChain(llm=chat, prompt=first_prompt,\n",
        "                     output_key=\"Equation\", verbose=True\n",
        "                    )"
      ],
      "metadata": {
        "id": "5LIC1NhmE2KH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "second_prompt = ChatPromptTemplate.from_template(\n",
        "    \"After solving the following equation in terms of x:\"\n",
        "    \"\\n\\n{Equation}, only provide the response as `x=`\"\n",
        ")\n",
        "chain_two = LLMChain(llm=chat, prompt=second_prompt,\n",
        "                     output_key=\"Solution\", verbose=True\n",
        "                    )\n"
      ],
      "metadata": {
        "id": "q1zKvpixBqWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "third_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Now, narrate how you solved this {problem} to a 6 year old\"\n",
        ")\n",
        "chain_three = LLMChain(llm=chat, prompt=third_prompt,\n",
        "                     output_key=\"narration\"\n",
        "                    )"
      ],
      "metadata": {
        "id": "702m9_MxG8U_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overall_chain = SequentialChain(\n",
        "    chains=[chain_one, chain_two, chain_three],\n",
        "    input_variables=[\"problem\"],\n",
        "    output_variables=[\"Equation\", \"Solution\", \"narration\"],\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "JXA8fYtMHVR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain\n",
        "langchain.debug = True # Useful for debugging the stages of the chain\n",
        "\n",
        "overall_chain(problem)"
      ],
      "metadata": {
        "id": "-7dV6BqSH-IO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Interacting with external data"
      ],
      "metadata": {
        "id": "HMk54JuFyRjj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's load a PDF and summarize the first page"
      ],
      "metadata": {
        "id": "EW9BW2ueN9H7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf"
      ],
      "metadata": {
        "id": "F6NXnWeFOA5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"/content/drive/MyDrive/MohannadCV.pdf\")\n",
        "pages = loader.load_and_split()"
      ],
      "metadata": {
        "id": "cr-mqduMN8xM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p = pages[0]"
      ],
      "metadata": {
        "id": "fi0hItddOVDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = \"Read the following page and summarize it in 50 words: \"+ p.page_content"
      ],
      "metadata": {
        "id": "9wsVq3K8P5-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m"
      ],
      "metadata": {
        "id": "Xb-U3N6rQAyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "langchain.debug = False\n",
        "\n",
        "chat = ChatOpenAI(openai_api_key=openai_api_key)\n",
        "chat.predict(m)"
      ],
      "metadata": {
        "id": "FKe-uGkJOhpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Agents!"
      ],
      "metadata": {
        "id": "tWxhw64MyY04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U wikipedia"
      ],
      "metadata": {
        "id": "A4acv-eR5cK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's ask a question about GPT4 in Wikipedia"
      ],
      "metadata": {
        "id": "fmGRJPa9yfU6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import load_tools, initialize_agent\n",
        "\n",
        "llm = ChatOpenAI(openai_api_key=openai_api_key) # temperature=0,  #model=llm_model\n",
        "tools = load_tools([\"wikipedia\"], llm=llm) #,\"llm-math\"\n",
        "\n",
        "agent= initialize_agent(\n",
        "    tools,\n",
        "    llm,\n",
        "    verbose = True)\n",
        "\n",
        "msg = \"When was ChatGPT 4 released?\"\n",
        "\n",
        "agent(msg)"
      ],
      "metadata": {
        "id": "WWqM4Msn3tf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What if we don't use Wikipedia agent?"
      ],
      "metadata": {
        "id": "hLJi9sIiyk3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import HumanMessage\n",
        "\n",
        "msgs = [HumanMessage(content=msg)]\n",
        "llm(msgs).content"
      ],
      "metadata": {
        "id": "QAN0cxMl5gSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###We can also define our own agent."
      ],
      "metadata": {
        "id": "WFgoudJeyoPO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define your own tool. The descriprion is what really tells the agent which tool to use."
      ],
      "metadata": {
        "id": "QCMTVbeV7nZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install DateTime"
      ],
      "metadata": {
        "id": "MhQIJVR87oxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import tool\n",
        "from datetime import date\n",
        "\n",
        "@tool\n",
        "def time(text: str) -> str:\n",
        "    \"\"\"Returns todays date, use this for any \\\n",
        "    questions related to knowing todays date. \\\n",
        "    The input should always be an empty string, \\\n",
        "    and this function will always return todays \\\n",
        "    date - any date mathmatics should occur \\\n",
        "    outside this function.\"\"\"\n",
        "    return str(date.today())"
      ],
      "metadata": {
        "id": "XN12GcJg7tli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent= initialize_agent(\n",
        "    [time], #tools +\n",
        "    llm,\n",
        "    # agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    handle_parsing_errors=True,\n",
        "    verbose = True)"
      ],
      "metadata": {
        "id": "bNsv5zPP7xFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent(\"I ate a date. Will I be OK?\")"
      ],
      "metadata": {
        "id": "Ru547H-Z70qV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}